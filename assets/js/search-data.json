{
  
    
        "post0": {
            "title": "European Football Odds Analysis",
            "content": "I&#39;ve started thinking about applying machine learning to betting the European Soccer Leagues. It seems like it&#39;s going to be a different problem than my previous work on baseball, as there are 3 possible outcomes for every match (win, lose, draw). But still, I&#39;d like to see if the profitable machine learning process I put together will hold. . The first step in this process is getting data on how well the oddsmakers can predict the outcomes of these matches. This establishes the goals any model I make will need to exceed to be successful. Football-data.co.uk is a great site that publishes historical odds from different matchmakers in easy-to-use csv format. So let&#39;s pull down their spreadsheets and see how they did. . Each blog post on this site, including this one, is executable. Use the buttons at the top to run the code on Binder of Colab and get fresh results for yourself. You can also download it from Github to run the notebook locally. . import pandas as pd import warnings warnings.filterwarnings(&#39;ignore&#39;) . csvs = [ &#39;https://www.football-data.co.uk/mmz4281/2021/SP1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1920/SP1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1819/SP1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1718/SP1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/2021/E0.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1920/E0.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1819/E0.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1718/E0.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/2021/D1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1920/D1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1819/D1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1718/D1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/2021/I1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1920/I1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1819/I1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1718/I1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/2021/F1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1920/F1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1819/F1.csv&#39;, &#39;https://www.football-data.co.uk/mmz4281/1718/F1.csv&#39;, ] df = pd.DataFrame() for c in csvs: df = pd.concat([df, pd.read_csv(c)]) df.shape . (6083, 127) . Over 6000 matches should be enough, right? These spredsheets are pretty dense, with 127 columns of information. See their notes for how to decode the column headers. . Let&#39;s see the %ages for home, away and draw results using the &#39;FTR&#39; (Full Time Result) column . df[&#39;FTR&#39;].value_counts()/len(df) . H 0.438106 A 0.310044 D 0.251849 Name: FTR, dtype: float64 . The home team seems to have a 12% advantage over the away team and 25% of matches end in a draw. This has always been pretty frustrating for me as a fan, I hate it when matches end in a draw. Even more when they are scoreless. . df[&#39;total_score&#39;] = df[[&#39;FTAG&#39;,&#39;FTHG&#39;]].sum(axis=1) (df[&#39;total_score&#39;]==0).mean() . 0.06822291632418215 . OK, that only happens 6.8 percent of the time, but it still feels like a lot to me. . Back to the task at hand - how well do the oddsmakers predict the outcomes of the matches? First we need to find oddsmakers that are consistent throughout our dataset. The averages provided cover less than half of the matches. . df[[&#39;AvgH&#39;,&#39;AvgD&#39;,&#39;AvgA&#39;]].isna().sum() . AvgH 3652 AvgD 3652 AvgA 3652 dtype: int64 . It looks like most of the columns with the match betting odds are mostly 3 letters long, so we&#39;ll use that as as shortcut to compare them . df[[x for x in df.columns if len(x)==3]].isna().sum().sort_values(ascending=True)[:25] . Div 0 FTR 0 HTR 1 HST 1 AST 1 BWH 2 BWD 2 BWA 2 WHH 3 VCH 3 VCD 3 IWH 3 IWA 3 IWD 3 WHD 3 VCA 3 WHA 3 PSH 12 PSD 12 PSA 12 AHh 3653 LBD 4259 LBH 4259 LBA 4259 dtype: int64 . It looks like Bet&amp;Win has the most match coverage. So let&#39;s look at them. . df.dropna(subset=[&#39;BWH&#39;,&#39;BWD&#39;,&#39;BWA&#39;], inplace=True) df.shape . (6081, 128) . df[&#39;favorite&#39;] = df[[&#39;BWH&#39;,&#39;BWD&#39;,&#39;BWA&#39;]].idxmin(axis=1) (df[&#39;favorite&#39;].str[-1]==df[&#39;FTR&#39;]).mean() . 0.5331359973688538 . 53% of the time, the odds favorite wins the match. This is much better than chance (33%). . Calibration . Next, let&#39;s look at calibration. The odds for a match can be converted into a probability of each team winning. There&#39;s a great article on how to do this on bettingexpert.com. For this dataset, we are lucky to have dollar odds, so the probabilities are just the inverse of the odds. If the dollar odds are 2.5, then the probability is 1 / 2.5 = 40%. . Below we check whether those probabilities are accurate. When they say the home club has 40% odds, do they win 40% of the time? there are 2 good ways of knowing this: graphically with a reliability curve, and quantitatively, with a metric called Brier score. Both are below. . from sklearn.calibration import calibration_curve from sklearn.metrics import brier_score_loss import matplotlib.pyplot as plt def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 8)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} t Brier Loss: {:.4f}&quot;.format( name, brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() data = [ # truth, probabilities, labels (df[&#39;FTR&#39;]==&#39;H&#39;, 1/df[&#39;BWH&#39;], &#39;Home Win&#39;), (df[&#39;FTR&#39;]==&#39;A&#39;, 1/df[&#39;BWA&#39;], &#39;Away Win&#39;), (df[&#39;FTR&#39;]==&#39;D&#39;, 1/df[&#39;BWD&#39;], &#39;Draw t&#39;), ] cal_curve(data, 10) . Home Win Brier Loss: 0.2098 Away Win Brier Loss: 0.1806 Draw Brier Loss: 0.1848 . Except for the aberration in draws at 40%, that&#39;s really tight. The MLB baseball project had 0.2358 Brier Score. Lower is better in brier land, so this is 10-20% better. . MLB Baseball accuracy was about 10% beter than chance, this seems to be 20%. . Conclusion . It looks like European soccer is significantly easier for the oddsmakers to predict than American baseball. I&#39;m interested to see if this affects our ability to exceed the odds maker predictions. . This was only meant to give an idea of how well oddsmakers predict matches, but I already see some interesting features of the data. I&#39;ll leave you to explore further. I&#39;d be especially interested in the profitability of predicting match draws, as well as any differences among the leagues - perhaps some are more competitive than others. . Remember - hit the buttons at the top to download and run this notebook yourself. .",
            "url": "https://rdpharr.github.io/project_notes/soccer/benchmark/webscraping/brier/accuracy/calibration/2020/12/26/eurofootball_odds_analysis.html",
            "relUrl": "/soccer/benchmark/webscraping/brier/accuracy/calibration/2020/12/26/eurofootball_odds_analysis.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Nevada Sex Worker Arrest Analysis",
            "content": "Nevada Sex Worker Arrest Analysis . I recently had the opportunity to work with a group of sex workers and sex worker advocates. They came together to oppose a proposed bill in the Nevada legislature that is calling for harsher penalties for many types of sex work (AB64). I absolutely support the decriminalization of sex work, and this bill goes the wrong direction. . So I was lucky that they reached out to me to help them understand the data in a public records request that had been filed by dswork.org. I prepared the below dashboard in tableau that helped us come to the following conclusions: . About 2000 people per year get arrested and charged with prostitution, but less than 40% of them are convicted of that charge. The rest either get reduced charges (16%), or their cases are not pursued by the prosecutors or are dismissed in court (43%). This is clear evidence that the police are arresting large numbers of people without evidence. | For those charged with Loitering for Purposes of Prostitution, it’s even worse. Almost 92% of arrests end with dismissal or denial of charges. Almost no one gets a conviction of this “crime” in Nevada, even though there are more than 200 arrests/year. | Even though the state of Nevada is only 10.3% Black, 56% of women arrested for prostitution are Black. White women are 74% of Nevadan women and only 39% of arrests for prostitution. Police are clearly targeting Black women. | 94% of arrests for sex work are done by Las Vegas Metro. Reno and Sparks are &lt;3%. North Las Vegas and Henderson are less than 1% together. | . Tableau Dashboard . Click the image to go to the live dashboard .",
            "url": "https://rdpharr.github.io/project_notes/tableau/sex%20work/arrests/convictions/prostitution/activism/nevada/legislature/2020/12/14/Nevada_Sex_Worker_Convictions.html",
            "relUrl": "/tableau/sex%20work/arrests/convictions/prostitution/activism/nevada/legislature/2020/12/14/Nevada_Sex_Worker_Convictions.html",
            "date": " • Dec 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "MLB Predictions and Betting Strategy",
            "content": "MLB Baseball Prediction Series: Part 1 Part 2 Part 3 Part 4 Part 5 . | . It&#39;s finally time to put all of this work into practice. This is going to be another long notebook, though. It&#39;s a lot of work. . Warning: Do this at your own risk. If you lose money, it&#8217;s not my problem. Gamble responsibly. If you have think you may have a problem controlling yourself, get some help. . Important: But if you win, you owe me 10%. . To make bet recommendations, this notebook needs to do all this: . Download games and odds that have completed | Download today&#39;s games and their odds | Calculate the stats for today&#39;s games | Generate predictions using our saved model | Calculate bet sizes based on the moneyline odds and our predictions | . Luckily a bunch of the code for this was written in previous posts, we just need to get it into this notebook. . Update Historic Data . First, we&#39;ll update the data from our web scraping. I&#39;ve hidden all the code you saw in parts 2 and 3. Click the button to unhide it. . from bs4 import BeautifulSoup as bs import requests # these are functions related to parsing the baseball reference page def get_game_summary(soup, game_id): game = {&#39;game_id&#39;: game_id} scorebox = soup.find(&#39;div&#39;, {&#39;class&#39;:&#39;scorebox&#39;}) teams = scorebox.findAll(&#39;a&#39;,{&#39;itemprop&#39;:&#39;name&#39;}) game[&#39;away_team_abbr&#39;] = teams[0][&#39;href&#39;].split(&#39;/&#39;)[2] game[&#39;home_team_abbr&#39;] = teams[1][&#39;href&#39;].split(&#39;/&#39;)[2] meta = scorebox.find(&#39;div&#39;, {&#39;class&#39;:&#39;scorebox_meta&#39;}).findAll(&#39;div&#39;) game[&#39;date&#39;] = meta[0].text.strip() game[&#39;start_time&#39;] = meta[1].text[12:-6].strip() return game def get_table_summary(soup, table_no): stats_tables = soup.findAll(&#39;table&#39;, {&#39;class&#39;:&#39;stats_table&#39;}) t = stats_tables[table_no].find(&#39;tfoot&#39;) summary = {x[&#39;data-stat&#39;]:x.text.strip() for x in t.findAll(&#39;td&#39;)} return summary def get_pitcher_data(soup, table_no): stats_tables = soup.findAll(&#39;table&#39;, {&#39;class&#39;:&#39;stats_table&#39;}) t = stats_tables[table_no] data = [] rows = t.findAll(&#39;tr&#39;)[1:-1] # not the header and footer rows for r in rows: summary = {x[&#39;data-stat&#39;]:x.text.strip() for x in r.findAll(&#39;td&#39;)} summary[&#39;name&#39;] = r.find(&#39;th&#39;,{&#39;data-stat&#39;:&#39;player&#39;}).find(&#39;a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1][:-6].strip() data.append(summary) return data def process_link(url): resp = requests.get(url) game_id = url.split(&#39;/&#39;)[-1][:-6] # strange preprocessing routine uncommented_html = &#39;&#39; for h in resp.text.split(&#39; n&#39;): if &#39;&lt;!-- &lt;div&#39; in h: continue if h.strip() == &#39;&lt;!--&#39;: continue if h.strip() == &#39;--&gt;&#39;: continue uncommented_html += h + &#39; n&#39; soup = bs(uncommented_html) data = { &#39;game&#39;: get_game_summary(soup, game_id), &#39;away_batting&#39;: get_table_summary(soup, 1), &#39;home_batting&#39;:get_table_summary(soup, 2), &#39;away_pitching&#39;:get_table_summary(soup, 3), &#39;home_pitching&#39;:get_table_summary(soup, 4), &#39;away_pitchers&#39;: get_pitcher_data(soup, 3), &#39;home_pitchers&#39;: get_pitcher_data(soup, 4) } return data def get_covers_data(date_string): odds_data = [] # get the web page with game data on it url = f&#39;https://www.covers.com/Sports/MLB/Matchups?selectedDate={date_string}&#39; resp = requests.get(url) # parse the games scraped_games = bs(resp.text).findAll(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_game_box&#39;}) for g in scraped_games: game = {} game[&#39;home_moneyline&#39;] = g[&#39;data-game-odd&#39;] game[&#39;date&#39;] = g[&#39;data-game-date&#39;] game[&#39;away_team_abbr&#39;] = g[&#39;data-away-team-shortname-search&#39;] game[&#39;home_team_abbr&#39;] = g[&#39;data-home-team-shortname-search&#39;] try: game[&#39;home_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_home&#39;}).text.strip() game[&#39;away_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_away&#39;}).text.strip() except: game[&#39;home_score&#39;] =&#39;&#39; game[&#39;away_score&#39;] =&#39;&#39; odds_data.append(game) return odds_data . . First we&#39;ll load our saved data . import pickle game_data = pickle.load(open(&#39;game_data.pkl&#39;,&#39;rb&#39;)) . Let&#39;s get the scores for the games that aren&#39;t in our database yet. . current_year=2020 # find all games for the year url = f&quot;https://www.baseball-reference.com/leagues/MLB/{current_year}-schedule.shtml&quot; resp = requests.get(url) soup=bs(resp.text) game_soups = soup.findAll(&#39;a&#39;,text=&#39;Boxscore&#39;) game_links = [x[&#39;href&#39;] for x in game_soups] # for instance &#39;/boxes/LAN/LAN202007230.shtml&#39; # compare against downloaded games downloaded_games = [g[&#39;game&#39;][&#39;game_id&#39;] for g in game_data] new_game_links = [x for x in game_links if x[-18:-6] not in downloaded_games] # get the new games for link in new_game_links: url = &#39;https://www.baseball-reference.com&#39; + link game_data.append(process_link(url)) print(&quot;New games downloaded: &quot;, len(new_game_links)) . New games downloaded: 46 . Append Today&#39;s Games . Now we&#39;ll append today&#39;s games to this data . import pandas as pd import datetime as dt today_games=[] url = &#39;https://www.baseball-reference.com/previews/&#39; page = requests.get(url).text soup = bs(page) summaries = soup.findAll(&#39;div&#39;, {&#39;class&#39;:&#39;game_summary&#39;}) for s in summaries: game = { &#39;game&#39;:{ &#39;game_id&#39;: s.find(&#39;a&#39;, text=&#39;Preview&#39;)[&#39;href&#39;][-18:-6], &#39;is_test&#39;:True, &#39;date&#39;: dt.datetime.now().strftime(&#39;%A, %B %d, %Y&#39;) }, &#39;home_batting&#39;:{&#39;R&#39;:0}, &#39;away_batting&#39;:{&#39;R&#39;:0}, &#39;home_pitching&#39;:{&#39;R&#39;:0}, &#39;away_pitching&#39;:{&#39;R&#39;:0}, &#39;home_pitchers&#39;:[{&#39;R&#39;:0}], &#39;away_pitchers&#39;:[{&#39;R&#39;:0}] } cells = s.findAll(&#39;table&#39;)[0].findAll(&#39;td&#39;) # skip postponed games if not s.find(&#39;a&#39;, text=&quot;Preview&quot;): continue # skip game 2 in double header - links look like this for 2nd games: &quot;/previews/2020/PHI202008051.shtml&quot; if s.find(&#39;a&#39;, text=&quot;Preview&quot;)[&#39;href&#39;][-7]==&#39;2&#39;: continue try: team_links = s.findAll(&#39;a&#39;) game[&#39;game&#39;][&#39;away_team_abbr&#39;] = team_links[0][&#39;href&#39;].split(&#39;/&#39;)[2] game[&#39;game&#39;][&#39;home_team_abbr&#39;] = team_links[2][&#39;href&#39;].split(&#39;/&#39;)[2] except Exception as e: #just all star games trigger this, I think print(team_links) continue # get time game[&#39;game&#39;][&#39;start_time&#39;] = s.find(&#39;table&#39;,{&#39;class&#39;:&#39;teams&#39;}).find(&#39;tbody&#39;).findAll(&#39;tr&#39;)[1].findAll(&#39;td&#39;)[2].text.strip() # get pitchers try: cells = s.findAll(&#39;table&#39;)[1].findAll(&#39;td&#39;) game[&#39;away_pitchers&#39;][0][&#39;name&#39;] = cells[1].find(&#39;a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1][:-6].strip() game[&#39;home_pitchers&#39;][0][&#39;name&#39;] = cells[3].find(&#39;a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1][:-6].strip() except Exception as e: # no pitcher game[&#39;away_pitchers&#39;][0][&#39;name&#39;] = &#39;&#39; game[&#39;home_pitchers&#39;][0][&#39;name&#39;] = &#39;&#39; today_games.append(game) game_data.extend(today_games) print(len(today_games), &quot;Games today&quot;) for x in today_games: print(x[&#39;game&#39;][&#39;game_id&#39;], x[&#39;game&#39;][&#39;start_time&#39;], x[&#39;game&#39;][&#39;away_team_abbr&#39;],x[&#39;away_pitchers&#39;][0][&#39;name&#39;], x[&#39;game&#39;][&#39;home_team_abbr&#39;],x[&#39;home_pitchers&#39;][0][&#39;name&#39;]) . 15 Games today NYA202009260 1:05PM MIA rogertr01 NYY garcide01 WAS202009261 3:05PM NYM degroja01 WSN scherma01 OAK202009261 4:10PM SEA sheffju01 OAK minormi01 TOR202009260 6:37PM BAL meansjo01 TOR zeuchtj01 KCA202009260 7:05PM DET boydma01 KCR hernaca04 TEX202009260 7:05PM HOU TEX SLN202009260 7:07PM MIL woodrbr01 STL wainwad01 TBA202009260 7:07PM PHI wheelza01 TBR curtijo02 ATL202009260 7:10PM BOS houckta01 ATL CHA202009260 7:10PM CHC lestejo01 CHW dunnida01 CLE202009260 7:10PM PIT musgrjo01 CLE civalaa01 MIN202009260 7:10PM CIN castilu02 MIN pinedmi01 ARI202009260 8:10PM COL marquge01 ARI weavelu01 LAN202009260 9:10PM LAA bundydy01 LAD gonsoto01 SFN202009260 9:15PM SDP davieza02 SFG cuetojo01 . Generate Stats and Features Using Code from Part 2 and Part 3 of this Series . Now we&#39;ll create our dataframe in the same way we did in Part 2 of this series. Since the above data is just appended to the end of the db, the stats for our test data will get filled in during the shift(1) statements that build the stats. . Again I hid the code because it&#39;s a lot and you&#39;ve already seen it. . import pandas as pd games = [] batting = [] pitching = [] pitchers = [] for g in game_data: game_summary = g[&#39;game&#39;] if &#39;is_test&#39; not in game_summary.keys(): game_summary[&#39;is_test&#39;]=False # fix date game_summary[&#39;date&#39;] = game_summary[&#39;date&#39;] + &quot; &quot; + game_summary[&#39;start_time&#39;] del game_summary[&#39;start_time&#39;] # get starting pitchers game_summary[&#39;home_pitcher&#39;] = g[&#39;home_pitchers&#39;][0][&#39;name&#39;] game_summary[&#39;away_pitcher&#39;] = g[&#39;away_pitchers&#39;][0][&#39;name&#39;] # this is the field we&#39;ll train our model to predict game_summary[&#39;home_team_win&#39;] = int(g[&#39;home_batting&#39;][&#39;R&#39;])&gt;int(g[&#39;away_batting&#39;][&#39;R&#39;]) games.append(game_summary) # add all stats to appropriate lists target_pairs = [ (&#39;away_batting&#39;, batting), (&#39;home_batting&#39;, batting), (&#39;away_pitching&#39;, pitching), (&#39;home_pitching&#39;, pitching), (&#39;away_pitchers&#39;, pitchers), (&#39;home_pitchers&#39;, pitchers) ] for key, d in target_pairs: if isinstance(g[key], list): # pitchers for x in g[key]: if &#39;home&#39; in key: x[&#39;is_home_team&#39;] = True x[&#39;team&#39;] = g[&#39;game&#39;][&#39;home_team_abbr&#39;] else: x[&#39;is_home_team&#39;] = False x[&#39;team&#39;] = g[&#39;game&#39;][&#39;away_team_abbr&#39;] x[&#39;game_id&#39;] = g[&#39;game&#39;][&#39;game_id&#39;] d.append(x) else: #batting, pitching x = g[key] if &#39;home&#39; in key: x[&#39;is_home_team&#39;] = True x[&#39;team&#39;] = g[&#39;game&#39;][&#39;home_team_abbr&#39;] x[&#39;spread&#39;] = int(g[key][&#39;R&#39;]) - int(g[key.replace(&#39;home&#39;,&#39;away&#39;)][&#39;R&#39;]) else: x[&#39;is_home_team&#39;] = False x[&#39;team&#39;] = g[&#39;game&#39;][&#39;away_team_abbr&#39;] x[&#39;spread&#39;] = int(g[key][&#39;R&#39;]) - int(g[key.replace(&#39;away&#39;,&#39;home&#39;)][&#39;R&#39;]) x[&#39;game_id&#39;] = g[&#39;game&#39;][&#39;game_id&#39;] d.append(x) game_df = pd.DataFrame(games) game_df[&#39;date&#39;] = pd.to_datetime(game_df[&#39;date&#39;], errors=&#39;coerce&#39;) game_df = game_df[~game_df[&#39;game_id&#39;].str.contains(&#39;allstar&#39;)].copy() #don&#39;t care about allstar games batting_df = pd.DataFrame(batting) for k in batting_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue batting_df[k] =pd.to_numeric(batting_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) batting_df.drop(columns=[&#39;details&#39;], inplace=True) pitching_df = pd.DataFrame(pitching) for k in pitching_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue pitching_df[k] =pd.to_numeric(pitching_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) pitcher_df = pd.DataFrame(pitchers) for k in pitcher_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;name&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue pitcher_df[k] =pd.to_numeric(pitcher_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) # filter the pitcher performances to just the starting pitcher pitcher_df = pitcher_df[~pitcher_df[&#39;game_score&#39;].isna()].copy().reset_index(drop=True) pitcher_df.drop(columns=[x for x in pitcher_df.keys() if &#39;inherited&#39; in x], inplace=True) print(&quot;Created game_df, batting_df, pitching_df and pitcher_df&quot;) . . Created game_df, batting_df, pitching_df and pitcher_df . Now we&#39;ll create all the calculated features of the model that we made in Part 2 of this blog . import numpy as np def add_rolling(period, df, stat_columns): for s in stat_columns: if &#39;object&#39; in str(df[s].dtype): continue df[s+&#39;_&#39;+str(period)+&#39;_Avg&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).mean()) df[s+&#39;_&#39;+str(period)+&#39;_Std&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).std()) df[s+&#39;_&#39;+str(period)+&#39;_Skew&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).skew()) return df def get_diff_df(df, name, is_pitcher=False): #runs for each of the stat dataframes, returns the difference in stats #set up dataframe with time index df[&#39;date&#39;] = pd.to_datetime(df[&#39;game_id&#39;].str[3:-1], format=&quot;%Y%m%d&quot;) df = df.sort_values(by=&#39;date&#39;).copy() newindex = df.groupby(&#39;date&#39;)[&#39;date&#39;] .apply(lambda x: x + np.arange(x.size).astype(np.timedelta64)) df = df.set_index(newindex).sort_index() # get stat columns stat_cols = [x for x in df.columns if &#39;int&#39; in str(df[x].dtype)] stat_cols.extend([x for x in df.columns if &#39;float&#39; in str(df[x].dtype)]) #add lags df = add_rolling(&#39;5d&#39;, df, stat_cols) # this game series df = add_rolling(&#39;10d&#39;, df, stat_cols) df = add_rolling(&#39;45d&#39;, df, stat_cols) df = add_rolling(&#39;180d&#39;, df, stat_cols) # this season df = add_rolling(&#39;730d&#39;, df, stat_cols) # 2 years # reset stat columns to just the lags (removing the original stats) df.drop(columns=stat_cols, inplace=True) stat_cols = [x for x in df.columns if &#39;int&#39; in str(df[x].dtype)] stat_cols.extend([x for x in df.columns if &#39;float&#39; in str(df[x].dtype)]) # shift results so that each row is a pregame stat df = df.reset_index(drop=True) df = df.sort_values(by=&#39;date&#39;) for s in stat_cols: if is_pitcher: df[s] = df.groupby(&#39;name&#39;)[s].shift(1) else: df[s] = df.groupby(&#39;team&#39;)[s].shift(1) # calculate differences in pregame stats from home vs. away teams away_df = df[~df[&#39;is_home_team&#39;]].copy() away_df = away_df.set_index(&#39;game_id&#39;) away_df = away_df[stat_cols] home_df = df[df[&#39;is_home_team&#39;]].copy() home_df = home_df.set_index(&#39;game_id&#39;) home_df = home_df[stat_cols] diff_df = home_df.subtract(away_df, fill_value=0) diff_df = diff_df.reset_index() # clean column names for s in stat_cols: diff_df[name + &quot;_&quot; + s] = diff_df[s] diff_df.drop(columns=s, inplace=True) return diff_df df = game_df df = pd.merge(left=df, right = get_diff_df(batting_df, &#39;batting&#39;), on = &#39;game_id&#39;, how=&#39;left&#39;) df = pd.merge(left=df, right = get_diff_df(pitching_df, &#39;pitching&#39;), on = &#39;game_id&#39;, how=&#39;left&#39;) df = pd.merge(left=df, right = get_diff_df(pitcher_df, &#39;pitcher&#39;,is_pitcher=True), on = &#39;game_id&#39;, how=&#39;left&#39;) #pitcher rest feature pitcher_df = pd.DataFrame(pitchers) # old version was filtered to just starters dates = pitcher_df[&#39;game_id&#39;].str[3:-1] pitcher_df[&#39;date&#39;] = pd.to_datetime(dates,format=&#39;%Y%m%d&#39;, errors=&#39;coerce&#39;) pitcher_df[&#39;rest&#39;] = pitcher_df.groupby(&#39;name&#39;)[&#39;date&#39;].diff().dt.days # filter the pitcher performances to just the starting pitcher pitcher_df = pitcher_df[~pitcher_df[&#39;game_score&#39;].isna()].copy().reset_index(drop=True) home_pitchers = pitcher_df[pitcher_df[&#39;is_home_team&#39;]].copy().reset_index(drop=True) df = pd.merge(left=df, right=home_pitchers[[&#39;game_id&#39;,&#39;name&#39;, &#39;rest&#39;]], left_on=[&#39;game_id&#39;,&#39;home_pitcher&#39;], right_on=[&#39;game_id&#39;,&#39;name&#39;], how=&#39;left&#39;) df.rename(columns={&#39;rest&#39;:&#39;home_pitcher_rest&#39;}, inplace=True) away_pitchers = pitcher_df[~pitcher_df[&#39;is_home_team&#39;]].copy().reset_index(drop=True) df = pd.merge(left=df, right=away_pitchers[[&#39;game_id&#39;,&#39;name&#39;,&#39;rest&#39;]], left_on=[&#39;game_id&#39;,&#39;away_pitcher&#39;], right_on=[&#39;game_id&#39;,&#39;name&#39;], how=&#39;left&#39;) df.rename(columns={&#39;rest&#39;:&#39;away_pitcher_rest&#39;}, inplace=True) df[&#39;rest_diff&#39;] = df[&#39;home_pitcher_rest&#39;]-df[&#39;away_pitcher_rest&#39;] #datetime features df.dropna(subset=[&#39;date&#39;], inplace=True) df[&#39;season&#39;] = df[&#39;date&#39;].dt.year df[&#39;month&#39;]=df[&#39;date&#39;].dt.month df[&#39;week&#39;]=df[&#39;date&#39;].dt.isocalendar().week.astype(&#39;int&#39;) df[&#39;dow&#39;]=df[&#39;date&#39;].dt.weekday df[&#39;date&#39;] = (pd.to_datetime(df[&#39;date&#39;]) - pd.Timestamp(&quot;1970-01-01&quot;)) // pd.Timedelta(&#39;1s&#39;) #epoch time print(&quot;The shape of our main dataframe is now (rows x columns):&quot;,df.shape) . . The shape of our main dataframe is now (rows x columns): (10596, 1037) . Now we&#39;ll add the power rankings using the code we built in Part 3 of this series. . from elote import EloCompetitor ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=EloCompetitor() for x in df.away_team_abbr.unique(): ratings[x]=EloCompetitor() home_team_elo = [] away_team_elo = [] elo_exp = [] df = df.sort_values(by=&#39;date&#39;).reset_index(drop=True) for i, r in df.iterrows(): # get pre-game ratings elo_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr])) home_team_elo.append(ratings[r.home_team_abbr].rating) away_team_elo.append(ratings[r.away_team_abbr].rating) # update ratings if r.home_team_win: ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr]) else: ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr]) df[&#39;elo_exp&#39;] = elo_exp df[&#39;home_team_elo&#39;] = home_team_elo df[&#39;away_team_elo&#39;] = away_team_elo #elo slow ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=EloCompetitor() ratings[x]._k_score=16 for x in df.away_team_abbr.unique(): ratings[x]=EloCompetitor() ratings[x]._k_score=16 home_team_elo = [] away_team_elo = [] elo_exp = [] df = df.sort_values(by=&#39;date&#39;).reset_index(drop=True) for i, r in df.iterrows(): # get pregame ratings elo_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr])) home_team_elo.append(ratings[r.home_team_abbr].rating) away_team_elo.append(ratings[r.away_team_abbr].rating) # update ratings if r.home_team_win: ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr]) else: ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr]) df[&#39;elo_slow_exp&#39;] = elo_exp df[&#39;home_team_elo_slow&#39;] = home_team_elo df[&#39;away_team_elo_slow&#39;] = away_team_elo #glicko from elote import GlickoCompetitor ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=GlickoCompetitor() for x in df.away_team_abbr.unique(): ratings[x]=GlickoCompetitor() home_team_glick = [] away_team_glick = [] glick_exp = [] df = df.sort_values(by=&#39;date&#39;).reset_index(drop=True) for i, r in df.iterrows(): # get pregame ratings glick_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr])) home_team_glick.append(ratings[r.home_team_abbr].rating) away_team_glick.append(ratings[r.away_team_abbr].rating) # update ratings if r.home_team_win: ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr]) else: ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr]) df[&#39;glick_exp&#39;] = glick_exp df[&#39;home_team_glick&#39;] = home_team_glick df[&#39;away_team_glick&#39;] = away_team_glick #trueskill from trueskill import Rating, quality, rate ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=Rating(25) for x in df.away_team_abbr.unique(): ratings[x]=Rating(25) for x in df.home_pitcher.unique(): ratings[x]=Rating(25) for x in df.away_pitcher.unique(): ratings[x]=Rating(25) ts_quality = [] pitcher_ts_diff = [] team_ts_diff = [] home_pitcher_ts = [] away_pitcher_ts = [] home_team_ts = [] away_team_ts = [] df = df.sort_values(by=&#39;date&#39;).copy() for i, r in df.iterrows(): # get pre-match trueskill ratings from dict match = [(ratings[r.home_team_abbr], ratings[r.home_pitcher]), (ratings[r.away_team_abbr], ratings[r.away_pitcher])] ts_quality.append(quality(match)) pitcher_ts_diff.append(ratings[r.home_pitcher].mu-ratings[r.away_pitcher].mu) team_ts_diff.append(ratings[r.home_team_abbr].mu-ratings[r.away_team_abbr].mu) home_pitcher_ts.append(ratings[r.home_pitcher].mu) away_pitcher_ts.append(ratings[r.away_pitcher].mu) home_team_ts.append(ratings[r.home_team_abbr].mu) away_team_ts.append(ratings[r.away_team_abbr].mu) if r.date &lt; df.date.max(): # update ratings dictionary with post-match ratings if r.home_team_win==1: match = [(ratings[r.home_team_abbr], ratings[r.home_pitcher]), (ratings[r.away_team_abbr], ratings[r.away_pitcher])] [(ratings[r.home_team_abbr], ratings[r.home_pitcher]), (ratings[r.away_team_abbr], ratings[r.away_pitcher])] = rate(match) else: match = [(ratings[r.away_team_abbr], ratings[r.away_pitcher]), (ratings[r.home_team_abbr], ratings[r.home_pitcher])] [(ratings[r.away_team_abbr], ratings[r.away_pitcher]), (ratings[r.home_team_abbr], ratings[r.home_pitcher])] = rate(match) df[&#39;ts_game_quality&#39;] = ts_quality df[&#39;pitcher_ts_diff&#39;] = pitcher_ts_diff df[&#39;team_ts_diff&#39;] = team_ts_diff df[&#39;home_pitcher_ts&#39;] = home_pitcher_ts df[&#39;away_pitcher_ts&#39;] = away_pitcher_ts df[&#39;home_team_ts&#39;] = home_team_ts df[&#39;away_team_ts&#39;] = away_team_ts print(&quot;The shape of our main dataframe after adding skill rankings is (rows x columns):&quot;,df.shape) . . The shape of our main dataframe after adding skill rankings is (rows x columns): (10596, 1053) . Now to update the odds data and get it into the dataframe properly . import pickle odds_data = pickle.load(open(&#39;covers_data_2.pkl&#39;,&#39;rb&#39;)) . import pandas as pd import requests from bs4 import BeautifulSoup as bs dates = pd.to_datetime(df[&#39;date&#39;], unit=&#39;s&#39;) game_days = dates.dt.strftime(&#39;%Y-%m-%d&#39;).unique() existing_odds_days = [x[&#39;date&#39;][:10] for x in odds_data] new_game_days = [x for x in game_days if x not in existing_odds_days] for d in new_game_days: # get the web page with game data on it url = f&#39;https://www.covers.com/Sports/MLB/Matchups?selectedDate={d}&#39; resp = requests.get(url) # parse the games scraped_games = bs(resp.text).findAll(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_game_box&#39;}) for g in scraped_games: game = {} game[&#39;home_moneyline&#39;] = g[&#39;data-game-odd&#39;] game[&#39;date&#39;] = g[&#39;data-game-date&#39;] game[&#39;away_team_abbr&#39;] = g[&#39;data-away-team-shortname-search&#39;] game[&#39;home_team_abbr&#39;] = g[&#39;data-home-team-shortname-search&#39;] try: game[&#39;home_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_home&#39;}).text.strip() game[&#39;away_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_away&#39;}).text.strip() except: game[&#39;home_score&#39;] =&#39;&#39; game[&#39;away_score&#39;] =&#39;&#39; odds_data.append(game) print(&quot;Done! Days of odds downloaded:&quot;, len(new_game_days)) . Done! Days of odds downloaded: 4 . Let&#39;s integrate that into the dataframe in the same way as in Part 3 . import numpy as np import pandas as pd odds = pd.DataFrame(odds_data) odds[&#39;home_moneyline&#39;].replace(&#39;&#39;, np.nan, inplace=True) odds.dropna(subset=[&#39;home_moneyline&#39;], inplace=True) odds.home_moneyline = pd.to_numeric(odds.home_moneyline) odds.date = pd.to_datetime(odds.date).dt.date import warnings warnings.filterwarnings(&#39;ignore&#39;) warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) odds.home_team_abbr[odds.home_team_abbr==&#39;SF&#39;]=&#39;SFG&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;TB&#39;]=&#39;TBR&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;WAS&#39;]=&#39;WSN&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;KC&#39;]=&#39;KCR&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;SD&#39;]=&#39;SDP&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;SF&#39;]=&#39;SFG&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;TB&#39;]=&#39;TBR&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;WAS&#39;]=&#39;WSN&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;KC&#39;]=&#39;KCR&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;SD&#39;]=&#39;SDP&#39; odds[&#39;odds_proba&#39;]=np.nan odds[&#39;odds_proba&#39;][odds.home_moneyline&lt;0] = -odds.home_moneyline/(-odds.home_moneyline + 100) odds[&#39;odds_proba&#39;][odds.home_moneyline&gt;0] = (100/(odds.home_moneyline + 100)) # get dates into the same format odds[&#39;date&#39;] = (pd.to_datetime(pd.to_datetime(odds[&#39;date&#39;])) - pd.Timestamp(&quot;1970-01-01&quot;)) // pd.Timedelta(&#39;1s&#39;) # do the merge df = pd.merge_asof(left=df.sort_values(by=&#39;date&#39;), right=odds[[&#39;home_team_abbr&#39;,&#39;date&#39;, &#39;away_team_abbr&#39;,&#39;odds_proba&#39;]].sort_values(by=&#39;date&#39;), by=[&#39;home_team_abbr&#39;,&#39;away_team_abbr&#39;], on=&#39;date&#39;) df = df.sort_values(by=&#39;date&#39;).copy().reset_index(drop=True) print(&#39;Dataframe shape after adding odds data:&#39;, df.shape) . . Dataframe shape after adding odds data: (10596, 1054) . print(&quot;Today&#39;s Games and the Home Team Win Probabilities&quot;) test_df = df[df[&#39;is_test&#39;]][[&#39;home_team_abbr&#39;, &#39;away_team_abbr&#39;, &#39;home_pitcher&#39;,&#39;away_pitcher&#39;, &#39;odds_proba&#39;]] display(test_df) . Today&#39;s Games and the Home Team Win Probabilities . home_team_abbr away_team_abbr home_pitcher away_pitcher odds_proba . 10581 NYY | MIA | garcide01 | rogertr01 | 0.708455 | . 10582 WSN | NYM | scherma01 | degroja01 | 0.434783 | . 10583 OAK | SEA | minormi01 | sheffju01 | 0.677419 | . 10584 TOR | BAL | zeuchtj01 | meansjo01 | 0.565217 | . 10585 KCR | DET | hernaca04 | boydma01 | 0.565217 | . 10586 TEX | HOU | | | 0.444444 | . 10587 TBR | PHI | curtijo02 | wheelza01 | 0.512195 | . 10588 STL | MIL | wainwad01 | woodrbr01 | 0.500000 | . 10589 ATL | BOS | | houckta01 | 0.598394 | . 10590 CHW | CHC | dunnida01 | lestejo01 | 0.607843 | . 10591 CLE | PIT | civalaa01 | musgrjo01 | 0.636364 | . 10592 MIN | CIN | pinedmi01 | castilu02 | 0.565217 | . 10593 ARI | COL | weavelu01 | marquge01 | 0.534884 | . 10594 LAD | LAA | gonsoto01 | bundydy01 | 0.649123 | . 10595 SFG | SDP | cuetojo01 | davieza02 | 0.420168 | . Generate Predictions . We&#39;ll continue to use code from the previous part of the series to prepare the data for predictions. . encode_me = [x for x in df.keys() if &#39;object&#39; in str(df[x].dtype)] for x in encode_me: df[x] = df.groupby(x)[&#39;home_team_win&#39;].apply(lambda x:x.rolling(180).mean()).shift(1) # create Prediction Set X_test = df[df[&#39;is_test&#39;]].drop(columns=[&#39;home_team_win&#39;, &#39;game_id&#39;,&#39;is_test&#39;]) len(X_test) . 15 . import pickle model = pickle.load(open(&#39;xgb_model.pkl&#39;,&#39;rb&#39;)) test_df[&#39;xgb_home_win&#39;] = model.predict(X_test).astype(&#39;bool&#39;) test_df[&#39;xgb_home_win_proba&#39;] = model.predict_proba(X_test)[:,1] test_df . home_team_abbr away_team_abbr home_pitcher away_pitcher odds_proba xgb_home_win xgb_home_win_proba . 10581 NYY | MIA | garcide01 | rogertr01 | 0.708455 | True | 0.701283 | . 10582 WSN | NYM | scherma01 | degroja01 | 0.434783 | False | 0.315103 | . 10583 OAK | SEA | minormi01 | sheffju01 | 0.677419 | True | 0.618798 | . 10584 TOR | BAL | zeuchtj01 | meansjo01 | 0.565217 | False | 0.492718 | . 10585 KCR | DET | hernaca04 | boydma01 | 0.565217 | False | 0.482250 | . 10586 TEX | HOU | | | 0.444444 | False | 0.410267 | . 10587 TBR | PHI | curtijo02 | wheelza01 | 0.512195 | False | 0.378972 | . 10588 STL | MIL | wainwad01 | woodrbr01 | 0.500000 | False | 0.395375 | . 10589 ATL | BOS | | houckta01 | 0.598394 | True | 0.549559 | . 10590 CHW | CHC | dunnida01 | lestejo01 | 0.607843 | True | 0.570431 | . 10591 CLE | PIT | civalaa01 | musgrjo01 | 0.636364 | True | 0.650989 | . 10592 MIN | CIN | pinedmi01 | castilu02 | 0.565217 | True | 0.553619 | . 10593 ARI | COL | weavelu01 | marquge01 | 0.534884 | False | 0.441937 | . 10594 LAD | LAA | gonsoto01 | bundydy01 | 0.649123 | True | 0.624194 | . 10595 SFG | SDP | cuetojo01 | davieza02 | 0.420168 | False | 0.405266 | . This is good. It looks like our model disagrees with the casino odds for a few games. One trap you need to be thinking about is that when a pitcher is not announced, xgboost is still going to give you a prediction. It will be a very different prediction if you tell it about the pitchers, my advice is to wait for the pitcher announcements. . Next step is to figure our if any of these games are good bets. . Kelly Criterion . The Kelly criterion is a betting strategy developed in the 50&#39;s that tells you the percentage of your bankroll to bet based on your advantage in the bets. Its objective is to maximize profit while minimizing risk of ruin, and it&#39;s been rigorously proven time and again. . People still hate it though. The major criticism is that it sets you up for some pretty extreme bets. People have found two major ways of dealing with this. The first is using a fractional-Kelly, usually a half-Kelly or quarter-Kelly, where they will use the Kelly formula and bet half or a quarter as much as it tells them. The other is a diversified approach, where the better will place several simultaneous bets dedicating a fraction of their bankroll to each bet. It&#39;s easy to utilize the latter in baseball, since multiple games are happening simultaneously. . Implementing the formula is pretty straight forward. We&#39;re going to loop through the data from above, applying the formula to both the home and away teams, then print out only the bets that the formula says have good expected value. . from IPython.display import HTML bets = [] for i,r in test_df.iterrows(): p = r[&#39;xgb_home_win_proba&#39;] # probability of win b = (1/r[&#39;odds_proba&#39;])-1 #return q = 1-r[&#39;xgb_home_win_proba&#39;] # probability of loss bet = { &#39;date&#39;: dt.datetime.now().date(), &#39;team&#39;: r[&#39;home_team_abbr&#39;], &#39;pitcher&#39;: r[&#39;home_pitcher&#39;], &#39;opposition&#39;:r[&#39;away_team_abbr&#39;], &#39;opp_pitcher&#39;:r[&#39;away_pitcher&#39;], &#39;odds_proba&#39;: r[&#39;odds_proba&#39;], &#39;dollar return&#39;: b, &#39;ml_proba&#39;: p, &#39;kelly_criterion&#39;: p-(q/b) } bets.append(bet) # away team p = 1 - r[&#39;xgb_home_win_proba&#39;] # probability of win b = (1/(1.02-r[&#39;odds_proba&#39;]))-1 #1.02 is calibrated to mgm vs consensus odds q = r[&#39;xgb_home_win_proba&#39;] # probability of loss bet = { &#39;date&#39;: dt.datetime.now().date(), &#39;team&#39;: r[&#39;away_team_abbr&#39;], &#39;pitcher&#39;: r[&#39;away_pitcher&#39;], &#39;opposition&#39;:r[&#39;home_team_abbr&#39;], &#39;opp_pitcher&#39;:r[&#39;home_pitcher&#39;], &#39;odds_proba&#39;: 1-r[&#39;odds_proba&#39;], &#39;dollar return&#39;: b, &#39;ml_proba&#39;: p, &#39;kelly_criterion&#39;: p-(q/b) } bets.append(bet) bet_df = pd.DataFrame(bets) HTML(bet_df[bet_df[&#39;kelly_criterion&#39;]&gt;0].to_html(index=False)) . date team pitcher opposition opp_pitcher odds_proba dollar return ml_proba kelly_criterion . 2020-09-26 | NYM | degroja01 | WSN | scherma01 | 0.565217 | 0.708767 | 0.684897 | 0.240319 | . 2020-09-26 | SEA | sheffju01 | OAK | minormi01 | 0.322581 | 1.919021 | 0.381202 | 0.058747 | . 2020-09-26 | BAL | meansjo01 | TOR | zeuchtj01 | 0.434783 | 1.198853 | 0.507282 | 0.096290 | . 2020-09-26 | DET | boydma01 | KCR | hernaca04 | 0.434783 | 1.198853 | 0.517750 | 0.115490 | . 2020-09-26 | HOU | | TEX | | 0.555556 | 0.737452 | 0.589733 | 0.033403 | . 2020-09-26 | PHI | wheelza01 | TBR | curtijo02 | 0.487805 | 0.969260 | 0.621028 | 0.230037 | . 2020-09-26 | MIL | woodrbr01 | STL | wainwad01 | 0.500000 | 0.923077 | 0.604625 | 0.176302 | . 2020-09-26 | BOS | houckta01 | ATL | | 0.401606 | 1.371880 | 0.450441 | 0.049853 | . 2020-09-26 | CHC | lestejo01 | CHW | dunnida01 | 0.392157 | 1.426261 | 0.429569 | 0.029620 | . 2020-09-26 | CLE | civalaa01 | PIT | musgrjo01 | 0.636364 | 0.571429 | 0.650989 | 0.040219 | . 2020-09-26 | COL | marquge01 | ARI | weavelu01 | 0.465116 | 1.061361 | 0.558063 | 0.141675 | . 2020-09-26 | LAA | bundydy01 | LAD | gonsoto01 | 0.350877 | 1.696310 | 0.375806 | 0.007834 | . Model Interpretation . Above it looks like the algorithm has selected 12 bets for us, with the Kelly formula ranging from &lt;1% of our bankroll to 24%. Let&#39;s take a deeper look at each. . In the first, the algorithm says to put 24% of our bankroll on the Mets. The casino gives them a 57% chance of willing and so will payback only 1.471 for every dollar we bet (&quot;dollar return&quot;). Our machine learning model gives them a 68.5% chance, so there&#39;s money to be made. . In the second, the algo says bet on Seattle, but only 5.8% of the bankroll. We think they are going to lose, but we thing there&#39;s a better chance of them winning than the casino does. Because there&#39;s a large payback, Kelly says we should make a bet. . In the Houston game, we don&#39;t have pitcher data, so maybe we should not bet on that one. . Etc., etc. I feel like you can take it from here. . My Learnings . I&#39;ve used this approach this season and done well. It&#39;s been a money maker overall. But my results are still super streaky. I&#39;ve had 2 days in a row where I&#39;ve gotten every game right, and then whole weeks where I&#39;ve steadily lost money every day. I haven&#39;t had any day where I lost every game, thankfully. But I&#39;m sure it&#39;s coming. . One thing I will say is that it&#39;s pretty boring. Betting off a spreadsheet and taking small wins/losses is not exactly an adventure. It starts to feel like more of a job. . I hope this series has helped you out, either getting you started, or giving you some ideas to incorporate back into your process. Get in touch and let me know. I&#39;d love to hear what you are up to. .",
            "url": "https://rdpharr.github.io/project_notes/baseball/webscraping/kelly%20criterion/xgboost/2020/09/26/predictions-and-betting-strategy.html",
            "relUrl": "/baseball/webscraping/kelly%20criterion/xgboost/2020/09/26/predictions-and-betting-strategy.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "MLB Model Optimization",
            "content": "MLB Baseball Prediction Series: Part 1 Part 2 Part 3 Part 4 Part 5 . | . In Part 3, our model was already performing better than the casino&#39;s oddsmakers, but it was only 0.6% better in accuracy and calibration was at parity. In this notebook, we&#39;ll get those numbers higher by doing some optimization of the hyperparameters and getting more data. . Get More Data . This is key to solving the overfitting problem we saw in Part 3. XGBoost wanting a low max_depth feels like a big deal to me, so let&#39;s solve it. I think of overfitting as a problem of either too many features or not enough data. I really love all the fancy and clever features we&#39;ve made, so let&#39;s get more data. . Start by going back to the blog post for Part 2 and run the whole thing again, except when you download from baseball-reference.com, start in 2013 instead fo 2016. The code to change is right under the header &quot;Create List of Games to Download&quot;. It might take a bit to download all that data. . ... I&#39;ll wait ... . Then re-run Part 3 to add the odds and power rankings to the dataset. . ... Go on ... . Done? Great. Let&#39;s get that new dataframe you saved into this notebook. Now we&#39;ll prepare it to be run by XGBoost in the same way we did in the last post, except that we&#39;re going to increase the size of the validation set from 500 games to 2500 games - a whole season of games. . import pickle df = pickle.load(open(&#39;dataframe_part3_big.pkl&#39;,&#39;rb&#39;)) # target encoding encode_me = [x for x in df.keys() if &#39;object&#39; in str(df[x].dtype)] for x in encode_me: df[x] = df.groupby(x)[&#39;home_team_win&#39;].apply(lambda x:x.rolling(180).mean()).shift(1) # create test, train splits df = df.sort_values(by=&#39;date&#39;).copy().reset_index(drop=True) X = df.drop(columns=[&#39;home_team_win&#39;, &#39;game_id&#39;]) y = df.home_team_win X_train = X[:-2500] y_train = y[:-2500] X_valid = X[-2500:-500] y_valid = y[-2500:-500] X_test = X[-500:] y_test = y[-500:] . Hyperparameter Optimization . For this post we&#39;ll use Hyperopt, a Baysean hyperparameter optimization tool. This will perform a search of the different parameters that we&#39;ll be feeding XGBoost to see which perform best. You can read on their website about how it works. . Install it like this pip install hyperopt . Below are the 3 functions I use to optimize XGBoost. The get_xgb_model function just trains the model, xgb_objective calls the first and does the model scoring, and get_xgbparams is the function that starts the hyperopt magic. . from hyperopt import fmin, tpe, hp, Trials import xgboost as xgb from sklearn.metrics import accuracy_score, brier_score_loss def get_xgb_model(params): # comment the next 2 lines out if you don&#39;t have gpu params[&#39;gpu_id&#39;] = 0 params[&#39;tree_method&#39;] = &#39;gpu_hist&#39; params[&#39;seed&#39;]=13 gbm = xgb.XGBClassifier(**params,n_estimators=999) model = gbm.fit(X_train, y_train, verbose=False, eval_set = [[X_train, y_train], [X_valid, y_valid]], eval_metric=&#39;logloss&#39;, early_stopping_rounds=15) return model def xgb_objective(params): params[&#39;max_depth&#39;]=int(params[&#39;max_depth&#39;]) model = get_xgb_model(params) xgb_test_proba = model.predict_proba(X_valid)[:,1] score = brier_score_loss(y_valid, xgb_test_proba) return(score) trials = Trials() # recorder for our results def get_xgbparams(space, evals=15): params = fmin(xgb_objective, space=space, algo=tpe.suggest, max_evals=evals, trials=trials) params[&#39;max_depth&#39;]=int(params[&#39;max_depth&#39;]) return params . Next we define the search space for hpyperopt and start the run. I won&#39;t go into the function of each of the hyperparameters, but I will give a shout out to this blog post - it&#39;s one of the best resources I&#39;ve seen on the subject. Most of the time I get good results in 250 runs, but if I&#39;m feeling thorough (and patient) I let it run for 2000 iterations. The space below could represent a million significantly different parameter combinations, so 250 trials might be putting too much faith in the search algorithm. . Let&#39;s do 500 iterations and see what happens. . import numpy as np hyperopt_runs = 500 space = { &#39;max_depth&#39;: hp.quniform(&#39;max_depth&#39;, 1, 8, 1), &#39;min_child_weight&#39;: hp.quniform(&#39;min_child_weight&#39;, 3, 15, 1), &#39;learning_rate&#39;: hp.qloguniform(&#39;learning_rate&#39;, np.log(.01),np.log(.1),.01), &#39;subsample&#39;: hp.quniform(&#39;subsample&#39;, 0.5, 1.0,.1), &#39;colsample_bytree&#39;: hp.quniform(&#39;colsample_bytree&#39;, 0.5, 1.0,.1), &#39;reg_alpha&#39;: hp.qloguniform(&#39;reg_alpha&#39;,np.log(1e-2),np.log(1e2),1e-2) } xgb_params = get_xgbparams(space,hyperopt_runs) xgb_params . 100%|█████████▉| 499/500 [24:42&lt;00:06, 6.13s/trial, best loss: 0.2338288995254672] . {&#39;colsample_bytree&#39;: 0.5, &#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 5.0, &#39;reg_alpha&#39;: 2.42, &#39;subsample&#39;: 1.0} . We just spent long time optimizing to the validation set and we found a very good set of parameters for it. That doesn&#39;t mean it&#39;s going to work well for the test set. But we&#39;ll cross our fingers anyway. . from sklearn.calibration import calibration_curve from sklearn.metrics import accuracy_score, brier_score_loss import matplotlib.pyplot as plt def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 8)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_pred, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} t tAccuracy:{:.4f} t Brier Loss: {:.4f}&quot;.format( name, accuracy_score(y_test, y_pred), brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() model = get_xgb_model(xgb_params) xgb_test_preds = model.predict(X_test) xgb_test_proba = model.predict_proba(X_test)[:,1] casino_proba = X_test[&#39;odds_proba&#39;] casino_preds = X_test[&#39;odds_proba&#39;]&gt;.5 data = [ (y_test, casino_preds, casino_proba, &#39;Casino&#39;), (y_test,xgb_test_preds, xgb_test_proba, &#39;XGBoost&#39;) ] cal_curve(data, 15) . . Casino Accuracy:0.5840 Brier Loss: 0.2398 XGBoost Accuracy:0.5860 Brier Loss: 0.2419 . Well that&#39;s not better. Even though our validation set had a brier_score_loss of 0.234, when we tried the same parameters on test data it scores 0.241 - 3% worse. Let&#39;s print out our top hyperopt runs and see if the top one is an aberration. . import pandas as pd results = [] for i in range(len(trials.trials)): trial = trials.trials[i] inputs = trial[&#39;misc&#39;][&#39;vals&#39;] d = {} for key in inputs.keys(): d[key]=inputs[key][0] d[&#39;loss&#39;] = trials.losses()[i] results.append(d) trial_df = pd.DataFrame(results).drop_duplicates() trial_df = trial_df.sort_values(by=&#39;loss&#39;).copy().reset_index(drop=True) trial_df[:15] . colsample_bytree learning_rate max_depth min_child_weight reg_alpha subsample loss . 0 0.5 | 0.10 | 5.0 | 5.0 | 2.42 | 1.0 | 0.233829 | . 1 0.5 | 0.10 | 5.0 | 5.0 | 2.24 | 1.0 | 0.234436 | . 2 0.5 | 0.10 | 4.0 | 7.0 | 1.59 | 1.0 | 0.234506 | . 3 0.5 | 0.08 | 5.0 | 8.0 | 1.56 | 1.0 | 0.234574 | . 4 0.5 | 0.08 | 5.0 | 6.0 | 2.45 | 1.0 | 0.234706 | . 5 0.5 | 0.10 | 5.0 | 6.0 | 4.13 | 1.0 | 0.234795 | . 6 0.5 | 0.10 | 4.0 | 8.0 | 1.51 | 1.0 | 0.234867 | . 7 0.5 | 0.10 | 4.0 | 9.0 | 0.78 | 1.0 | 0.234868 | . 8 0.5 | 0.06 | 2.0 | 15.0 | 0.10 | 0.8 | 0.234890 | . 9 0.5 | 0.06 | 2.0 | 12.0 | 0.41 | 0.8 | 0.234934 | . 10 0.5 | 0.09 | 4.0 | 7.0 | 1.04 | 1.0 | 0.234936 | . 11 0.5 | 0.10 | 4.0 | 7.0 | 1.68 | 1.0 | 0.234939 | . 12 0.5 | 0.10 | 4.0 | 8.0 | 0.01 | 1.0 | 0.234946 | . 13 0.5 | 0.10 | 4.0 | 8.0 | 2.93 | 1.0 | 0.234954 | . 14 0.5 | 0.10 | 4.0 | 6.0 | 1.42 | 1.0 | 0.234964 | . Let&#39;s iterate through the top results and try to find a combination that translates to our test set. . best_params = {} best_accuracy = 0 for i, r in trial_df.iterrows(): if i&gt;50: continue params = { &#39;colsample_bytree&#39;:r[&#39;colsample_bytree&#39;], &#39;learning_rate&#39;:r[&#39;learning_rate&#39;], &#39;max_depth&#39;:int(r[&#39;max_depth&#39;]), &#39;min_child_weight&#39;:r[&#39;min_child_weight&#39;], &#39;reg_alpha&#39;:r[&#39;reg_alpha&#39;], &#39;subsample&#39;:r[&#39;subsample&#39;] } model = get_xgb_model(params) xgb_test_preds = model.predict(X_test) xgb_test_proba = model.predict_proba(X_test)[:,1] acc = accuracy_score(y_test&gt;.5, xgb_test_preds) brier = brier_score_loss(y_test, xgb_test_proba) if acc &gt; best_accuracy: print(&#39;brier&#39;, brier, &#39; taccuracy&#39;, acc,&#39; trow:&#39;, i) best_accuracy = acc best_params = params best_params[&#39;max_depth&#39;] = int(best_params[&#39;max_depth&#39;]) print(&#39; nbest accuracy&#39;, best_accuracy) print(best_params) . brier 0.24185080720648772 accuracy 0.586 row: 0 brier 0.2411891311063104 accuracy 0.59 row: 3 brier 0.24080703076468848 accuracy 0.596 row: 7 brier 0.2403566982835091 accuracy 0.598 row: 20 brier 0.23987331277418703 accuracy 0.61 row: 42 best accuracy 0.61 {&#39;colsample_bytree&#39;: 0.5, &#39;learning_rate&#39;: 0.08, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 3.0, &#39;reg_alpha&#39;: 2.8000000000000003, &#39;subsample&#39;: 1.0, &#39;gpu_id&#39;: 0, &#39;tree_method&#39;: &#39;gpu_hist&#39;, &#39;seed&#39;: 13} . model = get_xgb_model(best_params) xgb_test_preds = model.predict(X_test) xgb_test_proba = model.predict_proba(X_test)[:,1] casino_proba = X_test[&#39;odds_proba&#39;] casino_preds = X_test[&#39;odds_proba&#39;]&gt;.5 data = [ (y_test, casino_preds, casino_proba, &#39;Casino&#39;), (y_test,xgb_test_preds, xgb_test_proba, &#39;XGBoost&#39;) ] cal_curve(data, 15) . Casino Accuracy:0.5840 Brier Loss: 0.2398 XGBoost Accuracy:0.6100 Brier Loss: 0.2399 . That&#39;s great. We&#39;ve got a model that 2.6% better than the casino&#39;s and just as well calibrated. The last thing I want to check is how closely our predictions are correlated to the casino&#39;s. The lower the correlation, the more bets we&#39;ll get to place. . from sklearn.metrics import r2_score r2_score(casino_proba, xgb_test_proba) . 0.7717587744033714 . That&#39;s great news. It looks like it&#39;s going to give us a lot of opportunities to bet. Let&#39;s save our model. We&#39;ll use it in Part 5. . import pickle pickle.dump(model,open(&#39;xgb_model.pkl&#39;,&#39;wb&#39;)) . Next Up . Part 5. Time move on to figuring out how to generate predictions and place bets. We&#39;ll use the Kelly criterion. .",
            "url": "https://rdpharr.github.io/project_notes/baseball/hyperopt/xgboost/2020/09/24/model-optimization.html",
            "relUrl": "/baseball/hyperopt/xgboost/2020/09/24/model-optimization.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "MLB Power Rankings and Casino Odds",
            "content": "MLB Baseball Prediction Series: Part 1 Part 2 Part 3 Part 4 Part 5 . | . Last time, we created a model that performs pretty well just based on the statistics that we downloaded from baseball-reference.com. In this post, we&#39;ll extend that model by adding power rankings and casino odds to the model. . Important: You can run this notebook from Colab or Binder using the buttons above, but you&#8217;ll also need the files we created in Part 2. . Power Rankings . The 538 Blog has famously modified the Elo system from chess to make their baseball rankings. The Elo system tries to determine the relative skill level of a player based on the skill levels of the other players encountered. If you beat a person with a high skill level, your skill level is going to improve more than if you win against a player of the same or lower skill level than you. And if your skill level is higher than your opponent&#39;s then you will probably win the match. . If 538 thinks Elo is foundational, then we should definitely put it in our model. The trouble is that not everyone agrees on how to implement it. In fact there are a whole family of different power ranking systems out there. In this project we&#39;re going to add four: 2 varieties of Elo (slow and fast changing), Glicko, and Trueskill. Luckily people have created libraries to help us get the code right. . Important: If you are running this notebook online, you may need to install the additional libraries. Here&#8217;s how to do it on Colab. . Let&#39;s get to it. We&#39;ll start by importing our dataframe from Part 2. . import pickle df = pickle.load(open(&quot;dataframe.pkl&quot;,&quot;rb&quot;)) . Elo Rankings . For Elo rankings, we&#39;re going to use the elote library, primarily because it&#39;s named after the Mexican habit of eating corn on the cob with mayonnaise (yuck!). Install it like this: . pip install elote . from elote import EloCompetitor ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=EloCompetitor() for x in df.away_team_abbr.unique(): ratings[x]=EloCompetitor() home_team_elo = [] away_team_elo = [] elo_exp = [] df = df.sort_values(by=&#39;date&#39;).reset_index(drop=True) for i, r in df.iterrows(): # get pre-game ratings elo_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr])) home_team_elo.append(ratings[r.home_team_abbr].rating) away_team_elo.append(ratings[r.away_team_abbr].rating) # update ratings if r.home_team_win: ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr]) else: ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr]) df[&#39;elo_exp&#39;] = elo_exp df[&#39;home_team_elo&#39;] = home_team_elo df[&#39;away_team_elo&#39;] = away_team_elo . Now we&#39;ll do the slow changing version, where we decrease the k-factor. . ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=EloCompetitor() ratings[x]._k_score=16 for x in df.away_team_abbr.unique(): ratings[x]=EloCompetitor() ratings[x]._k_score=16 home_team_elo = [] away_team_elo = [] elo_exp = [] df = df.sort_values(by=&#39;date&#39;).reset_index(drop=True) for i, r in df.iterrows(): # get pregame ratings elo_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr])) home_team_elo.append(ratings[r.home_team_abbr].rating) away_team_elo.append(ratings[r.away_team_abbr].rating) # update ratings if r.home_team_win: ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr]) else: ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr]) df[&#39;elo_slow_exp&#39;] = elo_exp df[&#39;home_team_elo_slow&#39;] = home_team_elo df[&#39;away_team_elo_slow&#39;] = away_team_elo . Glicko Ratings . Glicko can be calculated using the same library. . from elote import GlickoCompetitor ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=GlickoCompetitor() for x in df.away_team_abbr.unique(): ratings[x]=GlickoCompetitor() home_team_glick = [] away_team_glick = [] glick_exp = [] df = df.sort_values(by=&#39;date&#39;).reset_index(drop=True) for i, r in df.iterrows(): # get pregame ratings glick_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr])) home_team_glick.append(ratings[r.home_team_abbr].rating) away_team_glick.append(ratings[r.away_team_abbr].rating) # update ratings if r.home_team_win: ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr]) else: ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr]) df[&#39;glick_exp&#39;] = glick_exp df[&#39;home_team_glick&#39;] = home_team_glick df[&#39;away_team_glick&#39;] = away_team_glick . Trueskill Ratings . Trueskill was invented for Microsoft video games on the XBox. It&#39;s something you need to license if you are going to use it for commercial purposes. Trueskill ratings are a little bit more complex, because we have the opportunity to include the starting pitcher skill as well. Install the python package like this: . pip install trueskill . from trueskill import Rating, quality, rate ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=Rating(25) for x in df.away_team_abbr.unique(): ratings[x]=Rating(25) for x in df.home_pitcher.unique(): ratings[x]=Rating(25) for x in df.away_pitcher.unique(): ratings[x]=Rating(25) ts_quality = [] pitcher_ts_diff = [] team_ts_diff = [] home_pitcher_ts = [] away_pitcher_ts = [] home_team_ts = [] away_team_ts = [] df = df.sort_values(by=&#39;date&#39;).copy() for i, r in df.iterrows(): # get pre-match trueskill ratings from dict match = [(ratings[r.home_team_abbr], ratings[r.home_pitcher]), (ratings[r.away_team_abbr], ratings[r.away_pitcher])] ts_quality.append(quality(match)) pitcher_ts_diff.append(ratings[r.home_pitcher].mu-ratings[r.away_pitcher].mu) team_ts_diff.append(ratings[r.home_team_abbr].mu-ratings[r.away_team_abbr].mu) home_pitcher_ts.append(ratings[r.home_pitcher].mu) away_pitcher_ts.append(ratings[r.away_pitcher].mu) home_team_ts.append(ratings[r.home_team_abbr].mu) away_team_ts.append(ratings[r.away_team_abbr].mu) if r.date &lt; df.date.max(): # update ratings dictionary with post-match ratings if r.home_team_win==1: match = [(ratings[r.home_team_abbr], ratings[r.home_pitcher]), (ratings[r.away_team_abbr], ratings[r.away_pitcher])] [(ratings[r.home_team_abbr], ratings[r.home_pitcher]), (ratings[r.away_team_abbr], ratings[r.away_pitcher])] = rate(match) else: match = [(ratings[r.away_team_abbr], ratings[r.away_pitcher]), (ratings[r.home_team_abbr], ratings[r.home_pitcher])] [(ratings[r.away_team_abbr], ratings[r.away_pitcher]), (ratings[r.home_team_abbr], ratings[r.home_pitcher])] = rate(match) df[&#39;ts_game_quality&#39;] = ts_quality df[&#39;pitcher_ts_diff&#39;] = pitcher_ts_diff df[&#39;team_ts_diff&#39;] = team_ts_diff df[&#39;home_pitcher_ts&#39;] = home_pitcher_ts df[&#39;away_pitcher_ts&#39;] = away_pitcher_ts df[&#39;home_team_ts&#39;] = home_team_ts df[&#39;away_team_ts&#39;] = away_team_ts . That&#39;s all we need for power rankings. Let&#39;s move on. . Casino Odds . Having the casino odds in our model is really going to improve its predictions, but getting them in there is kind of a pain in the ass. The problem is that we need to match the games from two different systems (baseball-reference.com and covers.com). These systems don&#39;t use the same team abbreviations and don&#39;t even agree on what time the games started. So there&#39;s a bit of code to compensate. . But it starts the same as in Part 1 of this blog series - we need to find out which days to download odds data for. We&#39;ll use our dataframe to get this list. . import pandas as pd dates = pd.to_datetime(df[&#39;date&#39;], unit=&#39;s&#39;) game_days = dates.dt.strftime(&#39;%Y-%m-%d&#39;).unique() print(&quot;Days of odds data needed:&quot;, len(game_days)) . Days of odds data needed: 885 . The below code is largely the same from Part 1, except we are also grabbing team abbreviations from the data . import requests from bs4 import BeautifulSoup as bs import datetime as dt game_data = [] for d in game_days: # get the web page with game data on it url = f&#39;https://www.covers.com/Sports/MLB/Matchups?selectedDate={d}&#39; resp = requests.get(url) # parse the games scraped_games = bs(resp.text).findAll(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_game_box&#39;}) for g in scraped_games: game = {} game[&#39;home_moneyline&#39;] = g[&#39;data-game-odd&#39;] game[&#39;date&#39;] = g[&#39;data-game-date&#39;] game[&#39;away_team_abbr&#39;] = g[&#39;data-away-team-shortname-search&#39;] game[&#39;home_team_abbr&#39;] = g[&#39;data-home-team-shortname-search&#39;] try: game[&#39;home_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_home&#39;}).text.strip() game[&#39;away_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_away&#39;}).text.strip() except: game[&#39;home_score&#39;] =&#39;&#39; game[&#39;away_score&#39;] =&#39;&#39; game_data.append(game) if len(game_data) % 1000==0: #show progress print(dt.datetime.now(), d, len(game_data)) print(&quot;Done! Games downloaded:&quot;, len(game_data)) . 2020-09-23 16:52:21.794853 2016-06-15 1000 2020-09-23 16:53:17.556900 2016-08-31 2000 2020-09-23 16:54:28.987126 2017-05-10 3000 2020-09-23 16:55:25.810966 2017-07-24 4000 2020-09-23 16:56:35.138059 2018-03-29 5000 2020-09-23 16:57:33.919756 2018-06-10 6000 2020-09-23 16:58:54.173907 2018-08-26 7000 2020-09-23 17:00:18.965869 2019-05-01 8000 2020-09-23 17:01:23.090361 2019-07-17 9000 2020-09-23 17:02:27.603102 2019-09-29 10000 Done! Games downloaded: 10941 . So slow. Let&#39;s save this so we don&#39;t have to go through that again. . import pickle pickle.dump(game_data, open(&#39;covers_data_2.pkl&#39;,&#39;wb&#39;)) . We&#39;ll do some prepping and cleaning of the data. . import pickle game_data = pickle.load(open(&#39;covers_data_2.pkl&#39;,&#39;rb&#39;)) . import numpy as np import pandas as pd odds = pd.DataFrame(game_data) odds[&#39;home_moneyline&#39;].replace(&#39;&#39;, np.nan, inplace=True) odds.dropna(subset=[&#39;home_moneyline&#39;], inplace=True) odds.home_moneyline = pd.to_numeric(odds.home_moneyline) odds.date = pd.to_datetime(odds.date).dt.date . Now we convert the team names to be the same as baseball-reference.com . import warnings warnings.filterwarnings(&#39;ignore&#39;) warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) odds.home_team_abbr[odds.home_team_abbr==&#39;SF&#39;]=&#39;SFG&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;TB&#39;]=&#39;TBR&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;WAS&#39;]=&#39;WSN&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;KC&#39;]=&#39;KCR&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;SD&#39;]=&#39;SDP&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;SF&#39;]=&#39;SFG&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;TB&#39;]=&#39;TBR&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;WAS&#39;]=&#39;WSN&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;KC&#39;]=&#39;KCR&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;SD&#39;]=&#39;SDP&#39; . Finally, convert the moneyline odds to probabilities . odds[&#39;odds_proba&#39;]=np.nan odds[&#39;odds_proba&#39;][odds.home_moneyline&lt;0] = -odds.home_moneyline/(-odds.home_moneyline + 100) odds[&#39;odds_proba&#39;][odds.home_moneyline&gt;0] = (100/(odds.home_moneyline + 100)) . Because the game times aren&#39;t exact, we&#39;ll use pandas merge_asof to find the closest match. The syntax is that you the fields in &quot;by&quot; parameter need to be exact, and it will find the closest by the &quot;on&quot; parameter. I think this feature is awesome, and another reason I love pandas. . print(&#39;dataframe shape before merge:&#39;, df.shape) # get dates into the same format odds[&#39;date&#39;] = (pd.to_datetime(pd.to_datetime(odds[&#39;date&#39;])) - pd.Timestamp(&quot;1970-01-01&quot;)) // pd.Timedelta(&#39;1s&#39;) # do the merge df = pd.merge_asof(left=df.sort_values(by=&#39;date&#39;), right=odds[[&#39;home_team_abbr&#39;,&#39;date&#39;, &#39;away_team_abbr&#39;,&#39;odds_proba&#39;]].sort_values(by=&#39;date&#39;), by=[&#39;home_team_abbr&#39;,&#39;away_team_abbr&#39;], on=&#39;date&#39;) df = df.sort_values(by=&#39;date&#39;).copy().reset_index(drop=True) print(&#39;dataframe shape after merge:&#39;, df.shape) . dataframe shape before merge: (10535, 1052) dataframe shape after merge: (10535, 1053) . Things look good now. Let&#39;s save this dataframe before we move on . import pickle pickle.dump(df, open(&#39;dataframe_part3.pkl&#39;,&#39;wb&#39;)) . Run The Model . This is almost the exact code we ran in part 2 . import pickle df = pickle.load(open(&#39;dataframe_part3.pkl&#39;,&#39;rb&#39;)) . import xgboost as xgb # target encoding encode_me = [x for x in df.keys() if &#39;object&#39; in str(df[x].dtype)] for x in encode_me: df[x] = df.groupby(x)[&#39;home_team_win&#39;].apply(lambda x:x.rolling(180).mean()).shift(1) # create test, train splits df = df.sort_values(by=&#39;date&#39;).copy().reset_index(drop=True) X = df.drop(columns=[&#39;home_team_win&#39;, &#39;game_id&#39;]) y = df.home_team_win X_train = X[:-1000] y_train = y[:-1000] X_valid = X[-1000:-500] y_valid = y[-1000:-500] X_test = X[-500:] y_test = y[-500:] . Run the model . params = {&#39;learning_rate&#39;: 0.035,&#39;max_depth&#39;: 1} gbm = xgb.XGBClassifier(**params) model = gbm.fit(X_train, y_train, eval_set = [[X_train, y_train], [X_valid, y_valid]], eval_metric=&#39;logloss&#39;, early_stopping_rounds=10) xgb_test_preds = model.predict(X_test) xgb_test_proba = model.predict_proba(X_test)[:,1] . [0] validation_0-logloss:0.69210 validation_1-logloss:0.69159 Multiple eval metrics have been passed: &#39;validation_1-logloss&#39; will be used for early stopping. Will train until validation_1-logloss hasn&#39;t improved in 10 rounds. [1] validation_0-logloss:0.69111 validation_1-logloss:0.69047 [2] validation_0-logloss:0.69019 validation_1-logloss:0.68902 [3] validation_0-logloss:0.68932 validation_1-logloss:0.68802 [4] validation_0-logloss:0.68850 validation_1-logloss:0.68670 [5] validation_0-logloss:0.68773 validation_1-logloss:0.68580 [6] validation_0-logloss:0.68699 validation_1-logloss:0.68471 [7] validation_0-logloss:0.68630 validation_1-logloss:0.68360 [8] validation_0-logloss:0.68564 validation_1-logloss:0.68261 [9] validation_0-logloss:0.68502 validation_1-logloss:0.68188 [10] validation_0-logloss:0.68443 validation_1-logloss:0.68097 [11] validation_0-logloss:0.68385 validation_1-logloss:0.68031 [12] validation_0-logloss:0.68332 validation_1-logloss:0.67944 [13] validation_0-logloss:0.68279 validation_1-logloss:0.67883 [14] validation_0-logloss:0.68230 validation_1-logloss:0.67825 [15] validation_0-logloss:0.68183 validation_1-logloss:0.67750 [16] validation_0-logloss:0.68137 validation_1-logloss:0.67700 [17] validation_0-logloss:0.68094 validation_1-logloss:0.67663 [18] validation_0-logloss:0.68052 validation_1-logloss:0.67595 [19] validation_0-logloss:0.68012 validation_1-logloss:0.67521 [20] validation_0-logloss:0.67974 validation_1-logloss:0.67488 [21] validation_0-logloss:0.67937 validation_1-logloss:0.67428 [22] validation_0-logloss:0.67902 validation_1-logloss:0.67389 [23] validation_0-logloss:0.67869 validation_1-logloss:0.67324 [24] validation_0-logloss:0.67837 validation_1-logloss:0.67297 [25] validation_0-logloss:0.67806 validation_1-logloss:0.67245 [26] validation_0-logloss:0.67776 validation_1-logloss:0.67209 [27] validation_0-logloss:0.67748 validation_1-logloss:0.67178 [28] validation_0-logloss:0.67720 validation_1-logloss:0.67133 [29] validation_0-logloss:0.67694 validation_1-logloss:0.67102 [30] validation_0-logloss:0.67669 validation_1-logloss:0.67060 [31] validation_0-logloss:0.67644 validation_1-logloss:0.67038 [32] validation_0-logloss:0.67621 validation_1-logloss:0.67012 [33] validation_0-logloss:0.67598 validation_1-logloss:0.66963 [34] validation_0-logloss:0.67576 validation_1-logloss:0.66926 [35] validation_0-logloss:0.67554 validation_1-logloss:0.66903 [36] validation_0-logloss:0.67534 validation_1-logloss:0.66904 [37] validation_0-logloss:0.67514 validation_1-logloss:0.66879 [38] validation_0-logloss:0.67494 validation_1-logloss:0.66844 [39] validation_0-logloss:0.67476 validation_1-logloss:0.66821 [40] validation_0-logloss:0.67458 validation_1-logloss:0.66824 [41] validation_0-logloss:0.67440 validation_1-logloss:0.66801 [42] validation_0-logloss:0.67423 validation_1-logloss:0.66771 [43] validation_0-logloss:0.67406 validation_1-logloss:0.66755 [44] validation_0-logloss:0.67390 validation_1-logloss:0.66737 [45] validation_0-logloss:0.67374 validation_1-logloss:0.66729 [46] validation_0-logloss:0.67359 validation_1-logloss:0.66719 [47] validation_0-logloss:0.67344 validation_1-logloss:0.66708 [48] validation_0-logloss:0.67329 validation_1-logloss:0.66713 [49] validation_0-logloss:0.67315 validation_1-logloss:0.66707 [50] validation_0-logloss:0.67302 validation_1-logloss:0.66689 [51] validation_0-logloss:0.67288 validation_1-logloss:0.66676 [52] validation_0-logloss:0.67275 validation_1-logloss:0.66660 [53] validation_0-logloss:0.67262 validation_1-logloss:0.66656 [54] validation_0-logloss:0.67250 validation_1-logloss:0.66644 [55] validation_0-logloss:0.67238 validation_1-logloss:0.66635 [56] validation_0-logloss:0.67226 validation_1-logloss:0.66641 [57] validation_0-logloss:0.67214 validation_1-logloss:0.66628 [58] validation_0-logloss:0.67203 validation_1-logloss:0.66614 [59] validation_0-logloss:0.67192 validation_1-logloss:0.66612 [60] validation_0-logloss:0.67182 validation_1-logloss:0.66599 [61] validation_0-logloss:0.67171 validation_1-logloss:0.66591 [62] validation_0-logloss:0.67161 validation_1-logloss:0.66578 [63] validation_0-logloss:0.67151 validation_1-logloss:0.66569 [64] validation_0-logloss:0.67142 validation_1-logloss:0.66567 [65] validation_0-logloss:0.67132 validation_1-logloss:0.66575 [66] validation_0-logloss:0.67123 validation_1-logloss:0.66582 [67] validation_0-logloss:0.67114 validation_1-logloss:0.66571 [68] validation_0-logloss:0.67105 validation_1-logloss:0.66577 [69] validation_0-logloss:0.67096 validation_1-logloss:0.66594 [70] validation_0-logloss:0.67088 validation_1-logloss:0.66593 [71] validation_0-logloss:0.67079 validation_1-logloss:0.66582 [72] validation_0-logloss:0.67071 validation_1-logloss:0.66575 [73] validation_0-logloss:0.67063 validation_1-logloss:0.66565 [74] validation_0-logloss:0.67055 validation_1-logloss:0.66571 [75] validation_0-logloss:0.67048 validation_1-logloss:0.66578 [76] validation_0-logloss:0.67040 validation_1-logloss:0.66594 [77] validation_0-logloss:0.67033 validation_1-logloss:0.66598 [78] validation_0-logloss:0.67025 validation_1-logloss:0.66589 [79] validation_0-logloss:0.67018 validation_1-logloss:0.66595 [80] validation_0-logloss:0.67011 validation_1-logloss:0.66587 [81] validation_0-logloss:0.67005 validation_1-logloss:0.66573 [82] validation_0-logloss:0.66998 validation_1-logloss:0.66580 [83] validation_0-logloss:0.66991 validation_1-logloss:0.66585 Stopping. Best iteration: [73] validation_0-logloss:0.67063 validation_1-logloss:0.66565 . Since we now know the casino odds for these specific games, we can directly compare our model preditions with on the same games. Peviously we&#39;d been comparing our results against how the casino predicted 2019 games. . from sklearn.calibration import calibration_curve from sklearn.metrics import accuracy_score, brier_score_loss import matplotlib.pyplot as plt import pickle def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 8)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_pred, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} t tAccuracy:{:.4f} t Brier Loss: {:.4f}&quot;.format( name, accuracy_score(y_test, y_pred), brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() casino_proba = X_test[&#39;odds_proba&#39;] casino_preds = X_test[&#39;odds_proba&#39;]&gt;.5 data = [ (y_test, casino_preds, casino_proba, &#39;Casino&#39;), (y_test,xgb_test_preds, xgb_test_proba, &#39;XGBoost&#39;) ] cal_curve(data, 15) . . Casino Accuracy:0.5840 Brier Loss: 0.2401 XGBoost Accuracy:0.5900 Brier Loss: 0.2406 . Now we&#39;re talking. Our accuracy in our test data is 0.6% better than the oddsmakers and our calibration is virtually the same. It&#39;s a concern that to get these results, I needed to set the max depth to 1. That is very low, and implies that we are susceptible to overfitting. One way to fix that is to get more data... . Next up . In Part 4, we&#39;re going to train this thing for real, and try to squeak out a few more % through downloading more data and hyperparameter optimization. You may want to run the next notebook overnight... .",
            "url": "https://rdpharr.github.io/project_notes/baseball/webscraping/elo/trueskill/glick/2020/09/22/power-rankings-and-casino-odds.html",
            "relUrl": "/baseball/webscraping/elo/trueskill/glick/2020/09/22/power-rankings-and-casino-odds.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "MLB Training Data",
            "content": "MLB Baseball Prediction Series: Part 1 Part 2 Part 3 Part 4 Part 5 . | . This is the second post in my series on MLB Baseball Betting, and it&#39;s a long one. Here we&#39;re going to have a complete working model from scraping the data, processing it, adding features and finally giving it a run in XGBoost. . This is all done in a single notebook so you can start fiddling with it on your own as soon as you want. Use the buttons on the top to download your own copy from github or run it in the cloud on one of the services listed. . Scrape Data . For this model, I&#39;m going to use the stats from the baseball-reference.com box scores page. Here is and example of the page we&#39;ll be scraping. Let&#39;s get started. . Create List of Games to Download . We&#39;ll get the games from the schedule pages, creating a list of links to each game. Let&#39;s start in 2016, that should give us enough games to start making inferences. . import requests from bs4 import BeautifulSoup as bs game_links = [] for current_year in range(2016,2021): url = f&quot;https://www.baseball-reference.com/leagues/MLB/{current_year}-schedule.shtml&quot; resp = requests.get(url) soup=bs(resp.text) games = soup.findAll(&#39;a&#39;,text=&#39;Boxscore&#39;) game_links.extend([x[&#39;href&#39;] for x in games]) print(&quot;Number of games to download: &quot;, len(game_links)) game_links[0] . Number of games to download: 10684 . &#39;/boxes/KCA/KCA201604030.shtml&#39; . Download Game Data . For each game, I want to download team performance in batting and pitching, as well as individual performance pitching. For 10K games, this is going to take a little while to build out. . # these are functions related to parsing the baseball reference page def get_game_summary(soup, game_id): game = {&#39;game_id&#39;: game_id} scorebox = soup.find(&#39;div&#39;, {&#39;class&#39;:&#39;scorebox&#39;}) teams = scorebox.findAll(&#39;a&#39;,{&#39;itemprop&#39;:&#39;name&#39;}) game[&#39;away_team_abbr&#39;] = teams[0][&#39;href&#39;].split(&#39;/&#39;)[2] game[&#39;home_team_abbr&#39;] = teams[1][&#39;href&#39;].split(&#39;/&#39;)[2] meta = scorebox.find(&#39;div&#39;, {&#39;class&#39;:&#39;scorebox_meta&#39;}).findAll(&#39;div&#39;) game[&#39;date&#39;] = meta[0].text.strip() game[&#39;start_time&#39;] = meta[1].text[12:-6].strip() return game def get_table_summary(soup, table_no): stats_tables = soup.findAll(&#39;table&#39;, {&#39;class&#39;:&#39;stats_table&#39;}) t = stats_tables[table_no].find(&#39;tfoot&#39;) summary = {x[&#39;data-stat&#39;]:x.text.strip() for x in t.findAll(&#39;td&#39;)} return summary def get_pitcher_data(soup, table_no): stats_tables = soup.findAll(&#39;table&#39;, {&#39;class&#39;:&#39;stats_table&#39;}) t = stats_tables[table_no] data = [] rows = t.findAll(&#39;tr&#39;)[1:-1] # not the header and footer rows for r in rows: summary = {x[&#39;data-stat&#39;]:x.text.strip() for x in r.findAll(&#39;td&#39;)} summary[&#39;name&#39;] = r.find(&#39;th&#39;,{&#39;data-stat&#39;:&#39;player&#39;}).find(&#39;a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1][:-6].strip() data.append(summary) return data def process_link(url): resp = requests.get(url) game_id = url.split(&#39;/&#39;)[-1][:-6] # strange preprocessing routine uncommented_html = &#39;&#39; for h in resp.text.split(&#39; n&#39;): if &#39;&lt;!-- &lt;div&#39; in h: continue if h.strip() == &#39;&lt;!--&#39;: continue if h.strip() == &#39;--&gt;&#39;: continue uncommented_html += h + &#39; n&#39; soup = bs(uncommented_html) data = { &#39;game&#39;: get_game_summary(soup, game_id), &#39;away_batting&#39;: get_table_summary(soup, 1), &#39;home_batting&#39;:get_table_summary(soup, 2), &#39;away_pitching&#39;:get_table_summary(soup, 3), &#39;home_pitching&#39;:get_table_summary(soup, 4), &#39;away_pitchers&#39;: get_pitcher_data(soup, 3), &#39;home_pitchers&#39;: get_pitcher_data(soup, 4) } return data . . import datetime as dt game_data = [] for link in game_links: url = &#39;https://www.baseball-reference.com&#39; + link game_data.append(process_link(url)) if len(game_data)%1000==0: print(dt.datetime.now().time(), len(game_data)) . 14:24:15.188000 1000 14:34:20.590050 2000 14:44:37.326441 3000 14:54:50.018196 4000 15:05:12.944858 5000 15:15:18.664457 6000 15:25:07.445247 7000 15:35:07.049830 8000 15:44:55.807973 9000 15:54:50.771393 10000 . That took a while. We could definitely speed it up with threading, but we want to be nice to their servers. They seem like good people, making their data available to everyone. . Here is what a single game looks like. It&#39;s actually quite a bit of data. . game_data[0] . {&#39;game&#39;: {&#39;game_id&#39;: &#39;KCA201604030&#39;, &#39;away_team_abbr&#39;: &#39;NYM&#39;, &#39;home_team_abbr&#39;: &#39;KCR&#39;, &#39;date&#39;: &#39;Sunday, April 3, 2016&#39;, &#39;start_time&#39;: &#39;7:38 p.m.&#39;}, &#39;away_batting&#39;: {&#39;AB&#39;: &#39;33&#39;, &#39;R&#39;: &#39;3&#39;, &#39;H&#39;: &#39;7&#39;, &#39;RBI&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;6&#39;, &#39;SO&#39;: &#39;9&#39;, &#39;PA&#39;: &#39;39&#39;, &#39;batting_avg&#39;: &#39;.212&#39;, &#39;onbase_perc&#39;: &#39;.333&#39;, &#39;slugging_perc&#39;: &#39;.242&#39;, &#39;onbase_plus_slugging&#39;: &#39;.576&#39;, &#39;pitches&#39;: &#39;177&#39;, &#39;strikes_total&#39;: &#39;105&#39;, &#39;wpa_bat&#39;: &#39;-0.449&#39;, &#39;leverage_index_avg&#39;: &#39;1.58&#39;, &#39;wpa_bat_pos&#39;: &#39;0.746&#39;, &#39;wpa_bat_neg&#39;: &#39;-1.195&#39;, &#39;re24_bat&#39;: &#39;-1.7&#39;, &#39;PO&#39;: &#39;24&#39;, &#39;A&#39;: &#39;15&#39;, &#39;details&#39;: &#39;&#39;}, &#39;home_batting&#39;: {&#39;AB&#39;: &#39;30&#39;, &#39;R&#39;: &#39;4&#39;, &#39;H&#39;: &#39;9&#39;, &#39;RBI&#39;: &#39;4&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;3&#39;, &#39;PA&#39;: &#39;33&#39;, &#39;batting_avg&#39;: &#39;.300&#39;, &#39;onbase_perc&#39;: &#39;.333&#39;, &#39;slugging_perc&#39;: &#39;.300&#39;, &#39;onbase_plus_slugging&#39;: &#39;.633&#39;, &#39;pitches&#39;: &#39;114&#39;, &#39;strikes_total&#39;: &#39;71&#39;, &#39;wpa_bat&#39;: &#39;0.052&#39;, &#39;leverage_index_avg&#39;: &#39;0.74&#39;, &#39;wpa_bat_pos&#39;: &#39;0.488&#39;, &#39;wpa_bat_neg&#39;: &#39;-0.434&#39;, &#39;re24_bat&#39;: &#39;-0.1&#39;, &#39;PO&#39;: &#39;27&#39;, &#39;A&#39;: &#39;13&#39;, &#39;details&#39;: &#39;&#39;}, &#39;away_pitching&#39;: {&#39;IP&#39;: &#39;8&#39;, &#39;H&#39;: &#39;9&#39;, &#39;R&#39;: &#39;4&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;3&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;3.38&#39;, &#39;batters_faced&#39;: &#39;33&#39;, &#39;pitches&#39;: &#39;114&#39;, &#39;strikes_total&#39;: &#39;71&#39;, &#39;strikes_contact&#39;: &#39;42&#39;, &#39;strikes_swinging&#39;: &#39;7&#39;, &#39;strikes_looking&#39;: &#39;22&#39;, &#39;inplay_gb_total&#39;: &#39;18&#39;, &#39;inplay_fb_total&#39;: &#39;10&#39;, &#39;inplay_ld&#39;: &#39;5&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;39&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;1&#39;, &#39;wpa_def&#39;: &#39;-0.051&#39;, &#39;leverage_index_avg&#39;: &#39;0.74&#39;, &#39;re24_def&#39;: &#39;0.1&#39;}, &#39;home_pitching&#39;: {&#39;IP&#39;: &#39;9&#39;, &#39;H&#39;: &#39;7&#39;, &#39;R&#39;: &#39;3&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;6&#39;, &#39;SO&#39;: &#39;9&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;3.00&#39;, &#39;batters_faced&#39;: &#39;39&#39;, &#39;pitches&#39;: &#39;177&#39;, &#39;strikes_total&#39;: &#39;106&#39;, &#39;strikes_contact&#39;: &#39;59&#39;, &#39;strikes_swinging&#39;: &#39;21&#39;, &#39;strikes_looking&#39;: &#39;26&#39;, &#39;inplay_gb_total&#39;: &#39;16&#39;, &#39;inplay_fb_total&#39;: &#39;8&#39;, &#39;inplay_ld&#39;: &#39;5&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;70&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.449&#39;, &#39;leverage_index_avg&#39;: &#39;1.58&#39;, &#39;re24_def&#39;: &#39;1.7&#39;}, &#39;away_pitchers&#39;: [{&#39;IP&#39;: &#39;5.2&#39;, &#39;H&#39;: &#39;8&#39;, &#39;R&#39;: &#39;4&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;2&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;4.76&#39;, &#39;batters_faced&#39;: &#39;25&#39;, &#39;pitches&#39;: &#39;83&#39;, &#39;strikes_total&#39;: &#39;51&#39;, &#39;strikes_contact&#39;: &#39;32&#39;, &#39;strikes_swinging&#39;: &#39;6&#39;, &#39;strikes_looking&#39;: &#39;13&#39;, &#39;inplay_gb_total&#39;: &#39;13&#39;, &#39;inplay_fb_total&#39;: &#39;8&#39;, &#39;inplay_ld&#39;: &#39;5&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;39&#39;, &#39;inherited_runners&#39;: &#39;&#39;, &#39;inherited_score&#39;: &#39;&#39;, &#39;wpa_def&#39;: &#39;-0.061&#39;, &#39;leverage_index_avg&#39;: &#39;0.86&#39;, &#39;re24_def&#39;: &#39;-0.4&#39;, &#39;name&#39;: &#39;harvema01&#39;}, {&#39;IP&#39;: &#39;1.1&#39;, &#39;H&#39;: &#39;1&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;1&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;5&#39;, &#39;pitches&#39;: &#39;20&#39;, &#39;strikes_total&#39;: &#39;13&#39;, &#39;strikes_contact&#39;: &#39;7&#39;, &#39;strikes_swinging&#39;: &#39;0&#39;, &#39;strikes_looking&#39;: &#39;6&#39;, &#39;inplay_gb_total&#39;: &#39;3&#39;, &#39;inplay_fb_total&#39;: &#39;1&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;1&#39;, &#39;wpa_def&#39;: &#39;-0.022&#39;, &#39;leverage_index_avg&#39;: &#39;0.25&#39;, &#39;re24_def&#39;: &#39;0.0&#39;, &#39;name&#39;: &#39;colonba01&#39;}, {&#39;IP&#39;: &#39;1&#39;, &#39;H&#39;: &#39;0&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;0&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;3&#39;, &#39;pitches&#39;: &#39;11&#39;, &#39;strikes_total&#39;: &#39;7&#39;, &#39;strikes_contact&#39;: &#39;3&#39;, &#39;strikes_swinging&#39;: &#39;1&#39;, &#39;strikes_looking&#39;: &#39;3&#39;, &#39;inplay_gb_total&#39;: &#39;2&#39;, &#39;inplay_fb_total&#39;: &#39;1&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.032&#39;, &#39;leverage_index_avg&#39;: &#39;0.42&#39;, &#39;re24_def&#39;: &#39;0.5&#39;, &#39;name&#39;: &#39;blevije01&#39;}], &#39;home_pitchers&#39;: [{&#39;IP&#39;: &#39;6&#39;, &#39;H&#39;: &#39;2&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;3&#39;, &#39;SO&#39;: &#39;5&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;22&#39;, &#39;pitches&#39;: &#39;106&#39;, &#39;strikes_total&#39;: &#39;62&#39;, &#39;strikes_contact&#39;: &#39;32&#39;, &#39;strikes_swinging&#39;: &#39;14&#39;, &#39;strikes_looking&#39;: &#39;16&#39;, &#39;inplay_gb_total&#39;: &#39;11&#39;, &#39;inplay_fb_total&#39;: &#39;3&#39;, &#39;inplay_ld&#39;: &#39;2&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;70&#39;, &#39;inherited_runners&#39;: &#39;&#39;, &#39;inherited_score&#39;: &#39;&#39;, &#39;wpa_def&#39;: &#39;0.350&#39;, &#39;leverage_index_avg&#39;: &#39;0.92&#39;, &#39;re24_def&#39;: &#39;3.1&#39;, &#39;name&#39;: &#39;volqued01&#39;}, {&#39;IP&#39;: &#39;1&#39;, &#39;H&#39;: &#39;1&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;0&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;4&#39;, &#39;pitches&#39;: &#39;12&#39;, &#39;strikes_total&#39;: &#39;7&#39;, &#39;strikes_contact&#39;: &#39;5&#39;, &#39;strikes_swinging&#39;: &#39;1&#39;, &#39;strikes_looking&#39;: &#39;1&#39;, &#39;inplay_gb_total&#39;: &#39;1&#39;, &#39;inplay_fb_total&#39;: &#39;3&#39;, &#39;inplay_ld&#39;: &#39;1&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.030&#39;, &#39;leverage_index_avg&#39;: &#39;0.59&#39;, &#39;re24_def&#39;: &#39;0.5&#39;, &#39;name&#39;: &#39;herreke01&#39;}, {&#39;IP&#39;: &#39;0.2&#39;, &#39;H&#39;: &#39;3&#39;, &#39;R&#39;: &#39;3&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;1&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;40.50&#39;, &#39;batters_faced&#39;: &#39;7&#39;, &#39;pitches&#39;: &#39;29&#39;, &#39;strikes_total&#39;: &#39;16&#39;, &#39;strikes_contact&#39;: &#39;10&#39;, &#39;strikes_swinging&#39;: &#39;2&#39;, &#39;strikes_looking&#39;: &#39;4&#39;, &#39;inplay_gb_total&#39;: &#39;2&#39;, &#39;inplay_fb_total&#39;: &#39;2&#39;, &#39;inplay_ld&#39;: &#39;2&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;-0.203&#39;, &#39;leverage_index_avg&#39;: &#39;1.80&#39;, &#39;re24_def&#39;: &#39;-2.9&#39;, &#39;name&#39;: &#39;soriajo01&#39;}, {&#39;IP&#39;: &#39;0.1&#39;, &#39;H&#39;: &#39;0&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;1&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;1&#39;, &#39;pitches&#39;: &#39;4&#39;, &#39;strikes_total&#39;: &#39;3&#39;, &#39;strikes_contact&#39;: &#39;1&#39;, &#39;strikes_swinging&#39;: &#39;2&#39;, &#39;strikes_looking&#39;: &#39;0&#39;, &#39;inplay_gb_total&#39;: &#39;0&#39;, &#39;inplay_fb_total&#39;: &#39;0&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.106&#39;, &#39;leverage_index_avg&#39;: &#39;4.08&#39;, &#39;re24_def&#39;: &#39;0.4&#39;, &#39;name&#39;: &#39;hochelu01&#39;}, {&#39;IP&#39;: &#39;1&#39;, &#39;H&#39;: &#39;1&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;1&#39;, &#39;SO&#39;: &#39;2&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;5&#39;, &#39;pitches&#39;: &#39;26&#39;, &#39;strikes_total&#39;: &#39;18&#39;, &#39;strikes_contact&#39;: &#39;11&#39;, &#39;strikes_swinging&#39;: &#39;2&#39;, &#39;strikes_looking&#39;: &#39;5&#39;, &#39;inplay_gb_total&#39;: &#39;2&#39;, &#39;inplay_fb_total&#39;: &#39;0&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.166&#39;, &#39;leverage_index_avg&#39;: &#39;4.62&#39;, &#39;re24_def&#39;: &#39;0.5&#39;, &#39;name&#39;: &#39;daviswa01&#39;}]} . Let&#39;s save our work, so we don&#39;t have to do that download again. . Important: We will be using the files saved in subsequent blog posts. Be sure to save them locally if you run this notebook online. . import pickle pickle.dump(game_data, open(&#39;game_data.pkl&#39;, &#39;wb&#39;)) . Prep Data for Modelling . The idea behind the model is that we care about the difference in the stats for our two opposing teams. So if the home team starting pitcher usually gets a lot more strikeouts than the away team&#39;s pitcher, that&#39;s the number we want to feed the model. As you saw above, we&#39;re going to be doing this for several stats. . Load data into DataFrames . The first task is to load our data into dataframes. We&#39;re going to end up with 4: . game_df: This will be our main dataframe that will eventually get fed into the model | pitching_df: This holds the pitching data, two rows per game (one for how team, one for away team). It&#39;s constructed this way because we need to be able to group our data per team. | batting_df: Same as above but for batting stats. | pitcher_df: This is the same as pitching_df, but has one row per player. We&#39;ll derive our starting pitcher stats from here. | . First, let&#39;s load our saved work. . import pickle game_data = pickle.load(open(&#39;game_data.pkl&#39;, &#39;rb&#39;)) . import pandas as pd games = [] batting = [] pitching = [] pitchers = [] for g in game_data: game_summary = g[&#39;game&#39;] # fix date game_summary[&#39;date&#39;] = game_summary[&#39;date&#39;] + &quot; &quot; + game_summary[&#39;start_time&#39;] del game_summary[&#39;start_time&#39;] # get starting pitchers game_summary[&#39;home_pitcher&#39;] = g[&#39;home_pitchers&#39;][0][&#39;name&#39;] game_summary[&#39;away_pitcher&#39;] = g[&#39;away_pitchers&#39;][0][&#39;name&#39;] # this is the field we&#39;ll train our model to predict game_summary[&#39;home_team_win&#39;] = int(g[&#39;home_batting&#39;][&#39;R&#39;])&gt;int(g[&#39;away_batting&#39;][&#39;R&#39;]) games.append(game_summary) # add all stats to appropriate lists target_pairs = [ (&#39;away_batting&#39;, batting), (&#39;home_batting&#39;, batting), (&#39;away_pitching&#39;, pitching), (&#39;home_pitching&#39;, pitching), (&#39;away_pitchers&#39;, pitchers), (&#39;home_pitchers&#39;, pitchers) ] for key, d in target_pairs: if isinstance(g[key], list): # pitchers for x in g[key]: if &#39;home&#39; in key: x[&#39;is_home_team&#39;] = True x[&#39;team&#39;] = g[&#39;game&#39;][&#39;home_team_abbr&#39;] else: x[&#39;is_home_team&#39;] = False x[&#39;team&#39;] = g[&#39;game&#39;][&#39;away_team_abbr&#39;] x[&#39;game_id&#39;] = g[&#39;game&#39;][&#39;game_id&#39;] d.append(x) else: #batting, pitching x = g[key] if &#39;home&#39; in key: x[&#39;is_home_team&#39;] = True x[&#39;team&#39;] = g[&#39;game&#39;][&#39;home_team_abbr&#39;] x[&#39;spread&#39;] = int(g[key][&#39;R&#39;]) - int(g[key.replace(&#39;home&#39;,&#39;away&#39;)][&#39;R&#39;]) else: x[&#39;is_home_team&#39;] = False x[&#39;team&#39;] = g[&#39;game&#39;][&#39;away_team_abbr&#39;] x[&#39;spread&#39;] = int(g[key][&#39;R&#39;]) - int(g[key.replace(&#39;away&#39;,&#39;home&#39;)][&#39;R&#39;]) x[&#39;game_id&#39;] = g[&#39;game&#39;][&#39;game_id&#39;] d.append(x) len(games), len(batting), len(pitching), len(pitchers) . (10684, 21368, 21368, 92026) . Game DF . This one is where we&#39;ll eventually put all of our stats . game_df = pd.DataFrame(games) #TODO: fix games that were rescheduled which become NaT after this next command game_df[&#39;date&#39;] = pd.to_datetime(game_df[&#39;date&#39;], errors=&#39;coerce&#39;) game_df = game_df[~game_df[&#39;game_id&#39;].str.contains(&#39;allstar&#39;)].copy() #don&#39;t care about allstar games game_df.head() . game_id away_team_abbr home_team_abbr date home_pitcher away_pitcher home_team_win . 0 KCA201604030 | NYM | KCR | 2016-04-03 19:38:00 | volqued01 | harvema01 | True | . 1 PIT201604030 | STL | PIT | 2016-04-03 13:15:00 | liriafr01 | wainwad01 | True | . 2 TBA201604030 | TOR | TBR | 2016-04-03 16:09:00 | archech01 | stromma01 | False | . 3 ANA201604040 | CHC | LAA | 2016-04-04 19:08:00 | richaga01 | arrieja01 | False | . 4 ARI201604040 | COL | ARI | 2016-04-04 18:42:00 | greinza01 | rosajo01 | False | . Batting DF . Stats about batting, one row per team per game . batting_df = pd.DataFrame(batting) for k in batting_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue batting_df[k] =pd.to_numeric(batting_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) batting_df.drop(columns=[&#39;details&#39;], inplace=True) batting_df.head() . AB R H RBI BB SO PA batting_avg onbase_perc slugging_perc ... leverage_index_avg wpa_bat_pos wpa_bat_neg re24_bat PO A is_home_team team spread game_id . 0 33.0 | 3.0 | 7.0 | 3.0 | 6.0 | 9.0 | 39.0 | 0.212 | 0.333 | 0.242 | ... | 1.58 | 0.746 | -1.195 | -1.7 | 24.0 | 15.0 | False | NYM | -1.0 | KCA201604030 | . 1 30.0 | 4.0 | 9.0 | 4.0 | 2.0 | 3.0 | 33.0 | 0.300 | 0.333 | 0.300 | ... | 0.74 | 0.488 | -0.434 | -0.1 | 27.0 | 13.0 | True | KCR | 1.0 | KCA201604030 | . 2 32.0 | 1.0 | 5.0 | 1.0 | 5.0 | 14.0 | 38.0 | 0.156 | 0.289 | 0.156 | ... | 1.27 | 0.504 | -0.935 | -3.4 | 24.0 | 11.0 | False | STL | -3.0 | PIT201604030 | . 3 28.0 | 4.0 | 9.0 | 4.0 | 5.0 | 5.0 | 36.0 | 0.321 | 0.429 | 0.464 | ... | 0.71 | 0.466 | -0.394 | 0.1 | 27.0 | 8.0 | True | PIT | 3.0 | PIT201604030 | . 4 35.0 | 5.0 | 7.0 | 5.0 | 3.0 | 16.0 | 38.0 | 0.200 | 0.263 | 0.314 | ... | 0.76 | 0.558 | -0.423 | 0.7 | 27.0 | 15.0 | False | TOR | 2.0 | TBA201604030 | . 5 rows × 24 columns . Pitching DF . Team pitching stats, one row per team per game . pitching_df = pd.DataFrame(pitching) for k in pitching_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue pitching_df[k] =pd.to_numeric(pitching_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) pitching_df.head() . IP H R ER BB SO HR earned_run_avg batters_faced pitches ... game_score inherited_runners inherited_score wpa_def leverage_index_avg re24_def is_home_team team spread game_id . 0 8.0 | 9.0 | 4.0 | 3.0 | 2.0 | 3.0 | 0.0 | 3.38 | 33.0 | 114.0 | ... | 39.0 | 2.0 | 1.0 | -0.051 | 0.74 | 0.1 | False | NYM | 1.0 | KCA201604030 | . 1 9.0 | 7.0 | 3.0 | 3.0 | 6.0 | 9.0 | 0.0 | 3.00 | 39.0 | 177.0 | ... | 70.0 | 2.0 | 0.0 | 0.449 | 1.58 | 1.7 | True | KCR | -1.0 | KCA201604030 | . 2 8.0 | 9.0 | 4.0 | 4.0 | 5.0 | 5.0 | 0.0 | 4.50 | 36.0 | 144.0 | ... | 48.0 | 0.0 | 0.0 | -0.069 | 0.71 | -0.1 | False | STL | 3.0 | PIT201604030 | . 3 9.0 | 5.0 | 1.0 | 1.0 | 5.0 | 14.0 | 0.0 | 1.00 | 38.0 | 141.0 | ... | 71.0 | 0.0 | 0.0 | 0.431 | 1.27 | 3.4 | True | PIT | -3.0 | PIT201604030 | . 4 9.0 | 7.0 | 3.0 | 3.0 | 1.0 | 7.0 | 1.0 | 3.00 | 36.0 | 118.0 | ... | 62.0 | 1.0 | 1.0 | 0.366 | 0.98 | 1.3 | False | TOR | -2.0 | TBA201604030 | . 5 rows × 28 columns . Pitcher DF . Individual pitching stats (starting pitchers only), one row per pitcher per game . pitcher_df = pd.DataFrame(pitchers) for k in pitcher_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;name&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue pitcher_df[k] =pd.to_numeric(pitcher_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) # filter the pitcher performances to just the starting pitcher pitcher_df = pitcher_df[~pitcher_df[&#39;game_score&#39;].isna()].copy().reset_index(drop=True) pitcher_df.drop(columns=[x for x in pitcher_df.keys() if &#39;inherited&#39; in x], inplace=True) pitcher_df.head() . IP H R ER BB SO HR earned_run_avg batters_faced pitches ... inplay_ld inplay_unk game_score wpa_def leverage_index_avg re24_def name is_home_team team game_id . 0 5.2 | 8.0 | 4.0 | 3.0 | 2.0 | 2.0 | 0.0 | 4.76 | 25.0 | 83.0 | ... | 5.0 | 0.0 | 39.0 | -0.061 | 0.86 | -0.4 | harvema01 | False | NYM | KCA201604030 | . 1 6.0 | 2.0 | 0.0 | 0.0 | 3.0 | 5.0 | 0.0 | 0.00 | 22.0 | 106.0 | ... | 2.0 | 0.0 | 70.0 | 0.350 | 0.92 | 3.1 | volqued01 | True | KCR | KCA201604030 | . 2 6.0 | 6.0 | 3.0 | 3.0 | 3.0 | 3.0 | 0.0 | 4.50 | 26.0 | 96.0 | ... | 6.0 | 0.0 | 48.0 | -0.069 | 0.90 | -0.1 | wainwad01 | False | STL | PIT201604030 | . 3 6.0 | 3.0 | 0.0 | 0.0 | 5.0 | 10.0 | 0.0 | 0.00 | 26.0 | 94.0 | ... | 0.0 | 0.0 | 71.0 | 0.329 | 1.52 | 2.9 | liriafr01 | True | PIT | PIT201604030 | . 4 8.0 | 6.0 | 3.0 | 3.0 | 1.0 | 5.0 | 1.0 | 3.38 | 32.0 | 98.0 | ... | 5.0 | 0.0 | 62.0 | 0.282 | 0.92 | 1.5 | stromma01 | False | TOR | TBA201604030 | . 5 rows × 26 columns . Calculate Differences in the Statistics . Here is where we&#39;re going to generate a bunch of columns in the game_df. Our statistical differences are going to be calculated like this: . For every downloaded statistic: calculate the average, standard deviation and skew: for time periods of 5, 10, 45, 180 and 730 days: grouped by team (or pitcher name in the case of pitchers) then shift the data so each row contains pre-game statistics then take the difference of the opposing team . That result is put into the game_df with a name like &quot;5day_R_Avg&quot;, or &quot;45Day_IP_StDev&quot;. The other dfs are done at that point. . Below are the routines to help execute the above algorithm. . import numpy as np def add_rolling(period, df, stat_columns): for s in stat_columns: if &#39;object&#39; in str(df[s].dtype): continue df[s+&#39;_&#39;+str(period)+&#39;_Avg&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).mean()) df[s+&#39;_&#39;+str(period)+&#39;_Std&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).std()) df[s+&#39;_&#39;+str(period)+&#39;_Skew&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).skew()) return df def get_diff_df(df, name, is_pitcher=False): #runs for each of the stat dataframes, returns the difference in stats #set up dataframe with time index df[&#39;date&#39;] = pd.to_datetime(df[&#39;game_id&#39;].str[3:-1], format=&quot;%Y%m%d&quot;) df = df.sort_values(by=&#39;date&#39;).copy() newindex = df.groupby(&#39;date&#39;)[&#39;date&#39;] .apply(lambda x: x + np.arange(x.size).astype(np.timedelta64)) df = df.set_index(newindex).sort_index() # get stat columns stat_cols = [x for x in df.columns if &#39;int&#39; in str(df[x].dtype)] stat_cols.extend([x for x in df.columns if &#39;float&#39; in str(df[x].dtype)]) #add lags df = add_rolling(&#39;5d&#39;, df, stat_cols) # this game series df = add_rolling(&#39;10d&#39;, df, stat_cols) df = add_rolling(&#39;45d&#39;, df, stat_cols) df = add_rolling(&#39;180d&#39;, df, stat_cols) # this season df = add_rolling(&#39;730d&#39;, df, stat_cols) # 2 years # reset stat columns to just the lags (removing the original stats) df.drop(columns=stat_cols, inplace=True) stat_cols = [x for x in df.columns if &#39;int&#39; in str(df[x].dtype)] stat_cols.extend([x for x in df.columns if &#39;float&#39; in str(df[x].dtype)]) # shift results so that each row is a pregame stat df = df.reset_index(drop=True) df = df.sort_values(by=&#39;date&#39;) for s in stat_cols: if is_pitcher: df[s] = df.groupby(&#39;name&#39;)[s].shift(1) else: df[s] = df.groupby(&#39;team&#39;)[s].shift(1) # calculate differences in pregame stats from home vs. away teams away_df = df[~df[&#39;is_home_team&#39;]].copy() away_df = away_df.set_index(&#39;game_id&#39;) away_df = away_df[stat_cols] home_df = df[df[&#39;is_home_team&#39;]].copy() home_df = home_df.set_index(&#39;game_id&#39;) home_df = home_df[stat_cols] diff_df = home_df.subtract(away_df, fill_value=0) diff_df = diff_df.reset_index() # clean column names for s in stat_cols: diff_df[name + &quot;_&quot; + s] = diff_df[s] diff_df.drop(columns=s, inplace=True) return diff_df . Below calculates the differences for each dataframe of stats, then merges those differences back into the main df. . df = game_df df = pd.merge(left=df, right = get_diff_df(batting_df, &#39;batting&#39;), on = &#39;game_id&#39;, how=&#39;left&#39;) print(df.shape) df = pd.merge(left=df, right = get_diff_df(pitching_df, &#39;pitching&#39;), on = &#39;game_id&#39;, how=&#39;left&#39;) print(df.shape) df = pd.merge(left=df, right = get_diff_df(pitcher_df, &#39;pitcher&#39;,is_pitcher=True), on = &#39;game_id&#39;, how=&#39;left&#39;) df.shape . (10684, 322) (10684, 697) . (10684, 1027) . Other Features . One of the other items we&#39;ll pick up from the 538 blog is pitcher rest. How long has it been since the pitcher played? Let&#39;s add that. . pitcher_df = pd.DataFrame(pitchers) # old version was filtered to just starters dates = pitcher_df[&#39;game_id&#39;].str[3:-1] pitcher_df[&#39;date&#39;] = pd.to_datetime(dates,format=&#39;%Y%m%d&#39;, errors=&#39;coerce&#39;) pitcher_df[&#39;rest&#39;] = pitcher_df.groupby(&#39;name&#39;)[&#39;date&#39;].diff().dt.days # merge into main dataframe # filter the pitcher performances to just the starting pitcher pitcher_df = pitcher_df[~pitcher_df[&#39;game_score&#39;].isna()].copy().reset_index(drop=True) home_pitchers = pitcher_df[pitcher_df[&#39;is_home_team&#39;]].copy().reset_index(drop=True) df = pd.merge(left=df, right=home_pitchers[[&#39;game_id&#39;,&#39;name&#39;, &#39;rest&#39;]], left_on=[&#39;game_id&#39;,&#39;home_pitcher&#39;], right_on=[&#39;game_id&#39;,&#39;name&#39;], how=&#39;left&#39;) df.rename(columns={&#39;rest&#39;:&#39;home_pitcher_rest&#39;}, inplace=True) away_pitchers = pitcher_df[~pitcher_df[&#39;is_home_team&#39;]].copy().reset_index(drop=True) df = pd.merge(left=df, right=away_pitchers[[&#39;game_id&#39;,&#39;name&#39;,&#39;rest&#39;]], left_on=[&#39;game_id&#39;,&#39;away_pitcher&#39;], right_on=[&#39;game_id&#39;,&#39;name&#39;], how=&#39;left&#39;) df.rename(columns={&#39;rest&#39;:&#39;away_pitcher_rest&#39;}, inplace=True) df[&#39;rest_diff&#39;] = df[&#39;home_pitcher_rest&#39;]-df[&#39;away_pitcher_rest&#39;] . Below we add some datetime features, as I generally do with time-series data like this. . df.dropna(subset=[&#39;date&#39;], inplace=True) df[&#39;season&#39;] = df[&#39;date&#39;].dt.year df[&#39;month&#39;]=df[&#39;date&#39;].dt.month df[&#39;week&#39;]=df[&#39;date&#39;].dt.isocalendar().week.astype(&#39;int&#39;) df[&#39;dow&#39;]=df[&#39;date&#39;].dt.weekday df[&#39;date&#39;] = (pd.to_datetime(df[&#39;date&#39;]) - pd.Timestamp(&quot;1970-01-01&quot;)) // pd.Timedelta(&#39;1s&#39;) #epoch time df.shape . (10535, 1036) . Save our work . import pickle pickle.dump(df, open(&#39;dataframe.pkl&#39;, &#39;wb&#39;)) . Train the Model . Finally, we get to train the model! This is just going to be a crude first run to make sure everything is working properly. We&#39;ll add the cleverest bits in the next blog post. . First, load our saved saved work . import pickle df = pickle.load(open(&#39;dataframe.pkl&#39;, &#39;rb&#39;)) df.shape . (10535, 1036) . Target encoding for remaining string features. This will effectively create a 180-day win% value in each of the team name columns. . encode_me = [x for x in df.keys() if &#39;object&#39; in str(df[x].dtype)] for x in encode_me: df[x] = df.groupby(x)[&#39;home_team_win&#39;].apply(lambda x:x.rolling(180).mean()).shift(1) . Create our test, train and validate data sets . df = df.sort_values(by=&#39;date&#39;).copy().reset_index(drop=True) X = df.drop(columns=[&#39;home_team_win&#39;, &#39;game_id&#39;]) y = df.home_team_win X_train = X[:-1000] y_train = y[:-1000] X_valid = X[-1000:-500] y_valid = y[-1000:-500] X_test = X[-500:] y_test = y[-500:] . Here&#39;s the training bit . import xgboost as xgb params = {&#39;learning_rate&#39;: 0.05,&#39;max_depth&#39;: 5} gbm = xgb.XGBClassifier(**params) model = gbm.fit(X_train, y_train, eval_set = [[X_train, y_train], [X_valid, y_valid]], eval_metric=&#39;logloss&#39;, early_stopping_rounds=10) xgb_test_preds = model.predict(X_test) xgb_test_proba = model.predict_proba(X_test)[:,1] . [0] validation_0-logloss:0.68944 validation_1-logloss:0.69111 Multiple eval metrics have been passed: &#39;validation_1-logloss&#39; will be used for early stopping. Will train until validation_1-logloss hasn&#39;t improved in 10 rounds. [1] validation_0-logloss:0.68608 validation_1-logloss:0.68908 [2] validation_0-logloss:0.68254 validation_1-logloss:0.68787 [3] validation_0-logloss:0.67960 validation_1-logloss:0.68570 [4] validation_0-logloss:0.67650 validation_1-logloss:0.68354 [5] validation_0-logloss:0.67358 validation_1-logloss:0.68344 [6] validation_0-logloss:0.67067 validation_1-logloss:0.68217 [7] validation_0-logloss:0.66788 validation_1-logloss:0.68103 [8] validation_0-logloss:0.66554 validation_1-logloss:0.68107 [9] validation_0-logloss:0.66269 validation_1-logloss:0.68044 [10] validation_0-logloss:0.66021 validation_1-logloss:0.67965 [11] validation_0-logloss:0.65762 validation_1-logloss:0.67943 [12] validation_0-logloss:0.65521 validation_1-logloss:0.67900 [13] validation_0-logloss:0.65277 validation_1-logloss:0.67913 [14] validation_0-logloss:0.65034 validation_1-logloss:0.67900 [15] validation_0-logloss:0.64802 validation_1-logloss:0.67854 [16] validation_0-logloss:0.64586 validation_1-logloss:0.67774 [17] validation_0-logloss:0.64366 validation_1-logloss:0.67702 [18] validation_0-logloss:0.64148 validation_1-logloss:0.67632 [19] validation_0-logloss:0.63917 validation_1-logloss:0.67638 [20] validation_0-logloss:0.63716 validation_1-logloss:0.67634 [21] validation_0-logloss:0.63539 validation_1-logloss:0.67684 [22] validation_0-logloss:0.63335 validation_1-logloss:0.67668 [23] validation_0-logloss:0.63147 validation_1-logloss:0.67573 [24] validation_0-logloss:0.62950 validation_1-logloss:0.67573 [25] validation_0-logloss:0.62791 validation_1-logloss:0.67601 [26] validation_0-logloss:0.62590 validation_1-logloss:0.67588 [27] validation_0-logloss:0.62407 validation_1-logloss:0.67529 [28] validation_0-logloss:0.62219 validation_1-logloss:0.67558 [29] validation_0-logloss:0.62030 validation_1-logloss:0.67566 [30] validation_0-logloss:0.61848 validation_1-logloss:0.67543 [31] validation_0-logloss:0.61700 validation_1-logloss:0.67507 [32] validation_0-logloss:0.61524 validation_1-logloss:0.67573 [33] validation_0-logloss:0.61374 validation_1-logloss:0.67671 [34] validation_0-logloss:0.61237 validation_1-logloss:0.67655 [35] validation_0-logloss:0.61072 validation_1-logloss:0.67682 [36] validation_0-logloss:0.60931 validation_1-logloss:0.67709 [37] validation_0-logloss:0.60811 validation_1-logloss:0.67722 [38] validation_0-logloss:0.60602 validation_1-logloss:0.67684 [39] validation_0-logloss:0.60497 validation_1-logloss:0.67666 [40] validation_0-logloss:0.60342 validation_1-logloss:0.67722 [41] validation_0-logloss:0.60194 validation_1-logloss:0.67730 Stopping. Best iteration: [31] validation_0-logloss:0.61700 validation_1-logloss:0.67507 . And our results... . from sklearn.calibration import calibration_curve from sklearn.metrics import accuracy_score, brier_score_loss import matplotlib.pyplot as plt import pickle def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 8)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_pred, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} t tAccuracy:{:.4f} t Brier Loss: {:.4f}&quot;.format( name, accuracy_score(y_test, y_pred), brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() outcomes,predictions,probabilities = pickle.load(open(&#39;baseline.pkl&#39;,&#39;rb&#39;)) data = [ (outcomes, predictions, probabilities, &#39;Casino&#39;), (y_test,xgb_test_preds, xgb_test_proba, &#39;XGBoost&#39;) ] cal_curve(data, 15) . . Casino Accuracy:0.6006 Brier Loss: 0.2358 XGBoost Accuracy:0.5780 Brier Loss: 0.2430 . That&#39;s not terrible for a first run - &gt;57% accuracy for our model vs. 60% for the casinos. We&#39;re only off by 2.2% and we still have our secret features and model optimization to do. . Our calibration is also worse, but again it&#39;s not at all terrible for our first run. I like to think of it in terms of forecasting skill. Our calibration skill is 3% worse than the casino&#39;s, as measured by our brier loss function. . This model has some promise. Let&#39;s look at the feature importances just to see which features it&#39;s using most. It looks like the model likes the game score spread , the pitching RE24 metric, and it likes the longer lags: 180day &amp; 730day. . import pandas as pd x = pd.Series(model.get_booster().get_score(importance_type= &#39;total_gain&#39;) ).sort_values() _ = x[-25:].plot(kind=&#39;barh&#39;,title=&quot;XGBoost Feature Gain&quot;) . . Next Up . In Part 3, we&#39;re going to add team power rankings and casino odds to the dataset to improve the results further! .",
            "url": "https://rdpharr.github.io/project_notes/baseball/webscraping/xgboost/brier/accuracy/calibration/2020/09/21/MLB-Part2-First-Model.html",
            "relUrl": "/baseball/webscraping/xgboost/brier/accuracy/calibration/2020/09/21/MLB-Part2-First-Model.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "MLB Baseball Predictions",
            "content": "MLB Baseball Prediction Series: Part 1 Part 2 Part 3 Part 4 Part 5 . | . This is the second season I&#39;ve been using machine learning to make predictions and bets. Last year I made good predictions, but I hadn&#39;t figured out how to size my bets so I didn&#39;t make any money. This year I feel like I have a good strategy for this and it was solidly profitable. . I&#39;m going to share what I do in this series of blog posts. Hopefully I&#39;ll get some feedback that will help me improve. If not, at least it might help others get started. . This blog series is written in jupyter notebooks, which will show you how to build a program that predicts the outcome of MLB games. We&#39;ll be using our web scraping and machine learning skills to build a model that significantly outperforms the casino&#39;s sports books. . Each blog post, including this one, is executable. Use the buttons at the top to run the code on Binder of Colab and get fresh results for yourself. You can also download it from Github to run the notebook locally. . Important: Web scraping is dependant on other people&#8217;s web pages. If they change their site, this blog&#8217;s code will break. Don&#8217;t expect the code presented here to work forever. . Benchmarking the Sportsbooks . First thing to do is figure out how we’re going to know if we’re doing well. The most intuitive performance benchmark I found was the sportsbooks themselves. If I can make better predictions than the sportsbooks, then I should be doing well. . Downloading Sportsbook Data . We need to start by putting together a database of historic odds and outcomes for MLB games. First step is to get a list of days when games were played. We can get those from baseball-reference.com. . import requests import re import datetime as dt url = &#39;https://www.baseball-reference.com/leagues/MLB/2019-schedule.shtml&#39; resp = requests.get(url) # All the H3 tags contain day names days = re.findall(&quot;&lt;h3&gt;(.*2019)&lt;/h3&gt;&quot;, resp.text) dates = [dt.datetime.strptime(d,&quot;%A, %B %d, %Y&quot;) for d in days] print(&quot;Number of days MLB was played in 2019:&quot;, len(dates)) . Number of days MLB was played in 2019: 210 . We need the correct days because we&#39;ll be pulling the odds data from covers.com by day. Covers aggregates the published odds from several sources and then publishes a consensus moneyline. We&#39;ll grab that, along with the score of the game. Here&#39;s how we pull and parse that data. . from bs4 import BeautifulSoup as bs game_data = [] for d in dates: # get the web page with game data on it game_day = d.strftime(&#39;%Y-%m-%d&#39;) url = f&#39;https://www.covers.com/Sports/MLB/Matchups?selectedDate={game_day}&#39; resp = requests.get(url) # parse the games scraped_games = bs(resp.text).findAll(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_game_box&#39;}) for g in scraped_games: game = {} game[&#39;home_moneyline&#39;] = g[&#39;data-game-odd&#39;] game[&#39;date&#39;] = g[&#39;data-game-date&#39;] try: game[&#39;home_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_home&#39;}).text.strip() game[&#39;away_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_away&#39;}).text.strip() except: game[&#39;home_score&#39;] =&#39;&#39; game[&#39;away_score&#39;] =&#39;&#39; game_data.append(game) if len(game_data) % 500==0: #show progress print(dt.datetime.now(), game_day, len(game_data)) print(&quot;Done! Games downloaded:&quot;, len(game_data)) . 2020-09-22 10:18:28.135002 2019-05-02 500 2020-09-22 10:18:56.497463 2019-06-08 1000 2020-09-22 10:19:23.908988 2019-07-18 1500 2020-09-22 10:19:51.687110 2019-08-24 2000 2020-09-22 10:20:19.900455 2019-10-03 2500 Done! Games downloaded: 2533 . Here&#39;s what that data looks like. You can see the moneyline was negative, meaning that the home team was favored. But the home team lost, so the prediction from the casinos was inaccurate. . game_data[0] . {&#39;home_moneyline&#39;: &#39;-155&#39;, &#39;date&#39;: &#39;2019-03-20 05:35:00&#39;, &#39;home_score&#39;: &#39;7&#39;, &#39;away_score&#39;: &#39;9&#39;} . That would have been a pretty good payout if you bet on the away team. Let&#39;s save our data so we don&#39;t need to keep downloading it. . Important: We will be using the files saved in subsequent blog posts. Be sure to save them locally if you run this notebook online. . import pickle pickle.dump(game_data, open(&#39;covers_data.pkl&#39;,&#39;wb&#39;)) . Sportsbook Accuracy . Let&#39;s see how the sportsbook did in all the games we just downloaded. . from sklearn.metrics import accuracy_score # the actual outcome of the game, true if the the home team won outcomes = [] # predictions derived from moneyline odds. True if the home team was the favorite predictions = [] # probability the home team will win, derived from moneyline odds # derived from formulas at https://www.bettingexpert.com/academy/advanced-betting-theory/odds-conversion-to-percentage probabilities = [] for d in game_data: try: moneyline = int(d[&#39;home_moneyline&#39;]) home_score = int(d[&#39;home_score&#39;]) away_score = int(d[&#39;away_score&#39;]) except: #incomplete data continue if moneyline==100: # it&#39;s rare to have a tossup since covers is averaging the odds from several sports books # but we&#39;ll exclude them from our calculations continue # convert moneyline odds ot their implied probabilities if moneyline&lt;0: probabilities.append(-moneyline/(-moneyline + 100)) elif moneyline&gt;100: probabilities.append(100/(moneyline + 100)) outcomes.append(home_score&gt;away_score) predictions.append(moneyline&lt;0) print(&quot;Sportsbook accuracy (excluding tossups): {0:.2f}%&quot;.format(100*accuracy_score(outcomes,predictions))) . Sportsbook accuracy (excluding tossups): 60.06% . That&#39;s it, right? We need a model that is better than 60% accurate. . If you plan to use this data for betting, you should have more than a win/loss prediction. To really make money, we would like to know if we think the odds of a team winning are better or worse that what the sportsbook thinks they are. Then we&#39;d be able to use some sort of expected value calculation to determine if the bet is profitable. . Sportsbook Calibration . We really want to know if we can build a model that is better calibrated than the casino&#39;s sportsbooks. Knowing our calibration will help us with bet sizing, as well as more sophisticated betting algorithms. Here&#39;s a graphical view of the calibration of the casino sports book data. . from sklearn.calibration import calibration_curve from sklearn.metrics import accuracy_score, brier_score_loss import matplotlib.pyplot as plt def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 8)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_pred, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} tAccuracy:{:.4f} t Brier Loss: {:.4f}&quot;.format( name, accuracy_score(y_test, y_pred), brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() data = [(outcomes, predictions, probabilities, &#39;SportsBook&#39;)] cal_curve(data, 15) . . SportsBook Accuracy:0.6006 Brier Loss: 0.2358 . The graph above tells us several things about the calibration of the casino&#39;s predictions. The reliability curve clearly shows that the casino is highly calibrated. Interestingly, it looks like the blue line is shifted down slightly from the &quot;perfectly calibrated&quot; line. It would be a better fit if it was 0.05 higher. This may account for the house advantage. . The histogram below shows what portion of the games fall into each bin. We see a slight predicted advantage to the home team, with more than 50% of the observations above the 50% mark. Otherwise it looks pretty normally distributed. . Above, I said the reliability curve looks highly calibrated. If we are to judge our own efforts against the sportsbook, we can&#39;t just be eyeballing this graph all the time. A metric would be nice. One metric that is suited for calibration measurement is the Brier Score, which I&#39;ll be using to measure the model effectiveness going forward. Getting a model that scores less than 0.2358 is the target for our efforts. . Before we go, we should save the files for our baseline... . import pickle pickle.dump((outcomes,predictions, probabilities), open(&#39;baseline.pkl&#39;,&#39;wb&#39;)) . Next Up . In Part 2, we&#39;ll start building out our historic data and training the model using XGBoost. .",
            "url": "https://rdpharr.github.io/project_notes/baseball/benchmark/webscraping/brier/accuracy/calibration/2020/09/20/baseball_project.html",
            "relUrl": "/baseball/benchmark/webscraping/brier/accuracy/calibration/2020/09/20/baseball_project.html",
            "date": " • Sep 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a semi-retired tech professional, living in Las Vegas, Nevada. I’m forever in school, right now at University of the People. I do work to end racial injustices, focusing on criminal justice reform with Mass Liberation of Nevada. . You can get in touch with me using the links at the bottom of this page. . About the Site . This website is powered by fastpages. .",
          "url": "https://rdpharr.github.io/project_notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rdpharr.github.io/project_notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}