{
  
    
        "post0": {
            "title": "MLB Power Rankings and Casino Odds",
            "content": "Last time, we created a model that performs pretty well just based on the statistics that we downloaded from baseball-reference.com. In this post, we&#39;ll extend that model by adding power rankings and casino odds to the model. . Important: You can run this notebook from Colab or Binder using the buttons above, but you&#8217;ll also need the files we created in Part 2. . Power Rankings . The 538 Blog has famously modified the Elo system from chess to make their baseball rankings. The Elo system tries to determine the relative skill level of a player based on the skill levels of the other players encountered. If you beat a person with a high skill level, your skill level is going to improve more than if you win against a player of the same or lower skill level than you. And if your skill level is higher than your opponent&#39;s then you will probably win the match. . If 538 thinks Elo is foundational, then we should definitely put it in our model. The trouble is that not everyone agrees on how to implement it. In fact there are a whole family of different power ranking systems out there. In this project we&#39;re going to add four: 2 varieties of Elo (slow and fast changing), Glicko, and Trueskill. Luckily people have created libraries to help us get the code right. . Important: If you are running this notebook online, you may need to install the additional libraries. Here&#8217;s how to do it on Colab. . Let&#39;s get to it. We&#39;ll start by importing our dataframe from Part 2. . import pickle df = pickle.load(open(&quot;dataframe.pkl&quot;,&quot;rb&quot;)) . Elo Rankings . For Elo rankings, we&#39;re going to use the elote library, primarily because it&#39;s named after the Mexican habit of eating corn on the cob with mayonnaise (yuck!). Install it like this: . pip install elote . from elote import EloCompetitor ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=EloCompetitor() for x in df.away_team_abbr.unique(): ratings[x]=EloCompetitor() home_team_elo = [] away_team_elo = [] elo_exp = [] df = df.sort_values(by=&#39;date&#39;).reset_index(drop=True) for i, r in df.iterrows(): # get pre-game ratings elo_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr])) home_team_elo.append(ratings[r.home_team_abbr].rating) away_team_elo.append(ratings[r.away_team_abbr].rating) # update ratings if r.home_team_win: ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr]) else: ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr]) df[&#39;elo_exp&#39;] = elo_exp df[&#39;home_team_elo&#39;] = home_team_elo df[&#39;away_team_elo&#39;] = away_team_elo . Now we&#39;ll do the slow changing version, where we decrease the k-factor. . ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=EloCompetitor() ratings[x]._k_score=16 for x in df.away_team_abbr.unique(): ratings[x]=EloCompetitor() ratings[x]._k_score=16 home_team_elo = [] away_team_elo = [] elo_exp = [] df = df.sort_values(by=&#39;date&#39;).reset_index(drop=True) for i, r in df.iterrows(): # get pregame ratings elo_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr])) home_team_elo.append(ratings[r.home_team_abbr].rating) away_team_elo.append(ratings[r.away_team_abbr].rating) # update ratings if r.home_team_win: ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr]) else: ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr]) df[&#39;elo_slow_exp&#39;] = elo_exp df[&#39;home_team_elo_slow&#39;] = home_team_elo df[&#39;away_team_elo_slow&#39;] = away_team_elo . Glicko Ratings . Glicko can be calculated using the same library. . from elote import GlickoCompetitor ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=GlickoCompetitor() for x in df.away_team_abbr.unique(): ratings[x]=GlickoCompetitor() home_team_glick = [] away_team_glick = [] glick_exp = [] df = df.sort_values(by=&#39;date&#39;).reset_index(drop=True) for i, r in df.iterrows(): # get pregame ratings glick_exp.append(ratings[r.home_team_abbr].expected_score(ratings[r.away_team_abbr])) home_team_glick.append(ratings[r.home_team_abbr].rating) away_team_glick.append(ratings[r.away_team_abbr].rating) # update ratings if r.home_team_win: ratings[r.home_team_abbr].beat(ratings[r.away_team_abbr]) else: ratings[r.away_team_abbr].beat(ratings[r.home_team_abbr]) df[&#39;glick_exp&#39;] = glick_exp df[&#39;home_team_glick&#39;] = home_team_glick df[&#39;away_team_glick&#39;] = away_team_glick . Trueskill Ratings . Trueskill was invented for Microsoft video games on the XBox. It&#39;s something you need to license if you are going to use it for commercial purposes. Trueskill ratings are a little bit more complex, because we have the opportunity to include the starting pitcher skill as well. Install the python package like this: . pip install trueskill . from trueskill import Rating, quality, rate ratings = {} for x in df.home_team_abbr.unique(): ratings[x]=Rating(25) for x in df.away_team_abbr.unique(): ratings[x]=Rating(25) for x in df.home_pitcher.unique(): ratings[x]=Rating(25) for x in df.away_pitcher.unique(): ratings[x]=Rating(25) ts_quality = [] pitcher_ts_diff = [] team_ts_diff = [] home_pitcher_ts = [] away_pitcher_ts = [] home_team_ts = [] away_team_ts = [] df = df.sort_values(by=&#39;date&#39;).copy() for i, r in df.iterrows(): # get pre-match trueskill ratings from dict match = [(ratings[r.home_team_abbr], ratings[r.home_pitcher]), (ratings[r.away_team_abbr], ratings[r.away_pitcher])] ts_quality.append(quality(match)) pitcher_ts_diff.append(ratings[r.home_pitcher].mu-ratings[r.away_pitcher].mu) team_ts_diff.append(ratings[r.home_team_abbr].mu-ratings[r.away_team_abbr].mu) home_pitcher_ts.append(ratings[r.home_pitcher].mu) away_pitcher_ts.append(ratings[r.away_pitcher].mu) home_team_ts.append(ratings[r.home_team_abbr].mu) away_team_ts.append(ratings[r.away_team_abbr].mu) if r.date &lt; df.date.max(): # update ratings dictionary with post-match ratings if r.home_team_win==1: match = [(ratings[r.home_team_abbr], ratings[r.home_pitcher]), (ratings[r.away_team_abbr], ratings[r.away_pitcher])] [(ratings[r.home_team_abbr], ratings[r.home_pitcher]), (ratings[r.away_team_abbr], ratings[r.away_pitcher])] = rate(match) else: match = [(ratings[r.away_team_abbr], ratings[r.away_pitcher]), (ratings[r.home_team_abbr], ratings[r.home_pitcher])] [(ratings[r.away_team_abbr], ratings[r.away_pitcher]), (ratings[r.home_team_abbr], ratings[r.home_pitcher])] = rate(match) df[&#39;ts_game_quality&#39;] = ts_quality df[&#39;pitcher_ts_diff&#39;] = pitcher_ts_diff df[&#39;team_ts_diff&#39;] = team_ts_diff df[&#39;home_pitcher_ts&#39;] = home_pitcher_ts df[&#39;away_pitcher_ts&#39;] = away_pitcher_ts df[&#39;home_team_ts&#39;] = home_team_ts df[&#39;away_team_ts&#39;] = away_team_ts . That&#39;s all we need for power rankings. Let&#39;s move on. . Casino Odds . Having the casino odds in our model is really going to improve its predictions, but getting them in there is kind of a pain in the ass. The problem is that we need to match the games from two different systems (baseball-reference.com and covers.com). These systems don&#39;t use the same team abbreviations and don&#39;t even agree on what time the games started. So there&#39;s a bit of code to compensate. . But it starts the same as in Part 1 of this blog series - we need to find out which days to download odds data for. We&#39;ll use our dataframe to get this list. . import pandas as pd dates = pd.to_datetime(df[&#39;date&#39;], unit=&#39;s&#39;) game_days = dates.dt.strftime(&#39;%Y-%m-%d&#39;).unique() print(&quot;Days of odds data needed:&quot;, len(game_days)) . Days of odds data needed: 882 . The below code is largely the same from Part 1, except we are also grabbing team abbreviations from the data . import requests from bs4 import BeautifulSoup as bs import datetime as dt game_data = [] for d in game_days: # get the web page with game data on it url = f&#39;https://www.covers.com/Sports/MLB/Matchups?selectedDate={d}&#39; resp = requests.get(url) # parse the games scraped_games = bs(resp.text).findAll(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_game_box&#39;}) for g in scraped_games: game = {} game[&#39;home_moneyline&#39;] = g[&#39;data-game-odd&#39;] game[&#39;date&#39;] = g[&#39;data-game-date&#39;] game[&#39;away_team_abbr&#39;] = g[&#39;data-away-team-shortname-search&#39;] game[&#39;home_team_abbr&#39;] = g[&#39;data-home-team-shortname-search&#39;] try: game[&#39;home_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_home&#39;}).text.strip() game[&#39;away_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_away&#39;}).text.strip() except: game[&#39;home_score&#39;] =&#39;&#39; game[&#39;away_score&#39;] =&#39;&#39; game_data.append(game) if len(game_data) % 1000==0: #show progress print(dt.datetime.now(), d, len(game_data)) print(&quot;Done! Games downloaded:&quot;, len(game_data)) . 2020-09-22 11:05:50.761606 2016-06-15 1000 2020-09-22 11:06:49.874100 2016-08-31 2000 2020-09-22 11:08:01.555549 2017-05-10 3000 2020-09-22 11:08:59.782088 2017-07-24 4000 2020-09-22 11:10:12.009463 2018-03-29 5000 2020-09-22 11:11:15.911291 2018-06-10 6000 2020-09-22 11:12:19.081595 2018-08-26 7000 2020-09-22 11:13:33.951630 2019-05-01 8000 2020-09-22 11:14:32.515388 2019-07-17 9000 2020-09-22 11:15:31.501830 2019-09-29 10000 Done! Games downloaded: 10914 . So slow. Let&#39;s save this so we don&#39;t have to go through that again. . import pickle pickle.dump(game_data, open(&#39;covers_data_2.pkl&#39;,&#39;wb&#39;)) . We&#39;ll do some prepping and cleaning of the data. . import pickle game_data = pickle.load(open(&#39;covers_data_2.pkl&#39;,&#39;rb&#39;)) . import numpy as np import pandas as pd odds = pd.DataFrame(game_data) odds[&#39;home_moneyline&#39;].replace(&#39;&#39;, np.nan, inplace=True) odds.dropna(subset=[&#39;home_moneyline&#39;], inplace=True) odds.home_moneyline = pd.to_numeric(odds.home_moneyline) odds.date = pd.to_datetime(odds.date).dt.date . Now we convert the team names to be the same as baseball-reference.com . import warnings warnings.filterwarnings(&#39;ignore&#39;) warnings.simplefilter(action=&#39;ignore&#39;, category=FutureWarning) odds.home_team_abbr[odds.home_team_abbr==&#39;SF&#39;]=&#39;SFG&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;TB&#39;]=&#39;TBR&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;WAS&#39;]=&#39;WSN&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;KC&#39;]=&#39;KCR&#39; odds.home_team_abbr[odds.home_team_abbr==&#39;SD&#39;]=&#39;SDP&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;SF&#39;]=&#39;SFG&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;TB&#39;]=&#39;TBR&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;WAS&#39;]=&#39;WSN&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;KC&#39;]=&#39;KCR&#39; odds.away_team_abbr[odds.away_team_abbr==&#39;SD&#39;]=&#39;SDP&#39; . Finally, convert the moneyline odds to probabilities . odds[&#39;odds_proba&#39;]=np.nan odds[&#39;odds_proba&#39;][odds.home_moneyline&lt;0] = -odds.home_moneyline/(-odds.home_moneyline + 100) odds[&#39;odds_proba&#39;][odds.home_moneyline&gt;0] = (100/(odds.home_moneyline + 100)) . Because the game times aren&#39;t exact, we&#39;ll use pandas merge_asof to find the closest match. The syntax is that you the fields in &quot;by&quot; parameter need to be exact, and it will find the closest by the &quot;on&quot; parameter. I think this feature is awesome, and another reason I love pandas. . print(&#39;dataframe shape before merge:&#39;, df.shape) # get dates into the same format odds[&#39;date&#39;] = (pd.to_datetime(pd.to_datetime(odds[&#39;date&#39;])) - pd.Timestamp(&quot;1970-01-01&quot;)) // pd.Timedelta(&#39;1s&#39;) # do the merge df = pd.merge_asof(left=df.sort_values(by=&#39;date&#39;), right=odds[[&#39;home_team_abbr&#39;,&#39;date&#39;, &#39;away_team_abbr&#39;,&#39;odds_proba&#39;]].sort_values(by=&#39;date&#39;), by=[&#39;home_team_abbr&#39;,&#39;away_team_abbr&#39;], on=&#39;date&#39;) df = df.sort_values(by=&#39;date&#39;).copy().reset_index(drop=True) print(&#39;dataframe shape after merge:&#39;, df.shape) . dataframe shape before merge: (10492, 1016) dataframe shape after merge: (10492, 1017) . Things look good now. Let&#39;s save this dataframe before we move on . import pickle pickle.dump(df, open(&#39;dataframe_part3.pkl&#39;,&#39;wb&#39;)) . Run The Model . This is almost the exact code we ran in part 2 . import pickle df = pickle.load(open(&#39;dataframe_part3.pkl&#39;,&#39;rb&#39;)) . import xgboost as xgb # target encoding encode_me = [x for x in df.keys() if &#39;object&#39; in str(df[x].dtype)] for x in encode_me: df[x] = df.groupby(x)[&#39;home_team_win&#39;].apply(lambda x:x.rolling(180).mean()).shift(1) # create test, train splits df = df.sort_values(by=&#39;date&#39;).copy().reset_index(drop=True) X = df.drop(columns=[&#39;home_team_win&#39;, &#39;game_id&#39;]) y = df.home_team_win X_train = X[:-1000] y_train = y[:-1000] X_valid = X[-1000:-500] y_valid = y[-1000:-500] X_test = X[-500:] y_test = y[-500:] . Run the model . params = {&#39;learning_rate&#39;: 0.075,&#39;max_depth&#39;: 4} gbm = xgb.XGBClassifier(**params) model = gbm.fit(X_train, y_train, eval_set = [[X_train, y_train], [X_valid, y_valid]], eval_metric=&#39;logloss&#39;, early_stopping_rounds=10) xgb_test_preds = model.predict(X_test) xgb_test_proba = model.predict_proba(X_test)[:,1] . [0] validation_0-logloss:0.68855 validation_1-logloss:0.68919 Multiple eval metrics have been passed: &#39;validation_1-logloss&#39; will be used for early stopping. Will train until validation_1-logloss hasn&#39;t improved in 10 rounds. [1] validation_0-logloss:0.68441 validation_1-logloss:0.68602 [2] validation_0-logloss:0.68068 validation_1-logloss:0.68400 [3] validation_0-logloss:0.67736 validation_1-logloss:0.68144 [4] validation_0-logloss:0.67433 validation_1-logloss:0.68012 [5] validation_0-logloss:0.67159 validation_1-logloss:0.67901 [6] validation_0-logloss:0.66903 validation_1-logloss:0.67868 [7] validation_0-logloss:0.66654 validation_1-logloss:0.67790 [8] validation_0-logloss:0.66440 validation_1-logloss:0.67730 [9] validation_0-logloss:0.66216 validation_1-logloss:0.67634 [10] validation_0-logloss:0.65985 validation_1-logloss:0.67627 [11] validation_0-logloss:0.65795 validation_1-logloss:0.67603 [12] validation_0-logloss:0.65602 validation_1-logloss:0.67588 [13] validation_0-logloss:0.65414 validation_1-logloss:0.67572 [14] validation_0-logloss:0.65260 validation_1-logloss:0.67406 [15] validation_0-logloss:0.65117 validation_1-logloss:0.67310 [16] validation_0-logloss:0.64945 validation_1-logloss:0.67307 [17] validation_0-logloss:0.64780 validation_1-logloss:0.67365 [18] validation_0-logloss:0.64602 validation_1-logloss:0.67355 [19] validation_0-logloss:0.64458 validation_1-logloss:0.67376 [20] validation_0-logloss:0.64318 validation_1-logloss:0.67340 [21] validation_0-logloss:0.64150 validation_1-logloss:0.67332 [22] validation_0-logloss:0.63970 validation_1-logloss:0.67333 [23] validation_0-logloss:0.63847 validation_1-logloss:0.67269 [24] validation_0-logloss:0.63705 validation_1-logloss:0.67386 [25] validation_0-logloss:0.63545 validation_1-logloss:0.67392 [26] validation_0-logloss:0.63379 validation_1-logloss:0.67442 [27] validation_0-logloss:0.63281 validation_1-logloss:0.67532 [28] validation_0-logloss:0.63151 validation_1-logloss:0.67578 [29] validation_0-logloss:0.62992 validation_1-logloss:0.67624 [30] validation_0-logloss:0.62870 validation_1-logloss:0.67661 [31] validation_0-logloss:0.62767 validation_1-logloss:0.67685 [32] validation_0-logloss:0.62663 validation_1-logloss:0.67751 [33] validation_0-logloss:0.62527 validation_1-logloss:0.67742 Stopping. Best iteration: [23] validation_0-logloss:0.63847 validation_1-logloss:0.67269 . And our results... . from sklearn.calibration import calibration_curve from sklearn.metrics import accuracy_score, brier_score_loss import matplotlib.pyplot as plt import pickle def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 8)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_pred, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} t tAccuracy:{:.4f} t Brier Loss: {:.4f}&quot;.format( name, accuracy_score(y_test, y_pred), brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() outcomes,predictions,probabilities = pickle.load(open(&#39;baseline.pkl&#39;,&#39;rb&#39;)) data = [ (outcomes, predictions, probabilities, &#39;Casino&#39;), (y_test,xgb_test_preds, xgb_test_proba, &#39;XGBoost&#39;) ] cal_curve(data, 15) . . Casino Accuracy:0.6006 Brier Loss: 0.2358 XGBoost Accuracy:0.6120 Brier Loss: 0.2384 . Now we&#39;re talking. Our accuracy in our test data is 1.1% better than the oddsmakers and our calibration is virtually the same. Looking at our feature importances below, we can see that the odds dominate the importances, but our power rankings also are among the top features. Interestingly, almost none of the features that were important in the Part 2 blog post are still important. XGBoost is fickle. . import pandas as pd x = pd.Series(model.get_booster().get_score(importance_type= &#39;total_gain&#39;) ).sort_values() _ = x[-25:].plot(kind=&#39;barh&#39;,title=&quot;XGBoost Feature Gain&quot;) . . Next up . In Part 4, we&#39;re going to train this thing for real, and try to squeak out a few more % through downloading more data and hyperparameter optimization. You may want to run the next notebook overnight... .",
            "url": "https://rdpharr.github.io/project_notes/baseball/web_scraping/elo/trueskill/glick/2020/09/22/power-rankings-and-casino-odds.html",
            "relUrl": "/baseball/web_scraping/elo/trueskill/glick/2020/09/22/power-rankings-and-casino-odds.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "MLB Training Data",
            "content": "This is the second post in my series on MLB Baseball Betting, and it&#39;s a long one. Here we&#39;re going to have a complete working model from scraping the data, processing it, adding features and finally giving it a run in XGBoost. . This is all done in a single notebook so you can start fiddling with it on your own as soon as you want. Use the buttons on the top to download your own copy from github or run it in the cloud on one of the services listed. . Scrape Data . For this model, I&#39;m going to use the stats from the baseball-reference.com box scores page. Here is and example of the page we&#39;ll be scraping. Let&#39;s get started. . Create List of Games to Download . We&#39;ll get the games from the schedule pages, creating a list of links to each game. Let&#39;s start in 2016, that should give us enough games to start making inferences. . import requests from bs4 import BeautifulSoup as bs game_links = [] for current_year in range(2016,2021): url = f&quot;https://www.baseball-reference.com/leagues/MLB/{current_year}-schedule.shtml&quot; resp = requests.get(url) soup=bs(resp.text) games = soup.findAll(&#39;a&#39;,text=&#39;Boxscore&#39;) game_links.extend([x[&#39;href&#39;] for x in games]) print(&quot;Number of games to download: &quot;, len(game_links)) game_links[0] . Number of games to download: 10641 . &#39;/boxes/KCA/KCA201604030.shtml&#39; . Download Game Data . For each game, I want to download team performance in batting and pitching, as well as individual performance pitching. For 10K games, this is going to take a little while to build out. . # these are functions related to parsing the baseball reference page def get_game_summary(soup, game_id): game = {&#39;game_id&#39;: game_id} scorebox = soup.find(&#39;div&#39;, {&#39;class&#39;:&#39;scorebox&#39;}) teams = scorebox.findAll(&#39;a&#39;,{&#39;itemprop&#39;:&#39;name&#39;}) game[&#39;away_team_abbr&#39;] = teams[0][&#39;href&#39;].split(&#39;/&#39;)[2] game[&#39;home_team_abbr&#39;] = teams[1][&#39;href&#39;].split(&#39;/&#39;)[2] meta = scorebox.find(&#39;div&#39;, {&#39;class&#39;:&#39;scorebox_meta&#39;}).findAll(&#39;div&#39;) game[&#39;date&#39;] = meta[0].text.strip() game[&#39;start_time&#39;] = meta[1].text[12:-6].strip() return game def get_table_summary(soup, table_no): stats_tables = soup.findAll(&#39;table&#39;, {&#39;class&#39;:&#39;stats_table&#39;}) t = stats_tables[table_no].find(&#39;tfoot&#39;) summary = {x[&#39;data-stat&#39;]:x.text.strip() for x in t.findAll(&#39;td&#39;)} return summary def get_pitcher_data(soup, table_no): stats_tables = soup.findAll(&#39;table&#39;, {&#39;class&#39;:&#39;stats_table&#39;}) t = stats_tables[table_no] data = [] rows = t.findAll(&#39;tr&#39;)[1:-1] # not the header and footer rows for r in rows: summary = {x[&#39;data-stat&#39;]:x.text.strip() for x in r.findAll(&#39;td&#39;)} summary[&#39;name&#39;] = r.find(&#39;th&#39;,{&#39;data-stat&#39;:&#39;player&#39;}).find(&#39;a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1][:-6].strip() data.append(summary) return data def process_link(url): resp = requests.get(url) game_id = url.split(&#39;/&#39;)[-1][:-6] # strange preprocessing routine uncommented_html = &#39;&#39; for h in resp.text.split(&#39; n&#39;): if &#39;&lt;!-- &lt;div&#39; in h: continue if h.strip() == &#39;&lt;!--&#39;: continue if h.strip() == &#39;--&gt;&#39;: continue uncommented_html += h + &#39; n&#39; soup = bs(uncommented_html) data = { &#39;game&#39;: get_game_summary(soup, game_id), &#39;away_batting&#39;: get_table_summary(soup, 1), &#39;home_batting&#39;:get_table_summary(soup, 2), &#39;away_pitching&#39;:get_table_summary(soup, 3), &#39;home_pitching&#39;:get_table_summary(soup, 4), &#39;away_pitchers&#39;: get_pitcher_data(soup, 3), &#39;home_pitchers&#39;: get_pitcher_data(soup, 4) } return data . . import datetime as dt game_data = [] for link in game_links: url = &#39;https://www.baseball-reference.com&#39; + link game_data.append(process_link(url)) if len(game_data)%1000==0: print(dt.datetime.now().time(), len(game_data)) . 21:44:35.084156 1000 21:49:42.274916 2000 21:54:54.916589 3000 22:00:10.507164 4000 22:05:22.761320 5000 22:10:34.759694 6000 22:15:47.269227 7000 22:21:05.776663 8000 22:26:17.596819 9000 22:31:37.716245 10000 . That took a while. We could definitely speed it up with threading, but we want to be nice to their servers. They seem like good people, making their data available to everyone. . Here is what a single game looks like. It&#39;s actually quite a bit of data. . game_data[0] . {&#39;game&#39;: {&#39;game_id&#39;: &#39;KCA201604030&#39;, &#39;away_team_abbr&#39;: &#39;NYM&#39;, &#39;home_team_abbr&#39;: &#39;KCR&#39;, &#39;date&#39;: &#39;Sunday, April 3, 2016&#39;, &#39;start_time&#39;: &#39;7:38 p.m.&#39;}, &#39;away_batting&#39;: {&#39;AB&#39;: &#39;33&#39;, &#39;R&#39;: &#39;3&#39;, &#39;H&#39;: &#39;7&#39;, &#39;RBI&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;6&#39;, &#39;SO&#39;: &#39;9&#39;, &#39;PA&#39;: &#39;39&#39;, &#39;batting_avg&#39;: &#39;.212&#39;, &#39;onbase_perc&#39;: &#39;.333&#39;, &#39;slugging_perc&#39;: &#39;.242&#39;, &#39;onbase_plus_slugging&#39;: &#39;.576&#39;, &#39;pitches&#39;: &#39;177&#39;, &#39;strikes_total&#39;: &#39;105&#39;, &#39;wpa_bat&#39;: &#39;-0.449&#39;, &#39;leverage_index_avg&#39;: &#39;1.58&#39;, &#39;wpa_bat_pos&#39;: &#39;0.746&#39;, &#39;wpa_bat_neg&#39;: &#39;-1.195&#39;, &#39;re24_bat&#39;: &#39;-1.7&#39;, &#39;PO&#39;: &#39;24&#39;, &#39;A&#39;: &#39;15&#39;, &#39;details&#39;: &#39;&#39;}, &#39;home_batting&#39;: {&#39;AB&#39;: &#39;30&#39;, &#39;R&#39;: &#39;4&#39;, &#39;H&#39;: &#39;9&#39;, &#39;RBI&#39;: &#39;4&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;3&#39;, &#39;PA&#39;: &#39;33&#39;, &#39;batting_avg&#39;: &#39;.300&#39;, &#39;onbase_perc&#39;: &#39;.333&#39;, &#39;slugging_perc&#39;: &#39;.300&#39;, &#39;onbase_plus_slugging&#39;: &#39;.633&#39;, &#39;pitches&#39;: &#39;114&#39;, &#39;strikes_total&#39;: &#39;71&#39;, &#39;wpa_bat&#39;: &#39;0.052&#39;, &#39;leverage_index_avg&#39;: &#39;0.74&#39;, &#39;wpa_bat_pos&#39;: &#39;0.488&#39;, &#39;wpa_bat_neg&#39;: &#39;-0.434&#39;, &#39;re24_bat&#39;: &#39;-0.1&#39;, &#39;PO&#39;: &#39;27&#39;, &#39;A&#39;: &#39;13&#39;, &#39;details&#39;: &#39;&#39;}, &#39;away_pitching&#39;: {&#39;IP&#39;: &#39;8&#39;, &#39;H&#39;: &#39;9&#39;, &#39;R&#39;: &#39;4&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;3&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;3.38&#39;, &#39;batters_faced&#39;: &#39;33&#39;, &#39;pitches&#39;: &#39;114&#39;, &#39;strikes_total&#39;: &#39;71&#39;, &#39;strikes_contact&#39;: &#39;42&#39;, &#39;strikes_swinging&#39;: &#39;7&#39;, &#39;strikes_looking&#39;: &#39;22&#39;, &#39;inplay_gb_total&#39;: &#39;18&#39;, &#39;inplay_fb_total&#39;: &#39;10&#39;, &#39;inplay_ld&#39;: &#39;5&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;39&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;1&#39;, &#39;wpa_def&#39;: &#39;-0.051&#39;, &#39;leverage_index_avg&#39;: &#39;0.74&#39;, &#39;re24_def&#39;: &#39;0.1&#39;}, &#39;home_pitching&#39;: {&#39;IP&#39;: &#39;9&#39;, &#39;H&#39;: &#39;7&#39;, &#39;R&#39;: &#39;3&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;6&#39;, &#39;SO&#39;: &#39;9&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;3.00&#39;, &#39;batters_faced&#39;: &#39;39&#39;, &#39;pitches&#39;: &#39;177&#39;, &#39;strikes_total&#39;: &#39;106&#39;, &#39;strikes_contact&#39;: &#39;59&#39;, &#39;strikes_swinging&#39;: &#39;21&#39;, &#39;strikes_looking&#39;: &#39;26&#39;, &#39;inplay_gb_total&#39;: &#39;16&#39;, &#39;inplay_fb_total&#39;: &#39;8&#39;, &#39;inplay_ld&#39;: &#39;5&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;70&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.449&#39;, &#39;leverage_index_avg&#39;: &#39;1.58&#39;, &#39;re24_def&#39;: &#39;1.7&#39;}, &#39;away_pitchers&#39;: [{&#39;IP&#39;: &#39;5.2&#39;, &#39;H&#39;: &#39;8&#39;, &#39;R&#39;: &#39;4&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;2&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;4.76&#39;, &#39;batters_faced&#39;: &#39;25&#39;, &#39;pitches&#39;: &#39;83&#39;, &#39;strikes_total&#39;: &#39;51&#39;, &#39;strikes_contact&#39;: &#39;32&#39;, &#39;strikes_swinging&#39;: &#39;6&#39;, &#39;strikes_looking&#39;: &#39;13&#39;, &#39;inplay_gb_total&#39;: &#39;13&#39;, &#39;inplay_fb_total&#39;: &#39;8&#39;, &#39;inplay_ld&#39;: &#39;5&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;39&#39;, &#39;inherited_runners&#39;: &#39;&#39;, &#39;inherited_score&#39;: &#39;&#39;, &#39;wpa_def&#39;: &#39;-0.061&#39;, &#39;leverage_index_avg&#39;: &#39;0.86&#39;, &#39;re24_def&#39;: &#39;-0.4&#39;, &#39;name&#39;: &#39;harvema01&#39;}, {&#39;IP&#39;: &#39;1.1&#39;, &#39;H&#39;: &#39;1&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;1&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;5&#39;, &#39;pitches&#39;: &#39;20&#39;, &#39;strikes_total&#39;: &#39;13&#39;, &#39;strikes_contact&#39;: &#39;7&#39;, &#39;strikes_swinging&#39;: &#39;0&#39;, &#39;strikes_looking&#39;: &#39;6&#39;, &#39;inplay_gb_total&#39;: &#39;3&#39;, &#39;inplay_fb_total&#39;: &#39;1&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;1&#39;, &#39;wpa_def&#39;: &#39;-0.022&#39;, &#39;leverage_index_avg&#39;: &#39;0.25&#39;, &#39;re24_def&#39;: &#39;0.0&#39;, &#39;name&#39;: &#39;colonba01&#39;}, {&#39;IP&#39;: &#39;1&#39;, &#39;H&#39;: &#39;0&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;0&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;3&#39;, &#39;pitches&#39;: &#39;11&#39;, &#39;strikes_total&#39;: &#39;7&#39;, &#39;strikes_contact&#39;: &#39;3&#39;, &#39;strikes_swinging&#39;: &#39;1&#39;, &#39;strikes_looking&#39;: &#39;3&#39;, &#39;inplay_gb_total&#39;: &#39;2&#39;, &#39;inplay_fb_total&#39;: &#39;1&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.032&#39;, &#39;leverage_index_avg&#39;: &#39;0.42&#39;, &#39;re24_def&#39;: &#39;0.5&#39;, &#39;name&#39;: &#39;blevije01&#39;}], &#39;home_pitchers&#39;: [{&#39;IP&#39;: &#39;6&#39;, &#39;H&#39;: &#39;2&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;3&#39;, &#39;SO&#39;: &#39;5&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;22&#39;, &#39;pitches&#39;: &#39;106&#39;, &#39;strikes_total&#39;: &#39;62&#39;, &#39;strikes_contact&#39;: &#39;32&#39;, &#39;strikes_swinging&#39;: &#39;14&#39;, &#39;strikes_looking&#39;: &#39;16&#39;, &#39;inplay_gb_total&#39;: &#39;11&#39;, &#39;inplay_fb_total&#39;: &#39;3&#39;, &#39;inplay_ld&#39;: &#39;2&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;70&#39;, &#39;inherited_runners&#39;: &#39;&#39;, &#39;inherited_score&#39;: &#39;&#39;, &#39;wpa_def&#39;: &#39;0.350&#39;, &#39;leverage_index_avg&#39;: &#39;0.92&#39;, &#39;re24_def&#39;: &#39;3.1&#39;, &#39;name&#39;: &#39;volqued01&#39;}, {&#39;IP&#39;: &#39;1&#39;, &#39;H&#39;: &#39;1&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;0&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;4&#39;, &#39;pitches&#39;: &#39;12&#39;, &#39;strikes_total&#39;: &#39;7&#39;, &#39;strikes_contact&#39;: &#39;5&#39;, &#39;strikes_swinging&#39;: &#39;1&#39;, &#39;strikes_looking&#39;: &#39;1&#39;, &#39;inplay_gb_total&#39;: &#39;1&#39;, &#39;inplay_fb_total&#39;: &#39;3&#39;, &#39;inplay_ld&#39;: &#39;1&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.030&#39;, &#39;leverage_index_avg&#39;: &#39;0.59&#39;, &#39;re24_def&#39;: &#39;0.5&#39;, &#39;name&#39;: &#39;herreke01&#39;}, {&#39;IP&#39;: &#39;0.2&#39;, &#39;H&#39;: &#39;3&#39;, &#39;R&#39;: &#39;3&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;1&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;40.50&#39;, &#39;batters_faced&#39;: &#39;7&#39;, &#39;pitches&#39;: &#39;29&#39;, &#39;strikes_total&#39;: &#39;16&#39;, &#39;strikes_contact&#39;: &#39;10&#39;, &#39;strikes_swinging&#39;: &#39;2&#39;, &#39;strikes_looking&#39;: &#39;4&#39;, &#39;inplay_gb_total&#39;: &#39;2&#39;, &#39;inplay_fb_total&#39;: &#39;2&#39;, &#39;inplay_ld&#39;: &#39;2&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;-0.203&#39;, &#39;leverage_index_avg&#39;: &#39;1.80&#39;, &#39;re24_def&#39;: &#39;-2.9&#39;, &#39;name&#39;: &#39;soriajo01&#39;}, {&#39;IP&#39;: &#39;0.1&#39;, &#39;H&#39;: &#39;0&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;1&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;1&#39;, &#39;pitches&#39;: &#39;4&#39;, &#39;strikes_total&#39;: &#39;3&#39;, &#39;strikes_contact&#39;: &#39;1&#39;, &#39;strikes_swinging&#39;: &#39;2&#39;, &#39;strikes_looking&#39;: &#39;0&#39;, &#39;inplay_gb_total&#39;: &#39;0&#39;, &#39;inplay_fb_total&#39;: &#39;0&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.106&#39;, &#39;leverage_index_avg&#39;: &#39;4.08&#39;, &#39;re24_def&#39;: &#39;0.4&#39;, &#39;name&#39;: &#39;hochelu01&#39;}, {&#39;IP&#39;: &#39;1&#39;, &#39;H&#39;: &#39;1&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;1&#39;, &#39;SO&#39;: &#39;2&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;5&#39;, &#39;pitches&#39;: &#39;26&#39;, &#39;strikes_total&#39;: &#39;18&#39;, &#39;strikes_contact&#39;: &#39;11&#39;, &#39;strikes_swinging&#39;: &#39;2&#39;, &#39;strikes_looking&#39;: &#39;5&#39;, &#39;inplay_gb_total&#39;: &#39;2&#39;, &#39;inplay_fb_total&#39;: &#39;0&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.166&#39;, &#39;leverage_index_avg&#39;: &#39;4.62&#39;, &#39;re24_def&#39;: &#39;0.5&#39;, &#39;name&#39;: &#39;daviswa01&#39;}]} . Let&#39;s save our work, so we don&#39;t have to do that download again. . Important: We will be using the files saved in subsequent blog posts. Be sure to save them locally if you run this notebook online. . import pickle pickle.dump(game_data, open(&#39;game_data.pkl&#39;, &#39;wb&#39;)) . Prep Data for Modelling . The idea behind the model is that we care about the difference in the stats for our two opposing teams. So if the home team starting pitcher usually gets a lot more strikeouts than the away team&#39;s pitcher, that&#39;s the number we want to feed the model. As you saw above, we&#39;re going to be doing this for several stats. . Load data into DataFrames . The first task is to load our data into dataframes. We&#39;re going to end up with 4: . game_df: This will be our main dataframe that will eventually get fed into the model | pitching_df: This holds the pitching data, two rows per game (one for how team, one for away team). It&#39;s constructed this way because we need to be able to group our data per team. | batting_df: Same as above but for batting stats. | pitcher_df: This is the same as pitching_df, but has one row per player. We&#39;ll derive our starting pitcher stats from here. | . First, let&#39;s load our saved work. . import pickle game_data = pickle.load(open(&#39;game_data.pkl&#39;, &#39;rb&#39;)) . import pandas as pd games = [] batting = [] pitching = [] pitchers = [] for g in game_data: game_summary = g[&#39;game&#39;] # fix date game_summary[&#39;date&#39;] = game_summary[&#39;date&#39;] + &quot; &quot; + game_summary[&#39;start_time&#39;] del game_summary[&#39;start_time&#39;] # get starting pitchers game_summary[&#39;home_pitcher&#39;] = g[&#39;home_pitchers&#39;][0][&#39;name&#39;] game_summary[&#39;away_pitcher&#39;] = g[&#39;away_pitchers&#39;][0][&#39;name&#39;] # this is the field we&#39;ll train our model to predict game_summary[&#39;home_team_win&#39;] = int(g[&#39;home_batting&#39;][&#39;R&#39;])&gt;int(g[&#39;away_batting&#39;][&#39;R&#39;]) games.append(game_summary) target_pairs = [ (&#39;away_batting&#39;, batting), (&#39;home_batting&#39;, batting), (&#39;away_pitching&#39;, pitching), (&#39;home_pitching&#39;, pitching), (&#39;away_pitchers&#39;, pitchers), (&#39;home_pitchers&#39;, pitchers) ] for key, d in target_pairs: if isinstance(g[key], list): # pitchers for x in g[key]: if &#39;home&#39; in key: x[&#39;is_home_team&#39;] = True x[&#39;team&#39;] = g[&#39;game&#39;][&#39;home_team_abbr&#39;] else: x[&#39;is_home_team&#39;] = False x[&#39;team&#39;] = g[&#39;game&#39;][&#39;away_team_abbr&#39;] x[&#39;game_id&#39;] = g[&#39;game&#39;][&#39;game_id&#39;] d.append(x) else: #batting, pitching x = g[key] if &#39;home&#39; in key: x[&#39;is_home_team&#39;] = True x[&#39;team&#39;] = g[&#39;game&#39;][&#39;home_team_abbr&#39;] else: x[&#39;is_home_team&#39;] = False x[&#39;team&#39;] = g[&#39;game&#39;][&#39;away_team_abbr&#39;] x[&#39;game_id&#39;] = g[&#39;game&#39;][&#39;game_id&#39;] d.append(x) len(games), len(batting), len(pitching), len(pitchers) . (10641, 21282, 21282, 91664) . Game DF . This one is where we&#39;ll eventually put all of our stats . game_df = pd.DataFrame(games) #TODO: fix games that were rescheduled which become NaT after this next command game_df[&#39;date&#39;] = pd.to_datetime(game_df[&#39;date&#39;], errors=&#39;coerce&#39;) game_df = game_df[~game_df[&#39;game_id&#39;].str.contains(&#39;allstar&#39;)].copy() #don&#39;t care about allstar games game_df.head() . game_id away_team_abbr home_team_abbr date home_pitcher away_pitcher home_team_win . 0 KCA201604030 | NYM | KCR | 2016-04-03 19:38:00 | volqued01 | harvema01 | True | . 1 PIT201604030 | STL | PIT | 2016-04-03 13:15:00 | liriafr01 | wainwad01 | True | . 2 TBA201604030 | TOR | TBR | 2016-04-03 16:09:00 | archech01 | stromma01 | False | . 3 ANA201604040 | CHC | LAA | 2016-04-04 19:08:00 | richaga01 | arrieja01 | False | . 4 ARI201604040 | COL | ARI | 2016-04-04 18:42:00 | greinza01 | rosajo01 | False | . Batting DF . Stats about batting, one row per team per game . batting_df = pd.DataFrame(batting) for k in batting_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue batting_df[k] =pd.to_numeric(batting_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) batting_df.drop(columns=[&#39;details&#39;], inplace=True) batting_df.head() . AB R H RBI BB SO PA batting_avg onbase_perc slugging_perc ... wpa_bat leverage_index_avg wpa_bat_pos wpa_bat_neg re24_bat PO A is_home_team team game_id . 0 33.0 | 3.0 | 7.0 | 3.0 | 6.0 | 9.0 | 39.0 | 0.212 | 0.333 | 0.242 | ... | -0.449 | 1.58 | 0.746 | -1.195 | -1.7 | 24.0 | 15.0 | False | NYM | KCA201604030 | . 1 30.0 | 4.0 | 9.0 | 4.0 | 2.0 | 3.0 | 33.0 | 0.300 | 0.333 | 0.300 | ... | 0.052 | 0.74 | 0.488 | -0.434 | -0.1 | 27.0 | 13.0 | True | KCR | KCA201604030 | . 2 32.0 | 1.0 | 5.0 | 1.0 | 5.0 | 14.0 | 38.0 | 0.156 | 0.289 | 0.156 | ... | -0.431 | 1.27 | 0.504 | -0.935 | -3.4 | 24.0 | 11.0 | False | STL | PIT201604030 | . 3 28.0 | 4.0 | 9.0 | 4.0 | 5.0 | 5.0 | 36.0 | 0.321 | 0.429 | 0.464 | ... | 0.070 | 0.71 | 0.466 | -0.394 | 0.1 | 27.0 | 8.0 | True | PIT | PIT201604030 | . 4 35.0 | 5.0 | 7.0 | 5.0 | 3.0 | 16.0 | 38.0 | 0.200 | 0.263 | 0.314 | ... | 0.134 | 0.76 | 0.558 | -0.423 | 0.7 | 27.0 | 15.0 | False | TOR | TBA201604030 | . 5 rows × 23 columns . Pitching DF . Team pitching stats, one row per team per game . pitching_df = pd.DataFrame(pitching) for k in pitching_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue pitching_df[k] =pd.to_numeric(pitching_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) pitching_df.head() . IP H R ER BB SO HR earned_run_avg batters_faced pitches ... inplay_unk game_score inherited_runners inherited_score wpa_def leverage_index_avg re24_def is_home_team team game_id . 0 8.0 | 9.0 | 4.0 | 3.0 | 2.0 | 3.0 | 0.0 | 3.38 | 33.0 | 114.0 | ... | 0.0 | 39.0 | 2.0 | 1.0 | -0.051 | 0.74 | 0.1 | False | NYM | KCA201604030 | . 1 9.0 | 7.0 | 3.0 | 3.0 | 6.0 | 9.0 | 0.0 | 3.00 | 39.0 | 177.0 | ... | 0.0 | 70.0 | 2.0 | 0.0 | 0.449 | 1.58 | 1.7 | True | KCR | KCA201604030 | . 2 8.0 | 9.0 | 4.0 | 4.0 | 5.0 | 5.0 | 0.0 | 4.50 | 36.0 | 144.0 | ... | 0.0 | 48.0 | 0.0 | 0.0 | -0.069 | 0.71 | -0.1 | False | STL | PIT201604030 | . 3 9.0 | 5.0 | 1.0 | 1.0 | 5.0 | 14.0 | 0.0 | 1.00 | 38.0 | 141.0 | ... | 0.0 | 71.0 | 0.0 | 0.0 | 0.431 | 1.27 | 3.4 | True | PIT | PIT201604030 | . 4 9.0 | 7.0 | 3.0 | 3.0 | 1.0 | 7.0 | 1.0 | 3.00 | 36.0 | 118.0 | ... | 0.0 | 62.0 | 1.0 | 1.0 | 0.366 | 0.98 | 1.3 | False | TOR | TBA201604030 | . 5 rows × 27 columns . Pitcher DF . Individual pitching stats (starting pitchers only), one row per pitcher per game . pitcher_df = pd.DataFrame(pitchers) for k in pitcher_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;name&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue pitcher_df[k] =pd.to_numeric(pitcher_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) # filter the pitcher performances to just the starting pitcher pitcher_df = pitcher_df[~pitcher_df[&#39;game_score&#39;].isna()].copy().reset_index(drop=True) pitcher_df.drop(columns=[x for x in pitcher_df.keys() if &#39;inherited&#39; in x], inplace=True) pitcher_df.head() . IP H R ER BB SO HR earned_run_avg batters_faced pitches ... inplay_ld inplay_unk game_score wpa_def leverage_index_avg re24_def name is_home_team team game_id . 0 5.2 | 8.0 | 4.0 | 3.0 | 2.0 | 2.0 | 0.0 | 4.76 | 25.0 | 83.0 | ... | 5.0 | 0.0 | 39.0 | -0.061 | 0.86 | -0.4 | harvema01 | False | NYM | KCA201604030 | . 1 6.0 | 2.0 | 0.0 | 0.0 | 3.0 | 5.0 | 0.0 | 0.00 | 22.0 | 106.0 | ... | 2.0 | 0.0 | 70.0 | 0.350 | 0.92 | 3.1 | volqued01 | True | KCR | KCA201604030 | . 2 6.0 | 6.0 | 3.0 | 3.0 | 3.0 | 3.0 | 0.0 | 4.50 | 26.0 | 96.0 | ... | 6.0 | 0.0 | 48.0 | -0.069 | 0.90 | -0.1 | wainwad01 | False | STL | PIT201604030 | . 3 6.0 | 3.0 | 0.0 | 0.0 | 5.0 | 10.0 | 0.0 | 0.00 | 26.0 | 94.0 | ... | 0.0 | 0.0 | 71.0 | 0.329 | 1.52 | 2.9 | liriafr01 | True | PIT | PIT201604030 | . 4 8.0 | 6.0 | 3.0 | 3.0 | 1.0 | 5.0 | 1.0 | 3.38 | 32.0 | 98.0 | ... | 5.0 | 0.0 | 62.0 | 0.282 | 0.92 | 1.5 | stromma01 | False | TOR | TBA201604030 | . 5 rows × 26 columns . Calculate Differences in the Statistics . Here is where we&#39;re going to generate a bunch of columns in the game_df. Our statistical differences are going to be calculated like this: . For every downloaded statistic: calculate the average, standard deviation and skew: for time periods of 5, 10, 45, 180 and 730 days: grouped by team (or pitcher name in the case of pitchers) then shift the data so each row contains pre-game statistics then take the difference of the opposing team . That result is put into the game_df with a name like &quot;5day_R_Avg&quot;, or &quot;45Day_IP_StDev&quot;. The other dfs are done at that point. . Below are the routines to help execute the above algorithm. . import numpy as np def add_rolling(period, df, stat_columns): for s in stat_columns: if &#39;object&#39; in str(df[s].dtype): continue df[s+&#39;_&#39;+str(period)+&#39;_Avg&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).mean()) df[s+&#39;_&#39;+str(period)+&#39;_Std&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).std()) df[s+&#39;_&#39;+str(period)+&#39;_Skew&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).skew()) return df def get_diff_df(df, name, is_pitcher=False): #runs for each of the stat dataframes, returns the difference in stats #set up dataframe with time index df[&#39;date&#39;] = pd.to_datetime(df[&#39;game_id&#39;].str[3:-1], format=&quot;%Y%m%d&quot;) df = df.sort_values(by=&#39;date&#39;).copy() newindex = df.groupby(&#39;date&#39;)[&#39;date&#39;] .apply(lambda x: x + np.arange(x.size).astype(np.timedelta64)) df = df.set_index(newindex).sort_index() # get stat columns stat_cols = [x for x in df.columns if &#39;int&#39; in str(df[x].dtype)] stat_cols.extend([x for x in df.columns if &#39;float&#39; in str(df[x].dtype)]) #add lags df = add_rolling(&#39;5d&#39;, df, stat_cols) # this game series df = add_rolling(&#39;10d&#39;, df, stat_cols) df = add_rolling(&#39;45d&#39;, df, stat_cols) df = add_rolling(&#39;180d&#39;, df, stat_cols) # this season df = add_rolling(&#39;730d&#39;, df, stat_cols) # 2 years # reset stat columns to just the lags (removing the original stats) df.drop(columns=stat_cols, inplace=True) stat_cols = [x for x in df.columns if &#39;int&#39; in str(df[x].dtype)] stat_cols.extend([x for x in df.columns if &#39;float&#39; in str(df[x].dtype)]) # shift results so that each row is a pregame stat df = df.reset_index(drop=True) df = df.sort_values(by=&#39;date&#39;) for s in stat_cols: if is_pitcher: df[s] = df.groupby(&#39;name&#39;)[s].shift(1) else: df[s] = df.groupby(&#39;team&#39;)[s].shift(1) # calculate differences in pregame stats from home vs. away teams away_df = df[~df[&#39;is_home_team&#39;]].copy() away_df = away_df.set_index(&#39;game_id&#39;) away_df = away_df[stat_cols] home_df = df[df[&#39;is_home_team&#39;]].copy() home_df = home_df.set_index(&#39;game_id&#39;) home_df = home_df[stat_cols] diff_df = home_df.subtract(away_df, fill_value=0) diff_df = diff_df.reset_index() # clean column names for s in stat_cols: diff_df[name + &quot;_&quot; + s] = diff_df[s] diff_df.drop(columns=s, inplace=True) return diff_df . Below calculates the differences for each datafraame of stats, then merges those differences back into the main df. . df = game_df df = pd.merge(left=df, right = get_diff_df(batting_df, &#39;batting&#39;), on = &#39;game_id&#39;, how=&#39;left&#39;) print(df.shape) df = pd.merge(left=df, right = get_diff_df(pitching_df, &#39;pitching&#39;), on = &#39;game_id&#39;, how=&#39;left&#39;) print(df.shape) df = pd.merge(left=df, right = get_diff_df(pitcher_df, &#39;pitcher&#39;,is_pitcher=True), on = &#39;game_id&#39;, how=&#39;left&#39;) df.shape . (10641, 307) (10641, 667) . (10641, 997) . Below we add some datetime features, as I generally do with time-series data like this. . df.dropna(subset=[&#39;date&#39;], inplace=True) df[&#39;season&#39;] = df[&#39;date&#39;].dt.year df[&#39;month&#39;]=df[&#39;date&#39;].dt.month df[&#39;dow&#39;]=df[&#39;date&#39;].dt.weekday df[&#39;date&#39;] = (pd.to_datetime(df[&#39;date&#39;]) - pd.Timestamp(&quot;1970-01-01&quot;)) // pd.Timedelta(&#39;1s&#39;) #epoch time . Save our work . import pickle pickle.dump(df, open(&#39;dataframe.pkl&#39;, &#39;wb&#39;)) . Train the Model . Finally, we get to train the model! This is just going to be a crude first run to make sure everything is working properly. We&#39;ll add the cleverest bits in the next blog post. . First, load our saved saved work . import pickle df = pickle.load(open(&#39;dataframe.pkl&#39;, &#39;rb&#39;)) . Target encoding for remaining string features. This will effectively create a 180-day win% value in each of the team name columns. . encode_me = [x for x in df.keys() if &#39;object&#39; in str(df[x].dtype)] for x in encode_me: df[x] = df.groupby(x)[&#39;home_team_win&#39;].apply(lambda x:x.rolling(180).mean()).shift(1) . Create our test, train and validate data sets . df = df.sort_values(by=&#39;date&#39;).copy().reset_index(drop=True) X = df.drop(columns=[&#39;home_team_win&#39;, &#39;game_id&#39;]) y = df.home_team_win X_train = X[:-1000] y_train = y[:-1000] X_valid = X[-1000:-500] y_valid = y[-1000:-500] X_test = X[-500:] y_test = y[-500:] . Here&#39;s the training bit . import xgboost as xgb params = {&#39;learning_rate&#39;: 0.075,&#39;max_depth&#39;: 2} gbm = xgb.XGBClassifier(**params) model = gbm.fit(X_train, y_train, eval_set = [[X_train, y_train], [X_valid, y_valid]], eval_metric=&#39;logloss&#39;, early_stopping_rounds=10) xgb_test_preds = model.predict(X_test) xgb_test_proba = model.predict_proba(X_test)[:,1] . [0] validation_0-logloss:0.69101 validation_1-logloss:0.69157 Multiple eval metrics have been passed: &#39;validation_1-logloss&#39; will be used for early stopping. Will train until validation_1-logloss hasn&#39;t improved in 10 rounds. [1] validation_0-logloss:0.68916 validation_1-logloss:0.69014 [2] validation_0-logloss:0.68753 validation_1-logloss:0.68861 [3] validation_0-logloss:0.68608 validation_1-logloss:0.68769 [4] validation_0-logloss:0.68467 validation_1-logloss:0.68612 [5] validation_0-logloss:0.68343 validation_1-logloss:0.68522 [6] validation_0-logloss:0.68224 validation_1-logloss:0.68437 [7] validation_0-logloss:0.68121 validation_1-logloss:0.68320 [8] validation_0-logloss:0.68024 validation_1-logloss:0.68275 [9] validation_0-logloss:0.67937 validation_1-logloss:0.68241 [10] validation_0-logloss:0.67856 validation_1-logloss:0.68224 [11] validation_0-logloss:0.67783 validation_1-logloss:0.68179 [12] validation_0-logloss:0.67713 validation_1-logloss:0.68142 [13] validation_0-logloss:0.67650 validation_1-logloss:0.68159 [14] validation_0-logloss:0.67587 validation_1-logloss:0.68137 [15] validation_0-logloss:0.67525 validation_1-logloss:0.68115 [16] validation_0-logloss:0.67473 validation_1-logloss:0.68124 [17] validation_0-logloss:0.67418 validation_1-logloss:0.68110 [18] validation_0-logloss:0.67366 validation_1-logloss:0.68083 [19] validation_0-logloss:0.67315 validation_1-logloss:0.68089 [20] validation_0-logloss:0.67261 validation_1-logloss:0.68037 [21] validation_0-logloss:0.67212 validation_1-logloss:0.68039 [22] validation_0-logloss:0.67168 validation_1-logloss:0.68038 [23] validation_0-logloss:0.67125 validation_1-logloss:0.68066 [24] validation_0-logloss:0.67081 validation_1-logloss:0.68100 [25] validation_0-logloss:0.67035 validation_1-logloss:0.68140 [26] validation_0-logloss:0.66993 validation_1-logloss:0.68117 [27] validation_0-logloss:0.66955 validation_1-logloss:0.68088 [28] validation_0-logloss:0.66920 validation_1-logloss:0.68089 [29] validation_0-logloss:0.66888 validation_1-logloss:0.68093 [30] validation_0-logloss:0.66853 validation_1-logloss:0.68092 Stopping. Best iteration: [20] validation_0-logloss:0.67261 validation_1-logloss:0.68037 . And our results... . from sklearn.calibration import calibration_curve from sklearn.metrics import accuracy_score, brier_score_loss import matplotlib.pyplot as plt import pickle def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 8)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_pred, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} t tAccuracy:{:.4f} t Brier Loss: {:.4f}&quot;.format( name, accuracy_score(y_test, y_pred), brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() outcomes,predictions,probabilities = pickle.load(open(&#39;baseline.pkl&#39;,&#39;rb&#39;)) data = [ (outcomes, predictions, probabilities, &#39;Casino&#39;), (y_test,xgb_test_preds, xgb_test_proba, &#39;XGBoost&#39;) ] cal_curve(data, 15) . . Casino Accuracy:0.6006 Brier Loss: 0.2358 XGBoost Accuracy:0.5680 Brier Loss: 0.2405 . That&#39;s not terrible for a first run - ~57% accuracy for our model vs. 60% for the casinos. We&#39;re only off by 3% and we still have our secret features and model optimization to do. . Our calibration is also worse, but again it&#39;s not at all terrible for our first run. I like to think of it in terms of forecasting skill. Our calibration skill is 2.0% worse than the casino&#39;s, as measured by our brier loss function. . This model has some promise. Let&#39;s look at the feature importances just to see which features it&#39;s using most. It looks like the model likes RE24, and it likes the longer lags: 180day &amp; 730day. . import pandas as pd x = pd.Series(model.get_booster().get_score(importance_type= &#39;total_gain&#39;) ).sort_values() x[-25:].plot(kind=&#39;barh&#39;,title=&quot;XGBoost Feature Gain&quot;) . . &lt;AxesSubplot:title={&#39;center&#39;:&#39;XGBoost Feature Gain&#39;}&gt; . Next Up . In Part 3, we&#39;re going to add team power rankings and casino odds to the dataset to improve the results further! .",
            "url": "https://rdpharr.github.io/project_notes/baseball/web_scraping/xgboost/brier/accuracy/calibration/2020/09/21/MLB-Part2-First-Model.html",
            "relUrl": "/baseball/web_scraping/xgboost/brier/accuracy/calibration/2020/09/21/MLB-Part2-First-Model.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "MLB Baseball Predictions",
            "content": "This is the second season I&#39;ve been using machine learning to make predictions and bets. Last year I made good predictions, but I hadn&#39;t figured out how to size my bets so I didn&#39;t make any money. This year I feel like I have a good strategy for this and it was solidly profitable. . I&#39;m going to share what I do in this series of blog posts. Hopefully I&#39;ll get some feedback that will help me improve. If not, at least it might help others get started. . This blog series is written in jupyter notebooks, which will show you how to build a program that predicts the outcome of MLB games. We&#39;ll be using our web scraping and machine learning skills to build a model that significantly outperforms the casino&#39;s sports books. . Each blog post, including this one, is executable. Use the buttons at the top to run the code on Binder of Colab and get fresh results for yourself. You can also download it from Github to run the notebook locally. . Important: Web scraping is dependant on other people&#8217;s web pages. If they change their site, this blog&#8217;s code will break. Don&#8217;t expect the code presented here to work forever. . Benchmarking the Sportsbooks . First thing to do is figure out how we’re going to know if we’re doing well. The most intuitive performance benchmark I found was the sportsbooks themselves. If I can make better predictions than the sportsbooks, then I should be doing well. . Downloading Sportsbook Data . We need to start by putting together a database of historic odds and outcomes for MLB games. First step is to get a list of days when games were played. We can get those from baseball-reference.com. . import requests import re import datetime as dt url = &#39;https://www.baseball-reference.com/leagues/MLB/2019-schedule.shtml&#39; resp = requests.get(url) # All the H3 tags contain day names days = re.findall(&quot;&lt;h3&gt;(.*2019)&lt;/h3&gt;&quot;, resp.text) dates = [dt.datetime.strptime(d,&quot;%A, %B %d, %Y&quot;) for d in days] print(&quot;Number of days MLB was played in 2019:&quot;, len(dates)) . Number of days MLB was played in 2019: 210 . We need the correct days because we&#39;ll be pulling the odds data from covers.com by day. Covers aggregates the published odds from several sources and then publishes a consensus moneyline. We&#39;ll grab that, along with the score of the game. Here&#39;s how we pull and parse that data. . from bs4 import BeautifulSoup as bs game_data = [] for d in dates: # get the web page with game data on it game_day = d.strftime(&#39;%Y-%m-%d&#39;) url = f&#39;https://www.covers.com/Sports/MLB/Matchups?selectedDate={game_day}&#39; resp = requests.get(url) # parse the games scraped_games = bs(resp.text).findAll(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_game_box&#39;}) for g in scraped_games: game = {} game[&#39;home_moneyline&#39;] = g[&#39;data-game-odd&#39;] game[&#39;date&#39;] = g[&#39;data-game-date&#39;] try: game[&#39;home_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_home&#39;}).text.strip() game[&#39;away_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_away&#39;}).text.strip() except: game[&#39;home_score&#39;] =&#39;&#39; game[&#39;away_score&#39;] =&#39;&#39; game_data.append(game) if len(game_data) % 500==0: #show progress print(dt.datetime.now(), game_day, len(game_data)) print(&quot;Done! Games downloaded:&quot;, len(game_data)) . 2020-09-22 10:18:28.135002 2019-05-02 500 2020-09-22 10:18:56.497463 2019-06-08 1000 2020-09-22 10:19:23.908988 2019-07-18 1500 2020-09-22 10:19:51.687110 2019-08-24 2000 2020-09-22 10:20:19.900455 2019-10-03 2500 Done! Games downloaded: 2533 . Here&#39;s what that data looks like. You can see the moneyline was negative, meaning that the home team was favored. But the home team lost, so the prediction from the casinos was inaccurate. . game_data[0] . {&#39;home_moneyline&#39;: &#39;-155&#39;, &#39;date&#39;: &#39;2019-03-20 05:35:00&#39;, &#39;home_score&#39;: &#39;7&#39;, &#39;away_score&#39;: &#39;9&#39;} . That would have been a pretty good payout if you bet on the away team. Let&#39;s save our data so we don&#39;t need to keep downloading it. . Important: We will be using the files saved in subsequent blog posts. Be sure to save them locally if you run this notebook online. . import pickle pickle.dump(game_data, open(&#39;covers_data.pkl&#39;,&#39;wb&#39;)) . Sportsbook Accuracy . Let&#39;s see how the sportsbook did in all the games we just downloaded. . from sklearn.metrics import accuracy_score # the actual outcome of the game, true if the the home team won outcomes = [] # predictions derived from moneyline odds. True if the home team was the favorite predictions = [] # probability the home team will win, derived from moneyline odds # derived from formulas at https://www.bettingexpert.com/academy/advanced-betting-theory/odds-conversion-to-percentage probabilities = [] for d in game_data: try: moneyline = int(d[&#39;home_moneyline&#39;]) home_score = int(d[&#39;home_score&#39;]) away_score = int(d[&#39;away_score&#39;]) except: #incomplete data continue if moneyline==100: # it&#39;s rare to have a tossup since covers is averaging the odds from several sports books # but we&#39;ll exclude them from our calculations continue # convert moneyline odds ot their implied probabilities if moneyline&lt;0: probabilities.append(-moneyline/(-moneyline + 100)) elif moneyline&gt;100: probabilities.append(100/(moneyline + 100)) outcomes.append(home_score&gt;away_score) predictions.append(moneyline&lt;0) print(&quot;Sportsbook accuracy (excluding tossups): {0:.2f}%&quot;.format(100*accuracy_score(outcomes,predictions))) . Sportsbook accuracy (excluding tossups): 60.06% . That&#39;s it, right? We need a model that is better than 60% accurate. . If you plan to use this data for betting, you should have more than a win/loss prediction. To really make money, we would like to know if we think the odds of a team winning are better or worse that what the sportsbook thinks they are. Then we&#39;d be able to use some sort of expected value calculation to determine if the bet is profitable. . Sportsbook Calibration . We really want to know if we can build a model that is better calibrated than the casino&#39;s sportsbooks. Knowing our calibration will help us with bet sizing, as well as more sophisticated betting algorithms. Here&#39;s a graphical view of the calibration of the casino sports book data. . from sklearn.calibration import calibration_curve from sklearn.metrics import accuracy_score, brier_score_loss import matplotlib.pyplot as plt def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 8)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_pred, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} tAccuracy:{:.4f} t Brier Loss: {:.4f}&quot;.format( name, accuracy_score(y_test, y_pred), brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() data = [(outcomes, predictions, probabilities, &#39;SportsBook&#39;)] cal_curve(data, 15) . . SportsBook Accuracy:0.6006 Brier Loss: 0.2358 . The graph above tells us several things about the calibration of the casino&#39;s predictions. The reliability curve clearly shows that the casino is highly calibrated. Interestingly, it looks like the blue line is shifted down slightly from the &quot;perfectly calibrated&quot; line. It would be a better fit if it was 0.05 higher. This may account for the house advantage. . The histogram below shows what portion of the games fall into each bin. We see a slight predicted advantage to the home team, with more than 50% of the observations above the 50% mark. Otherwise it looks pretty normally distributed. . Above, I said the reliability curve looks highly calibrated. If we are to judge our own efforts against the sportsbook, we can&#39;t just be eyeballing this graph all the time. A metric would be nice. One metric that is suited for calibration measurement is the Brier Score, which I&#39;ll be using to measure the model effectiveness going forward. Getting a model that scores less than 0.2358 is the target for our efforts. . Before we go, we should save the files for our baseline... . import pickle pickle.dump((outcomes,predictions, probabilities), open(&#39;baseline.pkl&#39;,&#39;wb&#39;)) . Next Up . Next, we&#39;ll start building out our historic data and training the model using XGBoost and LightGBM. .",
            "url": "https://rdpharr.github.io/project_notes/baseball/benchmark/web_scraping/brier/accuracy/calibration/2020/09/20/baseball_project.html",
            "relUrl": "/baseball/benchmark/web_scraping/brier/accuracy/calibration/2020/09/20/baseball_project.html",
            "date": " • Sep 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a semi-retired tech professional, living in Las Vegas, Nevada. I’m forever in school, right now at University of the People. I do work to end racial injustices, focusing on criminal justice reform with Mass Liberation of Nevada. . You can get in touch with me using the links at the bottom of this page. . About the Site . This website is powered by fastpages. .",
          "url": "https://rdpharr.github.io/project_notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rdpharr.github.io/project_notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}