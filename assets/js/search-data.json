{
  
    
        "post0": {
            "title": "MLB Training Data",
            "content": "This is the second post in my series on MLB Baseball Betting, and it&#39;s a long one. Here we&#39;re going to have a complete working model from scraping the data, processing it, adding features and finally giving it a run in XGBoost. . This is all done in a single notebook so you can start fiddling with it on your own as soon as you want. Use the buttons on the top to download your own copy from github or run it in the cloud on one of the services listed. . Scrape Data . For this model, I&#39;m going to use the stats from the baseball-reference.com box scores page. Here is and example of the page we&#39;ll be scraping. Let&#39;s get started. . Create List of Games to Download . We&#39;ll get the games from the schedule pages, creating a list of links to each game. Let&#39;s start in 2016, that should give us enough games to start making inferences. . import requests from bs4 import BeautifulSoup as bs game_links = [] for current_year in range(2016,2021): url = f&quot;https://www.baseball-reference.com/leagues/MLB/{current_year}-schedule.shtml&quot; resp = requests.get(url) soup=bs(resp.text) games = soup.findAll(&#39;a&#39;,text=&#39;Boxscore&#39;) game_links.extend([x[&#39;href&#39;] for x in games]) print(&quot;Number of games to download: &quot;, len(game_links)) game_links[0] . Number of games to download: 10641 . &#39;/boxes/KCA/KCA201604030.shtml&#39; . Download Game Data . For each game, I want to download team performance in batting and pitching, as well as individual performance pitching. For 10K games, this is going to take a little while to build out. . # these are functions related to parsing the baseball reference page def get_game_summary(soup, game_id): game = {&#39;game_id&#39;: game_id} scorebox = soup.find(&#39;div&#39;, {&#39;class&#39;:&#39;scorebox&#39;}) teams = scorebox.findAll(&#39;a&#39;,{&#39;itemprop&#39;:&#39;name&#39;}) game[&#39;away_team_abbr&#39;] = teams[0][&#39;href&#39;].split(&#39;/&#39;)[2] game[&#39;home_team_abbr&#39;] = teams[1][&#39;href&#39;].split(&#39;/&#39;)[2] meta = scorebox.find(&#39;div&#39;, {&#39;class&#39;:&#39;scorebox_meta&#39;}).findAll(&#39;div&#39;) game[&#39;date&#39;] = meta[0].text.strip() game[&#39;start_time&#39;] = meta[1].text[12:-6].strip() return game def get_table_summary(soup, table_no): stats_tables = soup.findAll(&#39;table&#39;, {&#39;class&#39;:&#39;stats_table&#39;}) t = stats_tables[table_no].find(&#39;tfoot&#39;) summary = {x[&#39;data-stat&#39;]:x.text.strip() for x in t.findAll(&#39;td&#39;)} return summary def get_pitcher_data(soup, table_no): stats_tables = soup.findAll(&#39;table&#39;, {&#39;class&#39;:&#39;stats_table&#39;}) t = stats_tables[table_no] data = [] rows = t.findAll(&#39;tr&#39;)[1:-1] # not the header and footer rows for r in rows: summary = {x[&#39;data-stat&#39;]:x.text.strip() for x in r.findAll(&#39;td&#39;)} summary[&#39;name&#39;] = r.find(&#39;th&#39;,{&#39;data-stat&#39;:&#39;player&#39;}).find(&#39;a&#39;)[&#39;href&#39;].split(&#39;/&#39;)[-1][:-6].strip() data.append(summary) return data def process_link(url): resp = requests.get(url) game_id = url.split(&#39;/&#39;)[-1][:-6] # strange preprocessing routine uncommented_html = &#39;&#39; for h in resp.text.split(&#39; n&#39;): if &#39;&lt;!-- &lt;div&#39; in h: continue if h.strip() == &#39;&lt;!--&#39;: continue if h.strip() == &#39;--&gt;&#39;: continue uncommented_html += h + &#39; n&#39; soup = bs(uncommented_html) data = { &#39;game&#39;: get_game_summary(soup, game_id), &#39;away_batting&#39;: get_table_summary(soup, 1), &#39;home_batting&#39;:get_table_summary(soup, 2), &#39;away_pitching&#39;:get_table_summary(soup, 3), &#39;home_pitching&#39;:get_table_summary(soup, 4), &#39;away_pitchers&#39;: get_pitcher_data(soup, 3), &#39;home_pitchers&#39;: get_pitcher_data(soup, 4) } return data . . import datetime as dt game_data = [] for link in game_links: url = &#39;https://www.baseball-reference.com&#39; + link game_data.append(process_link(url)) if len(game_data)%1000==0: print(dt.datetime.now().time(), len(game_data)) . 21:44:35.084156 1000 21:49:42.274916 2000 21:54:54.916589 3000 22:00:10.507164 4000 22:05:22.761320 5000 22:10:34.759694 6000 22:15:47.269227 7000 22:21:05.776663 8000 22:26:17.596819 9000 22:31:37.716245 10000 . That took a while. We could definitely speed it up with threading, but we want to be nice to their servers. They seem like good people, making their data available to everyone. . Here is what a single game looks like. It&#39;s actually quite a bit of data. . game_data[0] . {&#39;game&#39;: {&#39;game_id&#39;: &#39;KCA201604030&#39;, &#39;away_team_abbr&#39;: &#39;NYM&#39;, &#39;home_team_abbr&#39;: &#39;KCR&#39;, &#39;date&#39;: &#39;Sunday, April 3, 2016&#39;, &#39;start_time&#39;: &#39;7:38 p.m.&#39;}, &#39;away_batting&#39;: {&#39;AB&#39;: &#39;33&#39;, &#39;R&#39;: &#39;3&#39;, &#39;H&#39;: &#39;7&#39;, &#39;RBI&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;6&#39;, &#39;SO&#39;: &#39;9&#39;, &#39;PA&#39;: &#39;39&#39;, &#39;batting_avg&#39;: &#39;.212&#39;, &#39;onbase_perc&#39;: &#39;.333&#39;, &#39;slugging_perc&#39;: &#39;.242&#39;, &#39;onbase_plus_slugging&#39;: &#39;.576&#39;, &#39;pitches&#39;: &#39;177&#39;, &#39;strikes_total&#39;: &#39;105&#39;, &#39;wpa_bat&#39;: &#39;-0.449&#39;, &#39;leverage_index_avg&#39;: &#39;1.58&#39;, &#39;wpa_bat_pos&#39;: &#39;0.746&#39;, &#39;wpa_bat_neg&#39;: &#39;-1.195&#39;, &#39;re24_bat&#39;: &#39;-1.7&#39;, &#39;PO&#39;: &#39;24&#39;, &#39;A&#39;: &#39;15&#39;, &#39;details&#39;: &#39;&#39;}, &#39;home_batting&#39;: {&#39;AB&#39;: &#39;30&#39;, &#39;R&#39;: &#39;4&#39;, &#39;H&#39;: &#39;9&#39;, &#39;RBI&#39;: &#39;4&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;3&#39;, &#39;PA&#39;: &#39;33&#39;, &#39;batting_avg&#39;: &#39;.300&#39;, &#39;onbase_perc&#39;: &#39;.333&#39;, &#39;slugging_perc&#39;: &#39;.300&#39;, &#39;onbase_plus_slugging&#39;: &#39;.633&#39;, &#39;pitches&#39;: &#39;114&#39;, &#39;strikes_total&#39;: &#39;71&#39;, &#39;wpa_bat&#39;: &#39;0.052&#39;, &#39;leverage_index_avg&#39;: &#39;0.74&#39;, &#39;wpa_bat_pos&#39;: &#39;0.488&#39;, &#39;wpa_bat_neg&#39;: &#39;-0.434&#39;, &#39;re24_bat&#39;: &#39;-0.1&#39;, &#39;PO&#39;: &#39;27&#39;, &#39;A&#39;: &#39;13&#39;, &#39;details&#39;: &#39;&#39;}, &#39;away_pitching&#39;: {&#39;IP&#39;: &#39;8&#39;, &#39;H&#39;: &#39;9&#39;, &#39;R&#39;: &#39;4&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;3&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;3.38&#39;, &#39;batters_faced&#39;: &#39;33&#39;, &#39;pitches&#39;: &#39;114&#39;, &#39;strikes_total&#39;: &#39;71&#39;, &#39;strikes_contact&#39;: &#39;42&#39;, &#39;strikes_swinging&#39;: &#39;7&#39;, &#39;strikes_looking&#39;: &#39;22&#39;, &#39;inplay_gb_total&#39;: &#39;18&#39;, &#39;inplay_fb_total&#39;: &#39;10&#39;, &#39;inplay_ld&#39;: &#39;5&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;39&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;1&#39;, &#39;wpa_def&#39;: &#39;-0.051&#39;, &#39;leverage_index_avg&#39;: &#39;0.74&#39;, &#39;re24_def&#39;: &#39;0.1&#39;}, &#39;home_pitching&#39;: {&#39;IP&#39;: &#39;9&#39;, &#39;H&#39;: &#39;7&#39;, &#39;R&#39;: &#39;3&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;6&#39;, &#39;SO&#39;: &#39;9&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;3.00&#39;, &#39;batters_faced&#39;: &#39;39&#39;, &#39;pitches&#39;: &#39;177&#39;, &#39;strikes_total&#39;: &#39;106&#39;, &#39;strikes_contact&#39;: &#39;59&#39;, &#39;strikes_swinging&#39;: &#39;21&#39;, &#39;strikes_looking&#39;: &#39;26&#39;, &#39;inplay_gb_total&#39;: &#39;16&#39;, &#39;inplay_fb_total&#39;: &#39;8&#39;, &#39;inplay_ld&#39;: &#39;5&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;70&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.449&#39;, &#39;leverage_index_avg&#39;: &#39;1.58&#39;, &#39;re24_def&#39;: &#39;1.7&#39;}, &#39;away_pitchers&#39;: [{&#39;IP&#39;: &#39;5.2&#39;, &#39;H&#39;: &#39;8&#39;, &#39;R&#39;: &#39;4&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;2&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;4.76&#39;, &#39;batters_faced&#39;: &#39;25&#39;, &#39;pitches&#39;: &#39;83&#39;, &#39;strikes_total&#39;: &#39;51&#39;, &#39;strikes_contact&#39;: &#39;32&#39;, &#39;strikes_swinging&#39;: &#39;6&#39;, &#39;strikes_looking&#39;: &#39;13&#39;, &#39;inplay_gb_total&#39;: &#39;13&#39;, &#39;inplay_fb_total&#39;: &#39;8&#39;, &#39;inplay_ld&#39;: &#39;5&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;39&#39;, &#39;inherited_runners&#39;: &#39;&#39;, &#39;inherited_score&#39;: &#39;&#39;, &#39;wpa_def&#39;: &#39;-0.061&#39;, &#39;leverage_index_avg&#39;: &#39;0.86&#39;, &#39;re24_def&#39;: &#39;-0.4&#39;, &#39;name&#39;: &#39;harvema01&#39;}, {&#39;IP&#39;: &#39;1.1&#39;, &#39;H&#39;: &#39;1&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;1&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;5&#39;, &#39;pitches&#39;: &#39;20&#39;, &#39;strikes_total&#39;: &#39;13&#39;, &#39;strikes_contact&#39;: &#39;7&#39;, &#39;strikes_swinging&#39;: &#39;0&#39;, &#39;strikes_looking&#39;: &#39;6&#39;, &#39;inplay_gb_total&#39;: &#39;3&#39;, &#39;inplay_fb_total&#39;: &#39;1&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;1&#39;, &#39;wpa_def&#39;: &#39;-0.022&#39;, &#39;leverage_index_avg&#39;: &#39;0.25&#39;, &#39;re24_def&#39;: &#39;0.0&#39;, &#39;name&#39;: &#39;colonba01&#39;}, {&#39;IP&#39;: &#39;1&#39;, &#39;H&#39;: &#39;0&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;0&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;3&#39;, &#39;pitches&#39;: &#39;11&#39;, &#39;strikes_total&#39;: &#39;7&#39;, &#39;strikes_contact&#39;: &#39;3&#39;, &#39;strikes_swinging&#39;: &#39;1&#39;, &#39;strikes_looking&#39;: &#39;3&#39;, &#39;inplay_gb_total&#39;: &#39;2&#39;, &#39;inplay_fb_total&#39;: &#39;1&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.032&#39;, &#39;leverage_index_avg&#39;: &#39;0.42&#39;, &#39;re24_def&#39;: &#39;0.5&#39;, &#39;name&#39;: &#39;blevije01&#39;}], &#39;home_pitchers&#39;: [{&#39;IP&#39;: &#39;6&#39;, &#39;H&#39;: &#39;2&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;3&#39;, &#39;SO&#39;: &#39;5&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;22&#39;, &#39;pitches&#39;: &#39;106&#39;, &#39;strikes_total&#39;: &#39;62&#39;, &#39;strikes_contact&#39;: &#39;32&#39;, &#39;strikes_swinging&#39;: &#39;14&#39;, &#39;strikes_looking&#39;: &#39;16&#39;, &#39;inplay_gb_total&#39;: &#39;11&#39;, &#39;inplay_fb_total&#39;: &#39;3&#39;, &#39;inplay_ld&#39;: &#39;2&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;70&#39;, &#39;inherited_runners&#39;: &#39;&#39;, &#39;inherited_score&#39;: &#39;&#39;, &#39;wpa_def&#39;: &#39;0.350&#39;, &#39;leverage_index_avg&#39;: &#39;0.92&#39;, &#39;re24_def&#39;: &#39;3.1&#39;, &#39;name&#39;: &#39;volqued01&#39;}, {&#39;IP&#39;: &#39;1&#39;, &#39;H&#39;: &#39;1&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;0&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;4&#39;, &#39;pitches&#39;: &#39;12&#39;, &#39;strikes_total&#39;: &#39;7&#39;, &#39;strikes_contact&#39;: &#39;5&#39;, &#39;strikes_swinging&#39;: &#39;1&#39;, &#39;strikes_looking&#39;: &#39;1&#39;, &#39;inplay_gb_total&#39;: &#39;1&#39;, &#39;inplay_fb_total&#39;: &#39;3&#39;, &#39;inplay_ld&#39;: &#39;1&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.030&#39;, &#39;leverage_index_avg&#39;: &#39;0.59&#39;, &#39;re24_def&#39;: &#39;0.5&#39;, &#39;name&#39;: &#39;herreke01&#39;}, {&#39;IP&#39;: &#39;0.2&#39;, &#39;H&#39;: &#39;3&#39;, &#39;R&#39;: &#39;3&#39;, &#39;ER&#39;: &#39;3&#39;, &#39;BB&#39;: &#39;2&#39;, &#39;SO&#39;: &#39;1&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;40.50&#39;, &#39;batters_faced&#39;: &#39;7&#39;, &#39;pitches&#39;: &#39;29&#39;, &#39;strikes_total&#39;: &#39;16&#39;, &#39;strikes_contact&#39;: &#39;10&#39;, &#39;strikes_swinging&#39;: &#39;2&#39;, &#39;strikes_looking&#39;: &#39;4&#39;, &#39;inplay_gb_total&#39;: &#39;2&#39;, &#39;inplay_fb_total&#39;: &#39;2&#39;, &#39;inplay_ld&#39;: &#39;2&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;-0.203&#39;, &#39;leverage_index_avg&#39;: &#39;1.80&#39;, &#39;re24_def&#39;: &#39;-2.9&#39;, &#39;name&#39;: &#39;soriajo01&#39;}, {&#39;IP&#39;: &#39;0.1&#39;, &#39;H&#39;: &#39;0&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;0&#39;, &#39;SO&#39;: &#39;1&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;1&#39;, &#39;pitches&#39;: &#39;4&#39;, &#39;strikes_total&#39;: &#39;3&#39;, &#39;strikes_contact&#39;: &#39;1&#39;, &#39;strikes_swinging&#39;: &#39;2&#39;, &#39;strikes_looking&#39;: &#39;0&#39;, &#39;inplay_gb_total&#39;: &#39;0&#39;, &#39;inplay_fb_total&#39;: &#39;0&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;2&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.106&#39;, &#39;leverage_index_avg&#39;: &#39;4.08&#39;, &#39;re24_def&#39;: &#39;0.4&#39;, &#39;name&#39;: &#39;hochelu01&#39;}, {&#39;IP&#39;: &#39;1&#39;, &#39;H&#39;: &#39;1&#39;, &#39;R&#39;: &#39;0&#39;, &#39;ER&#39;: &#39;0&#39;, &#39;BB&#39;: &#39;1&#39;, &#39;SO&#39;: &#39;2&#39;, &#39;HR&#39;: &#39;0&#39;, &#39;earned_run_avg&#39;: &#39;0.00&#39;, &#39;batters_faced&#39;: &#39;5&#39;, &#39;pitches&#39;: &#39;26&#39;, &#39;strikes_total&#39;: &#39;18&#39;, &#39;strikes_contact&#39;: &#39;11&#39;, &#39;strikes_swinging&#39;: &#39;2&#39;, &#39;strikes_looking&#39;: &#39;5&#39;, &#39;inplay_gb_total&#39;: &#39;2&#39;, &#39;inplay_fb_total&#39;: &#39;0&#39;, &#39;inplay_ld&#39;: &#39;0&#39;, &#39;inplay_unk&#39;: &#39;0&#39;, &#39;game_score&#39;: &#39;&#39;, &#39;inherited_runners&#39;: &#39;0&#39;, &#39;inherited_score&#39;: &#39;0&#39;, &#39;wpa_def&#39;: &#39;0.166&#39;, &#39;leverage_index_avg&#39;: &#39;4.62&#39;, &#39;re24_def&#39;: &#39;0.5&#39;, &#39;name&#39;: &#39;daviswa01&#39;}]} . import pickle pickle.dump(game_data, open(&#39;game_data.pkl&#39;, &#39;wb&#39;)) . Prep Data for Modelling . The idea behind the model is that we care about the difference in the stats for our two opposing teams. So if the home team starting pitcher usually gets a lot more strikeouts than the away team&#39;s pitcher, that&#39;s the number we want to feed the model. As you saw above, we&#39;re going to be doing this for several stats. . Load data into DataFrames . The first task is to load our data into dataframes. We&#39;re going to end up with 4: . game_df: This will be our main dataframe that will eventually get fed into the model | pitching_df: This holds the pitching data, two rows per game (one for how team, one for away team). It&#39;s constructed this way because we need to be able to group our data per team. | batting_df: Same as above but for batting stats. | pitcher_df: This is the same as pitching_df, but has one row per player. We&#39;ll derive our starting pitcher stats from here. | . import pickle game_data = pickle.load(open(&#39;game_data.pkl&#39;, &#39;rb&#39;)) . import pandas as pd games = [] batting = [] pitching = [] pitchers = [] for g in game_data: game_summary = g[&#39;game&#39;] # fix date game_summary[&#39;date&#39;] = game_summary[&#39;date&#39;] + &quot; &quot; + game_summary[&#39;start_time&#39;] del game_summary[&#39;start_time&#39;] # this is the field we&#39;ll train our model to predict game_summary[&#39;home_team_win&#39;] = int(g[&#39;home_batting&#39;][&#39;R&#39;])&gt;int(g[&#39;away_batting&#39;][&#39;R&#39;]) games.append(game_summary) target_pairs = [ (&#39;away_batting&#39;, batting), (&#39;home_batting&#39;, batting), (&#39;away_pitching&#39;, pitching), (&#39;home_pitching&#39;, pitching), (&#39;away_pitchers&#39;, pitchers), (&#39;home_pitchers&#39;, pitchers) ] for key, d in target_pairs: if isinstance(g[key], list): # pitchers for x in g[key]: if &#39;home&#39; in key: x[&#39;is_home_team&#39;] = True x[&#39;team&#39;] = g[&#39;game&#39;][&#39;home_team_abbr&#39;] else: x[&#39;is_home_team&#39;] = False x[&#39;team&#39;] = g[&#39;game&#39;][&#39;away_team_abbr&#39;] x[&#39;game_id&#39;] = g[&#39;game&#39;][&#39;game_id&#39;] d.append(x) else: #batting, pitching x = g[key] if &#39;home&#39; in key: x[&#39;is_home_team&#39;] = True x[&#39;team&#39;] = g[&#39;game&#39;][&#39;home_team_abbr&#39;] else: x[&#39;is_home_team&#39;] = False x[&#39;team&#39;] = g[&#39;game&#39;][&#39;away_team_abbr&#39;] x[&#39;game_id&#39;] = g[&#39;game&#39;][&#39;game_id&#39;] d.append(x) len(games), len(batting), len(pitching), len(pitchers) . (10641, 21282, 21282, 91664) . Game DF . This one is where we&#39;ll eventually put all of our stats . game_df = pd.DataFrame(games) #TODO: fix games that were rescheduled which become NaT after this next command game_df[&#39;date&#39;] = pd.to_datetime(game_df[&#39;date&#39;], errors=&#39;coerce&#39;) game_df = game_df[~game_df[&#39;game_id&#39;].str.contains(&#39;allstar&#39;)].copy() #don&#39;t care about allstar games game_df.head() . game_id away_team_abbr home_team_abbr date home_team_win . 0 KCA201604030 | NYM | KCR | 2016-04-03 19:38:00 | True | . 1 PIT201604030 | STL | PIT | 2016-04-03 13:15:00 | True | . 2 TBA201604030 | TOR | TBR | 2016-04-03 16:09:00 | False | . 3 ANA201604040 | CHC | LAA | 2016-04-04 19:08:00 | False | . 4 ARI201604040 | COL | ARI | 2016-04-04 18:42:00 | False | . Batting DF . Stats about batting, one row per team per game . batting_df = pd.DataFrame(batting) for k in batting_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue batting_df[k] =pd.to_numeric(batting_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) batting_df.drop(columns=[&#39;details&#39;], inplace=True) batting_df.head() . AB R H RBI BB SO PA batting_avg onbase_perc slugging_perc ... wpa_bat leverage_index_avg wpa_bat_pos wpa_bat_neg re24_bat PO A is_home_team team game_id . 0 33.0 | 3.0 | 7.0 | 3.0 | 6.0 | 9.0 | 39.0 | 0.212 | 0.333 | 0.242 | ... | -0.449 | 1.58 | 0.746 | -1.195 | -1.7 | 24.0 | 15.0 | False | NYM | KCA201604030 | . 1 30.0 | 4.0 | 9.0 | 4.0 | 2.0 | 3.0 | 33.0 | 0.300 | 0.333 | 0.300 | ... | 0.052 | 0.74 | 0.488 | -0.434 | -0.1 | 27.0 | 13.0 | True | KCR | KCA201604030 | . 2 32.0 | 1.0 | 5.0 | 1.0 | 5.0 | 14.0 | 38.0 | 0.156 | 0.289 | 0.156 | ... | -0.431 | 1.27 | 0.504 | -0.935 | -3.4 | 24.0 | 11.0 | False | STL | PIT201604030 | . 3 28.0 | 4.0 | 9.0 | 4.0 | 5.0 | 5.0 | 36.0 | 0.321 | 0.429 | 0.464 | ... | 0.070 | 0.71 | 0.466 | -0.394 | 0.1 | 27.0 | 8.0 | True | PIT | PIT201604030 | . 4 35.0 | 5.0 | 7.0 | 5.0 | 3.0 | 16.0 | 38.0 | 0.200 | 0.263 | 0.314 | ... | 0.134 | 0.76 | 0.558 | -0.423 | 0.7 | 27.0 | 15.0 | False | TOR | TBA201604030 | . 5 rows × 23 columns . Pitching DF . Team pitching stats, one row per team per game . pitching_df = pd.DataFrame(pitching) for k in pitching_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue pitching_df[k] =pd.to_numeric(pitching_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) pitching_df.head() . Pitcher DF . Individual pitching stats (starting pitchers only), one row per pitcher per game . pitcher_df = pd.DataFrame(pitchers) for k in pitcher_df.keys(): if any(x in k for x in [&#39;team&#39;,&#39;name&#39;,&#39;game_id&#39;, &#39;home_away&#39;]): continue pitcher_df[k] =pd.to_numeric(pitcher_df[k],errors=&#39;coerce&#39;, downcast=&#39;float&#39;) # filter the pitcher performances to just the starting pitcher pitcher_df = pitcher_df[~pitcher_df[&#39;game_score&#39;].isna()].copy().reset_index(drop=True) pitcher_df.drop(columns=[x for x in pitcher_df.keys() if &#39;inherited&#39; in x], inplace=True) pitcher_df.head() . Calculate Differences in the Statistics . Here is where we&#39;re going to generate a bunch of columns in the game_df. Our statistical differences are going to be calculated like this: . For every downloaded statistic: calculate the average, standard deviation and skew: for time periods of 5, 10, 45, 180 and 730 days: grouped by team (or pitcher name in the case of pitchers) then shift the data so each row contains pre-game statistics then take the difference of the opposing team . That result is put into the game_df with a name like &quot;5day_R_Avg&quot;, or &quot;45Day_IP_StDev&quot;. The other dfs are done at that point . # The code is a little dense. Sorry if it&#39;s hard to follow. import numpy as np def add_rolling(period, df, stat_columns): for s in stat_columns: if &#39;object&#39; in str(df[s].dtype): continue df[s+&#39;_&#39;+str(period)+&#39;_Avg&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).mean()) df[s+&#39;_&#39;+str(period)+&#39;_Std&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).std()) df[s+&#39;_&#39;+str(period)+&#39;_Skew&#39;] = df.groupby(&#39;team&#39;)[s].apply(lambda x:x.rolling(period).skew()) return df def get_diff_df(df, name, is_pitcher=False): #runs for each of the stat dataframes, returns the difference in stats #set up dataframe with time index df[&#39;date&#39;] = pd.to_datetime(df[&#39;game_id&#39;].str[3:-1], format=&quot;%Y%m%d&quot;) df = df.sort_values(by=&#39;date&#39;).copy() newindex = df.groupby(&#39;date&#39;)[&#39;date&#39;] .apply(lambda x: x + np.arange(x.size).astype(np.timedelta64)) df = df.set_index(newindex).sort_index() # get stat columns stat_cols = [x for x in df.columns if &#39;int&#39; in str(df[x].dtype)] stat_cols.extend([x for x in df.columns if &#39;float&#39; in str(df[x].dtype)]) #add lags df = add_rolling(&#39;5d&#39;, df, stat_cols) # this game series df = add_rolling(&#39;10d&#39;, df, stat_cols) df = add_rolling(&#39;45d&#39;, df, stat_cols) df = add_rolling(&#39;180d&#39;, df, stat_cols) # this season df = add_rolling(&#39;730d&#39;, df, stat_cols) # 2 years # reset stat columns to just the lags (removing the original stats) df.drop(columns=stat_cols, inplace=True) stat_cols = [x for x in df.columns if &#39;int&#39; in str(df[x].dtype)] stat_cols.extend([x for x in df.columns if &#39;float&#39; in str(df[x].dtype)]) # shift results so that each row is a pregame stat df = df.reset_index(drop=True) df = df.sort_values(by=&#39;date&#39;) for s in stat_cols: if is_pitcher: df[s] = df.groupby(&#39;name&#39;)[s].shift(1) else: df[s] = df.groupby(&#39;team&#39;)[s].shift(1) # calculate differences in pregame stats from home vs. away teams away_df = df[~df[&#39;is_home_team&#39;]].copy() away_df = away_df.set_index(&#39;game_id&#39;) away_df = away_df[stat_cols] home_df = df[df[&#39;is_home_team&#39;]].copy() home_df = home_df.set_index(&#39;game_id&#39;) home_df = home_df[stat_cols] diff_df = home_df.subtract(away_df, fill_value=0) diff_df = diff_df.reset_index() # clean column names for s in stat_cols: diff_df[name + &quot;_&quot; + s] = diff_df[s] diff_df.drop(columns=s, inplace=True) return diff_df . # then merges those differences back into the main df df = game_df df = pd.merge(left=df, right = get_diff_df(batting_df, &#39;batting&#39;), on = &#39;game_id&#39;, how=&#39;left&#39;) print(df.shape) df = pd.merge(left=df, right = get_diff_df(pitching_df, &#39;pitching&#39;), on = &#39;game_id&#39;, how=&#39;left&#39;) print(df.shape) df = pd.merge(left=df, right = get_diff_df(pitcher_df, &#39;pitcher&#39;,is_pitcher=True), on = &#39;game_id&#39;, how=&#39;left&#39;) df.shape . (10641, 305) (10641, 665) . (10641, 995) . df[&#39;season&#39;] = df[&#39;date&#39;].dt.year df[&#39;month&#39;]=df[&#39;date&#39;].dt.month df[&#39;week_num&#39;] = pd.to_numeric(df[&#39;date&#39;].dt.isocalendar().week, errors=&#39;coerce&#39;) df[&#39;dow&#39;]=df[&#39;date&#39;].dt.weekday df[&#39;date&#39;] = (pd.to_datetime(df[&#39;date&#39;]) - pd.Timestamp(&quot;1970-01-01&quot;)) // pd.Timedelta(&#39;1s&#39;) #epoch time . import pickle pickle.dump(df, open(&#39;dataframe.pkl&#39;, &#39;wb&#39;)) . Train the Model . Finally, we get to train the model! This is just going to be a crude first run to make sure everything is working properly. We&#39;ll add the cleverest bits in the next blog post. . import pickle df = pickle.load(open(&#39;dataframe.pkl&#39;, &#39;rb&#39;)) df.drop(columns=&#39;week_num&#39;, inplace=True) . encode_me = [x for x in df.keys() if &#39;object&#39; in str(df[x].dtype)] for x in encode_me: df[x] = df.groupby(x)[&#39;home_team_win&#39;].apply(lambda x:x.rolling(180).mean()).shift(1) . df = df.sort_values(by=&#39;date&#39;).copy().reset_index(drop=True) X = df.drop(columns=[&#39;home_team_win&#39;, &#39;game_id&#39;]) y = df.home_team_win . X_train = X[:-1000] y_train = y[:-1000] X_valid = X[-1000:-500] y_valid = y[-1000:-500] X_test = X[-500:] y_test = y[-500:] . import xgboost as xgb params = {&#39;learning_rate&#39;: 0.075,&#39;max_depth&#39;: 2} gbm = xgb.XGBClassifier(**params) model = gbm.fit(X_train, y_train, eval_set = [[X_train, y_train], [X_valid, y_valid]], eval_metric=&#39;logloss&#39;, early_stopping_rounds=10) xgb_test_preds = model.predict(X_test) xgb_test_proba = model.predict_proba(X_test)[:,1] . [0] validation_0-logloss:0.69100 validation_1-logloss:0.69176 Multiple eval metrics have been passed: &#39;validation_1-logloss&#39; will be used for early stopping. Will train until validation_1-logloss hasn&#39;t improved in 10 rounds. [1] validation_0-logloss:0.68912 validation_1-logloss:0.69012 [2] validation_0-logloss:0.68747 validation_1-logloss:0.68910 [3] validation_0-logloss:0.68596 validation_1-logloss:0.68856 [4] validation_0-logloss:0.68450 validation_1-logloss:0.68759 [5] validation_0-logloss:0.68324 validation_1-logloss:0.68659 [6] validation_0-logloss:0.68210 validation_1-logloss:0.68576 [7] validation_0-logloss:0.68103 validation_1-logloss:0.68509 [8] validation_0-logloss:0.68003 validation_1-logloss:0.68470 [9] validation_0-logloss:0.67912 validation_1-logloss:0.68387 [10] validation_0-logloss:0.67831 validation_1-logloss:0.68354 [11] validation_0-logloss:0.67754 validation_1-logloss:0.68298 [12] validation_0-logloss:0.67685 validation_1-logloss:0.68291 [13] validation_0-logloss:0.67617 validation_1-logloss:0.68277 [14] validation_0-logloss:0.67559 validation_1-logloss:0.68246 [15] validation_0-logloss:0.67500 validation_1-logloss:0.68242 [16] validation_0-logloss:0.67446 validation_1-logloss:0.68211 [17] validation_0-logloss:0.67394 validation_1-logloss:0.68184 [18] validation_0-logloss:0.67341 validation_1-logloss:0.68162 [19] validation_0-logloss:0.67290 validation_1-logloss:0.68176 [20] validation_0-logloss:0.67243 validation_1-logloss:0.68163 [21] validation_0-logloss:0.67195 validation_1-logloss:0.68162 [22] validation_0-logloss:0.67155 validation_1-logloss:0.68165 [23] validation_0-logloss:0.67109 validation_1-logloss:0.68187 [24] validation_0-logloss:0.67062 validation_1-logloss:0.68143 [25] validation_0-logloss:0.67022 validation_1-logloss:0.68119 [26] validation_0-logloss:0.66984 validation_1-logloss:0.68103 [27] validation_0-logloss:0.66946 validation_1-logloss:0.68115 [28] validation_0-logloss:0.66908 validation_1-logloss:0.68130 [29] validation_0-logloss:0.66867 validation_1-logloss:0.68138 [30] validation_0-logloss:0.66829 validation_1-logloss:0.68099 [31] validation_0-logloss:0.66791 validation_1-logloss:0.68138 [32] validation_0-logloss:0.66753 validation_1-logloss:0.68140 [33] validation_0-logloss:0.66723 validation_1-logloss:0.68139 [34] validation_0-logloss:0.66694 validation_1-logloss:0.68172 [35] validation_0-logloss:0.66658 validation_1-logloss:0.68185 [36] validation_0-logloss:0.66629 validation_1-logloss:0.68176 [37] validation_0-logloss:0.66594 validation_1-logloss:0.68164 [38] validation_0-logloss:0.66556 validation_1-logloss:0.68202 [39] validation_0-logloss:0.66523 validation_1-logloss:0.68237 [40] validation_0-logloss:0.66489 validation_1-logloss:0.68272 Stopping. Best iteration: [30] validation_0-logloss:0.66829 validation_1-logloss:0.68099 . from sklearn.calibration import calibration_curve from sklearn.metrics import accuracy_score, brier_score_loss import matplotlib.pyplot as plt import pickle def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 8)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_pred, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} t tAccuracy:{:.4f} t Brier Loss: {:.4f}&quot;.format( name, accuracy_score(y_test, y_pred), brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() outcomes,predictions,probabilities = pickle.load(open(&#39;baseline.pkl&#39;,&#39;rb&#39;)) data = [ (outcomes, predictions, probabilities, &#39;Casino&#39;), (y_test,xgb_test_preds, xgb_test_proba, &#39;XGBoost&#39;) ] cal_curve(data, 15) . . Casino Accuracy:0.6006 Brier Loss: 0.2358 XGBoost Accuracy:0.5700 Brier Loss: 0.2425 . That&#39;s not terrible for a first run. I think this model has some promise. Let&#39;s look at the feature importances just to see which features it&#39;s using most. It looks like the model likes RE24, and it likes the longer lags: 180day &amp; 730day. . import pandas as pd x = pd.Series(model.get_booster().get_score(importance_type= &#39;total_gain&#39;) ).sort_values() x[-25:].plot(kind=&#39;barh&#39;,title=&quot;XGBoost Feature Gain&quot;) . . &lt;AxesSubplot:title={&#39;center&#39;:&#39;XGBoost Feature Gain&#39;}&gt; . Next Up . In part 3, we&#39;re going to add team power rankings and casino odds to the dataset to improve the results further! .",
            "url": "https://rdpharr.github.io/project_notes/baseball/web_scraping/xgboost/brier/accuracy/calibration/2020/09/21/MLB-Part2-First-Model.html",
            "relUrl": "/baseball/web_scraping/xgboost/brier/accuracy/calibration/2020/09/21/MLB-Part2-First-Model.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "MLB Baseball Predictions",
            "content": "Baseball isn’t associated with good memories for me. I wasn’t very good at it, always hitting foul balls in little league. My parents eventually figured out I needed glasses, but not before I developed a mild disdain for the sport. So it’s with great surprise that I find myself spending so much time thinking about it. Even more so when I make a bet and start cheering for a team. It’s amazing what having some skin in the game will do for your interest level. . This is the first in a blog series, written in jupyter notebooks, which will show you how to build a program that predicts the outcome of MLB games. We&#39;ll be using our web scraping and machine learning skills to build a model that outperforms the casino&#39;s sports books. . Important: Web scraping is dependant on other people&#8217;s web pages. If they change their site, this blog&#8217;s code will break. Don&#8217;t expect the code presented here to work forever. . Benchmarking the Sportsbooks . First thing to do is figure out how we’re going to know if we’re doing well. The most intuitive performance benchmark I found was the sportsbooks themselves. If I can make better predictions than the sportsbooks, then I should be doing well. . Downloading Sportsbook Data . We need to start by putting together a database of historic odds and outcomes for MLB games. First step is to get a list of days when games were played. We can get those from baseball-reference.com. . import requests import re import datetime as dt url = &#39;https://www.baseball-reference.com/leagues/MLB/2019-schedule.shtml&#39; resp = requests.get(url) # All the H3 tags contain day names days = re.findall(&quot;&lt;h3&gt;(.*2019)&lt;/h3&gt;&quot;, resp.text) dates = [dt.datetime.strptime(d,&quot;%A, %B %d, %Y&quot;) for d in days] print(&quot;Number of days MLB was played in 2019:&quot;, len(dates)) . Number of days MLB was played in 2019: 210 . We need the correct days because we&#39;ll be pulling the odds data from covers.com by day. Covers aggregates the published odds from several sources and then publishes a consensus moneyline. We&#39;ll grab that, along with the score of the game. Here&#39;s how we pull and parse that data. . from bs4 import BeautifulSoup as bs game_data = [] for d in dates: # get the web page with game data on it game_day = d.strftime(&#39;%Y-%m-%d&#39;) url = f&#39;https://www.covers.com/Sports/MLB/Matchups?selectedDate={game_day}&#39; resp = requests.get(url) # parse the games scraped_games = bs(resp.text).findAll(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_game_box&#39;}) for g in scraped_games: game = {} game[&#39;home_moneyline&#39;] = g[&#39;data-game-odd&#39;] game[&#39;date&#39;] = g[&#39;data-game-date&#39;] try: game[&#39;home_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_home&#39;}).text.strip() game[&#39;away_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_away&#39;}).text.strip() except: game[&#39;home_score&#39;] =&#39;&#39; game[&#39;away_score&#39;] =&#39;&#39; game_data.append(game) if len(game_data) % 500==0: #show progress print(dt.datetime.now(), game_day, len(game_data)) print(&quot;Done! Games downloaded:&quot;, len(game_data)) . 2020-09-21 13:43:11.235271 2019-05-02 500 2020-09-21 13:43:48.030736 2019-06-08 1000 2020-09-21 13:44:16.474179 2019-07-18 1500 2020-09-21 13:44:45.831305 2019-08-24 2000 2020-09-21 13:45:15.965332 2019-10-03 2500 Done! Games downloaded: 2533 . Here&#39;s what that data looks like. You can see the moneyline was negative, meaning that the home team was favored. But the home team lost, so the prediction from the casinos was inaccurate. . game_data[0] . {&#39;home_moneyline&#39;: &#39;-155&#39;, &#39;date&#39;: &#39;2019-03-20 05:35:00&#39;, &#39;home_score&#39;: &#39;7&#39;, &#39;away_score&#39;: &#39;9&#39;} . That would have been a pretty good payout if you bet on the away team. . Sportsbook Accuracy . Let&#39;s see how the sportsbook did in all the games we just downloaded. . from sklearn.metrics import accuracy_score # the actual outcome of the game, true if the the home team won outcomes = [] # predictions derived from moneyline odds. True if the home team was the favorite predictions = [] # probability the home team will win, derived from moneyline odds # derived from formulas at https://www.bettingexpert.com/academy/advanced-betting-theory/odds-conversion-to-percentage probabilities = [] for d in game_data: try: moneyline = int(d[&#39;home_moneyline&#39;]) home_score = int(d[&#39;home_score&#39;]) away_score = int(d[&#39;away_score&#39;]) except: #incomplete data continue if moneyline==100: # it&#39;s rare to have a tossup since covers is averaging the odds from several sports books # but we&#39;ll exclude them from our calculations continue # convert moneyline odds ot their implied probabilities if moneyline&lt;0: probabilities.append(-moneyline/(-moneyline + 100)) elif moneyline&gt;100: probabilities.append(100/(moneyline + 100)) outcomes.append(home_score&gt;away_score) predictions.append(moneyline&lt;0) print(&quot;Sportsbook accuracy (excluding tossups): {0:.2f}%&quot;.format(100*accuracy_score(outcomes,predictions))) . Sportsbook accuracy (excluding tossups): 60.06% . That&#39;s it, right? We need a model that is better than 60% accurate. . If you plan to use this data for betting, you should have more than a win/loss prediction. To really make money, we would like to know if we think the odds of a team winning are better or worse that what the sportsbook thinks they are. Then we&#39;d be able to use some sort of expected value calculation to determine if the bet is profitable. . Sportsbook Calibration . We really want to know if we can build a model that is better calibrated than the casino&#39;s sportsbooks. Knowing our calibration will help us with bet sizing, as well as more sophisticated betting algorithms. Here&#39;s a graphical view of the calibration of the casino sports book data. . from sklearn.calibration import calibration_curve from sklearn.metrics import accuracy_score, brier_score_loss import matplotlib.pyplot as plt def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 8)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_pred, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} tAccuracy:{:.4f} t Brier Loss: {:.4f}&quot;.format( name, accuracy_score(y_test, y_pred), brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() data = [(outcomes, predictions, probabilities, &#39;SportsBook&#39;)] cal_curve(data, 15) . . SportsBook Accuracy:0.6006 Brier Loss: 0.2358 . The graph above tells us several things about the calibration of the casino&#39;s predictions. The reliability curve clearly shows that the casino is highly calibrated. Interestingly, it looks like the blue line is shifted down slightly from the &quot;perfectly calibrated&quot; line. It would be a better fit if it was 0.05 higher. This may account for the house advantage. . The histogram below shows what portion of the games fall into each bin. We see a slight predicted advantage to the home team, with more than 50% of the observations above the 50% mark. Otherwise it looks pretty normally distributed. . Above, I said the reliability curve looks highly calibrated. If we are to judge our own efforts against the sportsbook, we can&#39;t just be eyeballing this graph all the time. A metric would be nice. One metric that is suited for calibration measurement is the Brier Score, which I&#39;ll be using to measure the model effectiveness going forward. Getting a model that scores less than 0.2358 is the target. . Next Up . Next, we&#39;ll start building out our historic data and training the model using XGBoost and LightGBM. . import pickle pickle.dump((outcomes,predictions,probabilities), open(&#39;baseline.pkl&#39;,&#39;wb&#39;)) .",
            "url": "https://rdpharr.github.io/project_notes/baseball/benchmark/web_scraping/brier/accuracy/calibration/2020/09/20/baseball_project.html",
            "relUrl": "/baseball/benchmark/web_scraping/brier/accuracy/calibration/2020/09/20/baseball_project.html",
            "date": " • Sep 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a semi-retired tech professional, living in Las Vegas, Nevada. I’m forever in school, right now at University of the People. I do work to end racial injustices, focusing on criminal justice reform with Mass Liberation of Nevada. . You can get in touch with me using the links at the bottom of this page. . About the Site . This website is powered by fastpages. .",
          "url": "https://rdpharr.github.io/project_notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rdpharr.github.io/project_notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}