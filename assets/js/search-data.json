{
  
    
        "post0": {
            "title": "MLB Baseball Predictions",
            "content": "Baseball isn’t associated with good memories for me. I wasn’t very good at it, always hitting foul balls in little league. My parents eventually figured out I needed glasses, but not before I developed a mild disdain for the sport. So it’s with great surprise that I find myself spending so much time thinking about it. Even more so when I make a bet and start cheering for a team. It’s amazing what having some skin in the game will do for your interest level. . This is the first in a blog series, written in jupyter notebooks, which will show you how to build a program that predicts the outcome of MLB games. We&#39;ll be using our web scraping and machine learning skills to build a model that outperforms the casino&#39;s sports books. . Important: Web scraping is dependant on other people&#8217;s web pages. If they change their site, this blog&#8217;s code will break. Don&#8217;t expect the code presented here to work forever. . Benchmarking the Sportsbooks . First thing to do is figure out how we’re going to know if we’re doing well. The most intuitive performance benchmark I found was the sportsbooks themselves. If I can make better predictions than the sportsbooks, then I should be doing well. . Downloading Sportsbook Data . We need to start by putting together a database of historic odds and outcomes for MLB games. First step is to get a list of days when games were played. We can get those from baseball-reference.com. . import requests import re import datetime as dt url = &#39;https://www.baseball-reference.com/leagues/MLB/2019-schedule.shtml&#39; resp = requests.get(url) # All the H3 tags contain day names days = re.findall(&quot;&lt;h3&gt;(.*2019)&lt;/h3&gt;&quot;, resp.text) dates = [dt.datetime.strptime(d,&quot;%A, %B %d, %Y&quot;) for d in days] print(&quot;Number of days MLB was played in 2019:&quot;, len(dates)) . Number of days MLB was played in 2019: 210 . We need the correct days because we&#39;ll be pulling the odds data from covers.com by day. Covers aggregates the published odds from several sources and then publishes a consensus moneyline. We&#39;ll grab that, along with the score of the game. Here&#39;s how we pull and parse that data. . from bs4 import BeautifulSoup as bs game_data = [] for d in dates: # get the web page with game data on it game_day = d.strftime(&#39;%Y-%m-%d&#39;) url = f&#39;https://www.covers.com/Sports/MLB/Matchups?selectedDate={game_day}&#39; resp = requests.get(url) # parse the games scraped_games = bs(resp.text).findAll(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_game_box&#39;}) for g in scraped_games: game = {} game[&#39;home_moneyline&#39;] = g[&#39;data-game-odd&#39;] game[&#39;date&#39;] = g[&#39;data-game-date&#39;] try: game[&#39;home_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_home&#39;}).text.strip() game[&#39;away_score&#39;] =g.find(&#39;div&#39;,{&#39;class&#39;:&#39;cmg_matchup_list_score_away&#39;}).text.strip() except: game[&#39;home_score&#39;] =&#39;&#39; game[&#39;away_score&#39;] =&#39;&#39; game_data.append(game) if len(game_data) % 500==0: #show progress print(dt.datetime.now(), game_day, len(game_data)) print(&quot;Done! Games downloaded:&quot;, len(game_data)) . 2020-09-20 07:06:15.530599 2019-05-02 500 2020-09-20 07:06:44.327909 2019-06-08 1000 2020-09-20 07:07:11.075344 2019-07-18 1500 2020-09-20 07:07:38.829703 2019-08-24 2000 2020-09-20 07:08:07.881623 2019-10-03 2500 Done! Games downloaded: 2533 . Sportsbook Accuracy . Let&#39;s see how the sportsbook did in all these games we just downloaded. . from sklearn.metrics import accuracy_score outcomes = [] predictions = [] probabilities = [] for d in game_data: try: moneyline = int(d[&#39;home_moneyline&#39;]) home_score = int(d[&#39;home_score&#39;]) away_score = int(d[&#39;away_score&#39;]) except: #incomplete data continue if moneyline==100: # it&#39;s rare to have a tossup since covers is averaging the odds from several sports books # but we&#39;ll exclude them from our calculations continue # convert moneyline odds ot their implied pprobabilities # https://www.bettingexpert.com/academy/advanced-betting-theory/odds-conversion-to-percentage if moneyline&lt;0: probabilities.append(-moneyline/(-moneyline + 100)) elif moneyline&gt;100: probabilities.append(100/(moneyline + 100)) outcomes.append(home_score&gt;away_score) predictions.append(moneyline&lt;0) print(&quot;Sportsbook accuracy (excluding tossups): {0:.2f}%&quot;.format(100*accuracy_score(outcomes,predictions))) . Sportsbook accuracy (excluding tossups): 60.06% . That&#39;s it, right? We need a model that is better than 60% accurate. . It&#39;s not so simple if you plan to use this data for betting. If we really want to make money, we would like to know if we think the odds of a team winning are better or worse that what the sportsbook thinks they are. Then we&#39;d be able to use some sort of expected value calculation to determine if the bet is profitable. . Sportsbook Calibration . We really want to know if we can build a model that is better calibrated) than the casino&#39;s sportsbooks. Knowing our calibration will help us with bet sizing, as well as more sophisticated betting algorithms. Here&#39;s a graphical view of the calibration of the casino sports book data. . from sklearn.calibration import calibration_curve from sklearn.metrics import accuracy_score, brier_score_loss import matplotlib.pyplot as plt def cal_curve(data, bins): # adapted from: #https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html fig = plt.figure(1, figsize=(12, 12)) ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2) ax2 = plt.subplot2grid((3, 1), (2, 0)) ax1.plot([0, 1], [0, 1], &quot;k:&quot;, label=&quot;Perfectly calibrated&quot;) for y_test, y_pred, y_proba, name in data: brier = brier_score_loss(y_test, y_proba) print(&quot;{} tAccuracy:{:.4f} t Brier Loss: {:.4f}&quot;.format( name, accuracy_score(y_test, y_pred), brier)) fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_proba, n_bins=bins) ax1.plot(mean_predicted_value, fraction_of_positives, label=&quot;%s (%1.4f)&quot; % (name, brier)) ax2.hist(y_proba, range=(0, 1), bins=bins, label=name, histtype=&quot;step&quot;, lw=2) ax1.set_ylabel(&quot;Fraction of positives&quot;) ax1.set_ylim([-0.05, 1.05]) ax1.legend(loc=&quot;lower right&quot;) ax1.set_title(&#39;Calibration plots (reliability curve)&#39;) ax2.set_xlabel(&quot;Mean predicted value&quot;) ax2.set_ylabel(&quot;Count&quot;) ax2.legend(loc=&quot;lower right&quot;) plt.tight_layout() plt.show() data = [(outcomes, predictions, probabilities, &#39;SportsBook&#39;)] cal_curve(data, 15) . . SportsBook Accuracy:0.6006 Brier Loss: 0.2358 . The graph above tells us several things about the calibration of the casino&#39;s predictions. The reliability curve clearly shows that the casino is highly calibrated. Interestingly, it looks like the blue line is shifted down slightly from the &quot;perfectly calibrated&quot; line. It would be a better fit if it was 0.05 higher. This may account for the house advantage. . The histogram below shows what portion of the games fall into each bin. We see a slight predicted advantage to the home team, with more than 50% of the observations above the 50% mark. Otherwise it looks pretty normally distributed. . A metric well suited for calibration measurement is the Brier Score, which I&#39;ll be using to measure the model effectiveness going forward. Getting a model that scores less than 0.2358 is the target. . Next Up . Next, we&#39;ll start building out our historic data and training the model using XGBoost and LightGBM. .",
            "url": "https://rdpharr.github.io/project_notes/baseball/benchmark/web_scraping/brier/accuracy/calibration/2020/09/20/baseball_project.html",
            "relUrl": "/baseball/benchmark/web_scraping/brier/accuracy/calibration/2020/09/20/baseball_project.html",
            "date": " • Sep 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rdpharr.github.io/project_notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rdpharr.github.io/project_notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}